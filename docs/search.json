[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connectivity for Disaster Resiliance",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#project-context",
    "href": "index.html#project-context",
    "title": "Connectivity for Disaster Resiliance",
    "section": "Project context",
    "text": "Project context\n\n\n\nWhat problems are involved withnt the project context?\n\n\nWhat is the background to these problems?\n\n\nWhat soilutions are the stakeholders considering?"
  },
  {
    "objectID": "index.html#conceptual-design",
    "href": "index.html#conceptual-design",
    "title": "Connectivity for Disaster Resiliance",
    "section": "Conceptual design",
    "text": "Conceptual design\n\nThe scientific procedure is carried out from an evaluation research perspective shown in the figure 1. A set of criteria facilitates informed-decisions to improve city resilience by identifying and prioritizing critical infrastructures in the road network to access healthcare facilities. (a) An assessment of the network analysis and accessibility access to healthcare facilities before and after a disaster (b) identifies critical infrastructures (c).\nThe research objective is to improve urban resilience by identifying critical infrastructures necessary for accessing healthcare facilities, comparing road connectivity and accessibility. The following research questions are addressed in this study:"
  },
  {
    "objectID": "index.html#research-questions",
    "href": "index.html#research-questions",
    "title": "Connectivity for Disaster Resiliance",
    "section": "Research questions",
    "text": "Research questions\n\nHow did the road connectivity of the city network change after being impacted by the floodings in Rio Grande do Sul based on Edge Betweenness Centrality?\nWhich healthcare facilities were most affected by the floodings based on accessibility metrics?\nIn which area of a city’s network should decisions be made to reinforce or redesign routes to optimize healthcare accessibility and ensure minimal disruption during flood events?\n\n\n\nShow the code\nlibrary(tidygeocoder)\n\nsome_addresses &lt;- tibble::tribble(\n  ~name, ~addr,\n  \"number one\", \"Mathias Velho,Canoas\",\n  \"number_1\" ,\"Beira-Rio stadium\",\n  \"number_2\", \"Airport in Porto Alegre\",\n  \"number_8\",\"historic market in Porto Alegre\",\n  \"number_11\",\"Eldorado do Sul\",\n  \"number_13\",\" city of Encantado in Rio Grande do Sul\",\n  \"number_10\",\"Canoas\",\n  \"number_26\",\" Salgado Filho International Airport in Porto Alegre in Rio Grande do Sul\",\n  \"number_27\",\" Roca Sales in Rio Grande do Sul\",\n  \"number_28\",\"Roca Sales in Rio Grande do Sul,\",\n  \"number_29\",\"in Encantado,\",\n  \"number_31\",\"Lajeado, Rio Grande do Sul state,\",\n  \"number_\",\"Humaita, in Porto Alegre, Rio Grande do Sul\",\n)\n\nlat_longs &lt;- some_addresses |&gt;\n  geocode(addr, method=\"osm\", lat = latitude, lon = longitude, full_results = TRUE)\n\ntidygeocoder::geocode(\"\")"
  },
  {
    "objectID": "index.html#gannt-chart",
    "href": "index.html#gannt-chart",
    "title": "Connectivity for Disaster Resiliance",
    "section": "Gannt Chart",
    "text": "Gannt Chart"
  },
  {
    "objectID": "intro.html#describe-a-big-problem-that-needs-to-be-solved",
    "href": "intro.html#describe-a-big-problem-that-needs-to-be-solved",
    "title": "1  Introduction",
    "section": "1.1 Describe a big problem that needs to be solved",
    "text": "1.1 Describe a big problem that needs to be solved\nNatural hazards are becoming disasters, as the frequency and severity of extreme event related to climate change increase(Kuffer et al. 2021). It is in the magnitude of loss of lives and economical, environmental and material damage where the difference between natural hazards and disasters lies, requiring external aid (Riesgos Naturales 2002). In this situation, the system’s capacity to recover from change, namely, its resilience (Levin and Carpenter 2012), failed causing the interruption of the urban functionality."
  },
  {
    "objectID": "intro.html#state-your-strategy-to-help-solve-the-problem",
    "href": "intro.html#state-your-strategy-to-help-solve-the-problem",
    "title": "1  Introduction",
    "section": "1.3 State your strategy to help solve the problem",
    "text": "1.3 State your strategy to help solve the problem\n— Connectivity, Accesibility, Resilience"
  },
  {
    "objectID": "intro.html#state-a-specific-research-questionhypothesis-whose-ansswertest-will-help-to-solve-the-problem",
    "href": "intro.html#state-a-specific-research-questionhypothesis-whose-ansswertest-will-help-to-solve-the-problem",
    "title": "1  Introduction",
    "section": "1.4 State a specific research question/hypothesis whose ansswer/test will help to solve the problem",
    "text": "1.4 State a specific research question/hypothesis whose ansswer/test will help to solve the problem\n\n\n\n\n“Caracterização Da População Em Áreas de Risco No Brasil - PGI.” n.d. Accessed September 22, 2024. https://www.ibge.gov.br/apps/populacaoareasderisco/#/home/.\n\n\nKuffer, Monika, Dana R. Thomson, Andrew Maki, Sabine Vanhuysse, Stefanos Georganos, Richard Sliuzas, and Claudio Persello. 2021. “EO-Based Low-Cost Frameworks to Address Global Urban Data GAPS on Deprivation and Multiple Hazards.” In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, 2106–9. Brussels, Belgium: IEEE. https://doi.org/10.1109/IGARSS47720.2021.9554094.\n\n\nLevin, Simon Asher, and Stephen R. Carpenter, eds. 2012. The Princeton Guide to Ecology. 2. pr., 1. paperback pr. Princeton, NJ: Princeton University Press.\n\n\nRiesgos Naturales. 2002. 1a ed. Barcelona: Editorial Ariel."
  },
  {
    "objectID": "methodology.html#area-of-interest",
    "href": "methodology.html#area-of-interest",
    "title": "2  Methodology",
    "section": "2.2 Area of Interest",
    "text": "2.2 Area of Interest\nThe different study areas were in Rio Grande do Sul, which is located in the southernmost region of Brazil bordered by Uruguay in the south and Argentina to the west. Its total extension of 281.707 km² includes 497 municipalities with 274,390 population exposed to risk areas (“Caracterização Da População Em Áreas de Risco No Brasil - PGI” n.d.). The first study area lies between 51º 27’91’‘W and 50º94’07’‘W latitude and 30º17’22’‘S and 29º80’48’’S being identified as the dense urban centre 1210 that contains the municipalities of Porto Alegre, Canoas, Cachoeirinha,Alvorada, Gravataí, Esteio,Sapucaia do Sul and Viamão.\n\n2.2.1 Data\n\n\n\n\n\n\n\nDescribe experimental set-up\nExplain used techniques\n\n2.2.1.1 Closeness\n\n\n2.2.1.2 Sum edge betweenness\n\n\n2.2.1.3 Import\nThe tool ogr2ogr imported the data and adjusted the multilinestring geometry to linestring geometry to process single features more efficiently as recommended Obe and Hsu (2017).\n\n\nShow the code\n## OSM geometry: porto_alegre_net_pre.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_net_pre.geojson -nln porto_alegre_net_pre -lco GEOMETRY_NAME=the_geom -nlt LINESTRING -explodecollections \n\n## Administrative units: nuts.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/nuts.geojson -nln nuts -lco GEOMETRY_NAME=geom\n\n## Flooding extent: flooding_rio_grande_do_sul.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/flooding_rio_grande_do_sul.geojson -nln flooding_rio_grande_do_sul -lco GEOMETRY_NAME=the_geom \n\n## Building density: urban_center_4326.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/urban_center_4326.geojson -nln urban_center_4326 -lco GEOMETRY_NAME=geom \n\n## Hospitals\n### From Rio Grande do Sul Geoportal\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/Hospitais_com_Leitos_de_UTIs_no_RS.geojson -nln hospitals_bed_rs -lco GEOMETRY_NAME=geom\n\n\n\n\n\n2.2.2 Flooding mask\nThe downloaded flooding extent covered a larger area in Rio Grande do Sul. However, our area of interest for urban dense city was only Porto Alegre. Therefore, a serie of operations reduced the area focusing on Porto Alegre and at the same time improved the performance reducing the size of the mask.\nSubdividing the flooding mask to make the spatial indexes more efficient was the first step. The reason was the higher number of vertices of large objects and larges bounding boxes that hinder the spatial index performance (Link). After the spatial indexes were enable, the intersection with the Porto Alegre region subset the data. Additionally, the following code dissolve the borders to break the multipolygon into simple polygons to improve the performance of functions such as st_difference() (link).\n\n\nShow the code\n--- 1) Subdividing to make spatial indexes more efficent\n--- Select the GHS area that intersects with Porto Alegre\nCREATE TABLE flooding_subdivided_porto AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n---Subdivide the flood extent to increase spatial indexes performance \nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul) \n---- Select the flooding subunits that intersects with the previous bounding box\nSELECT \n    flooding_sul_subdivided.* \nFROM \n    flooding_sul_subdivided, \n    porto_alegre_ghs_bbox\nWHERE \n    st_intersects(flooding_sul_subdivided.the_geom, porto_alegre_ghs_bbox.geom_bbox);\n--- 2) Simplifying geometry\nCREATE TABLE flooding_cleaned_porto AS\n    SELECT\n        (ST_Dump(the_geom)).geom::geometry(polygon, 4326) geom \n    FROM \n        flooding_subdivided_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union AS\n    SELECT\n        ST_Union(geom)::geometry(multipolygon, 4326) geom \n    FROM \n        flooding_cleaned_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union_simple AS\n    SELECT \n        (ST_Dump(geom)).geom::geometry(polygon, 4326) geom \n    FROM \n        flooding_cleaned_porto_union;\n--- This allowed to calculate the area of each polygon finding slivers that were removed using the following code. \nSELECT COUNT(*)\n     FROM flooding_cleaned_porto_union_simple ; --- count= 54.\n\nDELETE FROM \n  flooding_cleaned_porto_union_simple\nWHERE \n  ST_Area(geom) &lt; 0.0001; ---count= 2.\n\n\n\n\n\n\n\n\n\n\n\nShow the code\n## Obtain Area\nSELECT \n  sum(st_area(geom::geography)/10000)::integer \nFROM  \n  flooding_cleaned_porto_union_simple;\n## Obtain size in memory\nSELECT \n  pg_size_pretty(SUM(ST_MemSize(geom))) \nFROM  \n  flooding_cleaned_porto_union_simple;\n## Obtain number of points\nSELECT \n  sum(ST_Npoints(geom)) \nFROM  \n  flooding_cleaned_porto_union_simple;"
  },
  {
    "objectID": "methodology.html#framework",
    "href": "methodology.html#framework",
    "title": "2  Methodology",
    "section": "2.1 Framework",
    "text": "2.1 Framework\nThree major research phases or lanes constituted the methodology.\n\nThe first lane defined an area of interest (AOI) can be identified using sources such as Copernicus EMS Rapid Mapping.\nThe second lane transforms this AOI into a network and graph using openrouteservice. Additionally, in this phase called “routable network” lane, the post-disaster network is derived through spatial overlay with the extent of flooding.\nLastly, centrality analysis, which includes connectivity and accessibility metrics, is conducted on the routable network. This analysis is currently done in R and the technical goal is the implementation of PostgreSQL."
  },
  {
    "objectID": "methodology.html#routable-network",
    "href": "methodology.html#routable-network",
    "title": "2  Methodology",
    "section": "2.3 Routable network",
    "text": "2.3 Routable network\nThe creation of the network after the disaster using the modified flooding mask and the creation of the origin-destination are considered in this section. The following code set the paremeters before carrying out these transformations following post 1 and post2.\n\n\nShow the code\n--- Firstly, add spatial index everywhere:\nCREATE INDEX idx_porto_alegre_net_largest_geom ON porto_alegre_net_largest USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_source ON porto_alegre_net_largest USING btree(fromid);\nCREATE INDEX idx_porto_alegre_net_largest_target ON porto_alegre_net_largest USING btree(toid);\nCREATE INDEX idx_porto_alegre_net_largest_id ON porto_alegre_net_largest USING btree(ogc_fid);\nCREATE INDEX idx_porto_alegre_net_largest_cost ON porto_alegre_net_largest USING btree(weight);\n--- Set up the configuration\n----- Increase performance by using more max_parallel_workes_per_gather\nSET max_parallel_workers_per_gather =4;\n----- Cluster the index\nCLUSTER porto_alegre_net_largest USING porto_alegre_net_largest;\n\n\n\n2.3.1 Geometry: OpenStreetMap network (OSM)\nThe following SQL code created the command to import OpenStreetMap (OSM) network using osmium. Firstly using the GHS-SMOD dataset the Global Human Settlement in Porto Alegre is chosen. Secondly, this GHS selected is used to create the bounding box that it is lastly used to generate the osmium code. The OSM network obtained with this osmium command was the input required for OpenRouteService (ORS).\n\n\nShow the code\n--- GHS urban area that intersected with Porto Alegre city\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n--- The command used the Porto Alegre GHS and its Bounding Box to import the OpenStreet Network. \nporto_alegre_ghs_bbox_osmium_command AS (\n    SELECT \n        ST_XMin(ST_SnapToGrid(geom_bbox, 0.0001)) AS min_lon,\n        St_xmax(ST_SnapToGrid(geom_bbox,0.0001)) AS max_lon,\n        St_ymin(ST_SnapToGrid(geom_bbox,0.0001)) AS min_lat,\n        St_ymax(ST_SnapToGrid(geom_bbox, 0.0001)) AS max_lat\n    FROM porto_alegre_ghs_bbox)\nSELECT\n    'osmium extract -b ' \n    || min_lon || ',' || min_lat || ',' || max_lon || ',' || max_lat || \n    ' sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf' AS osmium_command\nFROM porto_alegre_ghs_bbox_osmium_command;\n\n\n\n\n2.3.2 Graph: OpenRouteService network (ORS)\nFirstly, a docker for OpenRouteService transformed the OSM network that covered the GHS in Porto Alegre into a graph. In the “ors-config.yml” from OpenRotueService (ORS), the “source_file” parameter is set with the following directory. OpenRouteService (ORS) created a a routable network, assigining costs and adding information for each node, namely, “fromId” and “toId”. The R script “get_graph” from Marcel Reinmuth exported the ORS network named as “porto_alegre_net”.\nSecondly, the parameters start_vid, end_vid named “fromid” and “toid” in the network dataset from openrouteservice are transform into bigint data type to run the algorithm pgr_dijkstra as the official documentation indicates.\n\n\nShow the code\n--- Firstly, the source_file parameter in the ors.config.yml file is set to the OSM network previously obtained \nsource_file: /home/ors/files/puerto_alegre_urban_center.osm.pbf\n---- Secondly the exported ORS graph is prepared for pgrouting casting the right data type for the pgrouting functions.\nselect * from porto_alegre_net_pre; \nALTER TABLE porto_alegre_net_pre \n    ALTER COLUMN \"toid\" type bigint,\n    ALTER COLUMN \"fromid\" type bigint,\n    ALTER COLUMN \"ogc_fid\" type bigint;\n\n\n\n\nShow the code\n## Load data\nors_network &lt;- st_read(eisenberg_connection, layer=\"porto_alegre_net_pre\")\nosm_network &lt;- st_read(eisenberg_connection, layer=\"puerto_alegre_ghs_osm\")\nors_network_subset &lt;- ors_network |&gt; head()\n## Create subset using a bounding box\nors_subset &lt;- ors_network |&gt; filter(id ==140210) |&gt; st_bbox()\nxrange &lt;- ors_subset$xmax - ors_subset$xmin\nyrange &lt;- ors_subset$ymax - ors_subset$ymin\n## Expand the bounding box\nors_subset[1] &lt;- ors_subset[1] - (4 * xrange) # xmin - left\nors_subset[3] &lt;- ors_subset[3] + (4 * xrange) # xmax - right\nors_subset[2] &lt;- ors_subset[2] - (2 * yrange) # ymin - bottom\nors_subset[4] &lt;- ors_subset[4] + (2 * yrange) # ymax - top\n## Convert bounding box into polygon\nors_subset_bbox &lt;- ors_subset %&gt;%  # \n  st_as_sfc() \n## Use the polygon to subset the network\nintersection_ors &lt;- sf::st_intersection(ors_network, ors_subset_bbox)\nintersection_osm &lt;- sf::st_intersection(osm_network, ors_subset_bbox)\n## Mapview maps\nm1 &lt;- mapview(intersection_osm, \n              color=\"#6e93ff\",\n              layer.name =\"OpenStreetMap - Geometry\",\n              popup=popupTable(intersection_osm, \n                               zcol=c(\"id\",\"osm_id\",\"name\",\"geom\")))\nm2 &lt;- mapview(intersection_ors,\n              color =\"#d50038\",\n              layer.name=\"OpenRouteService -Graph\",\n              popup=popupTable(intersection_ors))\nsync(m1,m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n2.3.2.0.1 Inspection of ORS graph network\nBefore using the graph, a quick inspection of the graph using the pgrouting function pgr_strongComponents() revealed how many components or isolated self-connecting network the graph had. The largest network component is selected using the length. The following code created a table with the different components and another table containing the largest component ruling out the rest of the components with relatively small networks.\n\n\nShow the code\n--- 1) Quick inspection of the graph to determine components\n---- Create a vertice table for pgr_dijkstra()\nSELECT \n  pgr_createVerticesTable('porto_alegre_net_pre', source:='fromid', target:='toid');\n  \n---- Add data to the vertices created table \nCREATE TABLE component_analysis_network_porto AS\nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\nporto_alegre_net_component_geom AS (\nSELECT \n    net.*,\n    net_geom.the_geom\nFROM \n    porto_alegre_net_component AS net\nJOIN \n    porto_alegre_net_pre  AS net_geom\nON \n    net.node = net_geom.fromid)\nSELECT \n    component,\n    st_union(the_geom) AS the_geom,\n    st_length(st_union(the_geom)::geography)::int AS length\nFROM  \n    porto_alegre_net_component_geom\nGROUP BY component\nORDER BY length DESC;\n--- 2) A table with the component of the network with the highest length\nCREATE TABLE porto_alegre_net_largest AS\n---- Obtain again table classifying nodes in different components           \nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\n--- Calculate the largest component from the network\nlargest_component_net AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto \n    LIMIT 1),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component,\n    largest_component_net\nWHERE \n    porto_alegre_net_component.component = largest_component_net.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_pre AS net_multi_component,\n    largeset_component_network_porto AS net_largest_component\nWHERE  \n    net_multi_component.fromid IN (net_largest_component.node);\n\n\n\n\n\nShow the code\ncomponent_analysis_network &lt;- st_read(eisenberg_connection, \"component_analysis_network_porto\")\n## Select top 3\ncomponent_analysis_network_3 &lt;-  component_analysis_network |&gt; arrange(desc(distance)) |&gt; slice(1:3) \n### Tidy component to be used as categorical variable\ncomponent_analysis_network_3$component  &lt;- component_analysis_network_3$component |&gt; as.character() |&gt; as_factor()\n## Visualization\nm1_net &lt;- component_analysis_network_3 |&gt;\n              filter(component==\"1\") |&gt;\n              mapview(layer.name = \"1st Longest Network\",\n                      lwd= 0.5,\n                      color=\"#66c2a5\")\n\nm2_net &lt;- component_analysis_network_3 |&gt; \n              filter(component==\"3728\") |&gt;\n              mapview(layer.name = \"2nd Longest Network\",\n                      color =\"#8da0cb\",\n                      lwd=0.5)\n\nm3_net &lt;- component_analysis_network_3 |&gt;\n              filter(component==\"40578\") |&gt;\n              mapview(layer.name = \"3rd Longest Network\",\n                      color=\"#fc8d62\",\n                      lwd =0.5)\n\nm1_net + m2_net + m3_net\n\n\n\n\n2.3.2.1 Pre-Disaster network\n\n\nShow the code\nCREATE TABLE porto_alegre_net_largest AS\n---- Obtain again table classifying nodes in different components           \nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_pre')),\n--- Calculate the largest component from the network\nlargest_component_net AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto \n    LIMIT 1),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component,\n    largest_component_net\nWHERE \n    porto_alegre_net_component.component = largest_component_net.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_pre AS net_multi_component,\n    largeset_component_network_porto AS net_largest_component\nWHERE  \n    net_multi_component.source IN (net_largest_component.node);\n\n\n\n\n2.3.2.2 Post-Disaster network\nA naive approach overlaying the flooding mask with the road network by the function st_difference() caused the session to crash. The follwing multi-step methodology reduced the processing cost making the query feasible using less resources. Firstly the network inside the flooding mask is selected. Secondly, a subset of the network outside the flooding mask avoided using spatial operations saving computing resources by using the ID’s from the network inside the flooding to filter the data.Thirdly, to obtain the boundaries between the inside and outside network, the geometry is converted into its exterior ring. From this network on the boundary, only the exterior part that intersected with the outside boundary was selected.\n\n\nShow the code\n--- 1) Network inside the flooding mask\nCREATE TABLE porto_alegre_street_in_v2 AS\nSELECT net.id,\n    CASE \n        WHEN ST_Contains(flood.the_geom, net.the_geom)\n        THEN net.the_geom\n        ELSE st_intersection(net.the_geom, flood.the_geom)\n    END AS  geom\nFROM porto_alegre_net_largest net\nJOIN flooding_subdivided_porto flood\nON st_intersects(net.the_geom, flood.the_geom);\n--- 2) Network outside the flooding mask\nCREATE TABLE porto_alegre_street_out_v2 AS\nSELECT net.*\nFROM porto_alegre_net_largest net\nWHERE net.id NOT IN (\n    SELECT net.id\n    FROM porto_alegre_street_in_v2 net);\n--- 3) Network on the boundaries\nCREATE TABLE porto_alegre_net_outside_v2 AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul),\nexterior_ring_porto_alegre_v2 AS (\nSELECT \n    ST_ExteriorRing((ST_Dump(union_geom)).geom) as geom\nFROM (\n    SELECT \n        ST_Union(flood.the_geom) as union_geom\n    FROM \n        porto_alegre_ghs_bbox as bbox\n    JOIN \n        flooding_sul_subdivided as flood\n    ON \n        ST_Intersects(flood.the_geom, bbox.geom_bbox)\n) AS subquery)\nSELECT net.id,\n    CASE\n        WHEN NOT ST_Contains(flood.geom, net.the_geom)\n        THEN net.the_geom\n            ELSE st_intersection(net.the_geom, flood.geom)\n    END AS  geom,\n    net.target,\n       net.source,\n       cost,\n       \"unidirectid\",\n       \"bidirectid\"\nFROM\nporto_alegre_net_largest AS net\nJOIN exterior_ring_porto_alegre_v2 flood ON\nst_intersects(net.the_geom, flood.geom);\n\n---- For the network\nCREATE INDEX idx_porto_alegre_net_outside_v2 ON porto_alegre_net_outside_v2 USING gist (geom);\n\nCLUSTER porto_alegre_net_outside_v2 USING idx_porto_alegre_net_outside_v2;\n\n--- For the flooding mask\nCREATE INDEX flooding_sul_subdivided_idx ON flooding_sul_subdivided USING gist (the_geom);\n\nCLUSTER flooding_sul_subdivided USING flooding_sul_subdivided_idx;\n---- Before doing difference\nVACUUM(FULL, ANALYZE) porto_alegre_net_outside_v2;\nVACUUM(FULL, ANALYZE) flooding_sul_subdivided;\n--- st_difference and uniting the external to the boundaries\n---  Unite the subunits of the flooding\nCREATE TABLE flooding_symple as \nSELECT st_union(geom) as the_geom FROM flooding_cleaned_porto_union_simple;\n---- Index the flooding\nCREATE INDEX flooding_symple_idx ON flooding_symple USING gist (the_geom);\nCLUSTER flooding_symple USING flooding_symple_idx;\n--- 4) Obtain the boundary network that intersect with the outside network\nCREATE TABLE difference_outside_flood_v3 AS\nSELECT net.id,\n        target,\n        source,\n        cost,\n        unidirectid,\n        bidirectid,\nst_difference(net.geom, flood.the_geom) AS the_geom\nFROM porto_alegre_net_outside_v2 AS net,\nflooding_symple  AS flood;\n\n--- 5) Unite outside network with the external part of the boundary network\nCREATE TABLE porto_alegre_net_post_v5 AS\nSELECT *\nFROM porto_alegre_street_out_v2\nUNION\nSELECT *\nFROM difference_outside_flood_v3;\n\n\n\n\nShow the code\nporto_alegre_net_pre &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_net_largest\")\ncentrality_pre &lt;- sf::st_read(eisenberg_connection, \"centrality_weighted_100_bidirect_cleaned\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_cleaned_porto\")\nnet_in &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_in_v2\")\n## subset\nsubset_network_pre &lt;-  sf::st_read(eisenberg_connection, \"porto_alegre_net_largest_subset\")\nsubset_network_in &lt;- sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_in_v3_subset.geojson\") |&gt; select(\"geometry\")\nsubset_network_out &lt;- sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_out_subset.geojson\")  \nsubset_network_outside_flood &lt;-sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_outside_subset.geojson\") \nsubset_network_post &lt;- sf::st_read(eisenberg_connection, \"reet_united_v3\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_symple\") |&gt; st_as_sf() \n## centrality\nmapview(subset_network_pre,\n          color=\"#d4e7e7\",\n          lwd= 1,\n          layer.name=\"0.Pre-flooding network\",\n          popup=popupTable(subset_network_pre,\n          zcol=c(\"id\",\"source\",\"target\",\"bidirectid\"))) +\n  mapview(flooding,\n          color=\"darkblue\",\n          alpha.regions= 0.5,\n          layer.name=\"0.Flooding layer\") +\n  mapview(subset_network_in,\n          color=\"red\",\n          lwd= 1,\n          hide = TRUE,\n          layer.name=\"1.Network inside the flooding\") +\n  mapview(subset_network_out,\n          color=\"yellow\",\n          lwd= 1.2,\n          hide = TRUE,\n          layer.name=\"2.Flooding outside the flooding mask\",\n          popup=popupTable(subset_network_out)) +\n  mapview(subset_network_outside_flood,\n          color=\"lightgreen\",\n          lwd= 1.4,\n          hide = TRUE,\n          layer.name=\"3 & 4. Network on the boundaries\",\n          popup=popupTable(subset_network_outside_flood)) +\n    mapview(subset_network_post,\n          color=\"green\",\n          lwd= 1,\n          hide =TRUE,\n          layer.name=\"5. Uniting network external to the boundires and network from outside\",\n          popup=popupTable(subset_network_post))\n\n\n\n\n\nShow the code\nSELECT pgr_createVerticesTable(\n            'porto_alegre_net_largest',\n            the_geom:= 'the_geom',\n            source:= 'fromid',\n            target:= 'toid')\n\n\n\n\n\n2.3.3 Post-scenario\n\n\nShow the code\n---- Create vertices\nSELECT pgr_createverticesTable('porto_alegre_net_post_v5');\n----- Obtain components to be used later as a filter\nCREATE TABLE component_analysis_network_porto_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\nporto_alegre_net_component_geom_post AS (\nSELECT \n    net.*,\n    net_geom.the_geom\nFROM \n    porto_alegre_net_component_post AS net\nJOIN \n    porto_alegre_net_post_v5  AS net_geom\nON \n    net.node = net_geom.source)\nSELECT \n    component,\n    st_union(the_geom) AS the_geom,\n    st_length(st_union(the_geom)::geography)::int AS length\nFROM  \n    porto_alegre_net_component_geom_post\nGROUP BY component\nORDER BY length DESC;\n--- Summarise by component\nCREATE TABLE components_network_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\n--- Calculate the largest component from the network\nlargests_component_net_post AS (\n    SELECT \n        component,\n        the_geom,\n        length::int\n    FROM \n        component_analysis_network_porto_post)\nSELECT * FROM largests_component_net_post;\n\n---- Visualize which are more important\nCREATE TABLE main_components_post AS\nSELECT \n    * \nFROM \n    components_network_post\nWHERE\n    component IN (21,14,5187);\n\n-------------- Remember to justify why 5\n\nCREATE TABLE prueba_largest_network_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\n--- Calculate the largest component from the network\nlargest_component_net_post AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto_post\n    LIMIT 5),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto_post AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component_post,\n    largest_component_net_post\nWHERE \n    porto_alegre_net_component_post.component = largest_component_net_post.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_post_v5 AS net_multi_component,\n    largeset_component_network_porto_post AS net_largest_component\nWHERE  \n    net_multi_component.source IN (net_largest_component.node);\n--- Snap 2728 points to post-scenario network\n----\nCREATE TABLE od_2728_snapped_post AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_2728 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM prueba_largest_network_post_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n--- Create 100 origin samples from the snapped points post-scenario\n\nCREATE TABLE weight_sampling_100_origin_post  AS\nWITH porto_100_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_origin;  \n--- Create 100 destination from the snapped points post-scenario\n\nCREATE TABLE weight_sampling_100_destination_post  AS\nWITH porto_100_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_destination;  \n--- Indeces on origin\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);\n--- Indeces on destination\nCREATE INDEX weight_sampling_100_destination_post_idx_pt_id ON weight_sampling_100_destination_post USING btree(pt_id);\nCREATE INDEX weight_sampling_100_destination_post_idx_net_id ON weight_sampling_100_destination_post USING btree(net_id);\nCREATE INDEX weight_sampling_100_destination_post_idx_the_geom ON weight_sampling_100_destination_post USING gist(the_geom);\n---- Indexes on origin\nCREATE INDEX weight_sampling_100_origin_post_idx_pt_id ON weight_sampling_100_origin_post USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_post_idx_net_id ON weight_sampling_100_origin_post USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_post_idx_the_geom ON weight_sampling_100_origin_post USING gist(the_geom);\n---- Index on the new network\nCREATE INDEX prueba_largest_network_post_idx_GEOM ON prueba_largest_network_post USING gist(the_geom);\nCREATE INDEX prueba_largest_network_post_idx_id ON prueba_largest_network_post USING btree(id);\nCREATE INDEX prueba_largest_network_post_idx_target ON prueba_largest_network_post USING btree(target);\nCREATE INDEX prueba_largest_network_post_idx_source ON prueba_largest_network_post USING btree(source);\nCREATE INDEX prueba_largest_network_post_idx_cost ON prueba_largest_network_post USING btree(cost);\n---- Cluster \nCLUSTER prueba_largest_network_post USING prueba_largest_network_post_idx_GEOM;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_100_origin;\nVACUUM(full, ANALYZE) weight_sampling_100_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;"
  },
  {
    "objectID": "methodology.html#centrality-analysis",
    "href": "methodology.html#centrality-analysis",
    "title": "2  Methodology",
    "section": "2.4 Centrality Analysis",
    "text": "2.4 Centrality Analysis\n\n2.4.1 Origin-Destination\n\n2.4.1.1 Regular distribution (naive)\nA series of regular points that represented the origin and destination on the Area of Interest is created with the function “I_Grid_Point_Series”.\nThe following code created 1258 points representing the origin or destination regularly separated by 0.01º.\nThe application of two indexes improved further queries on this new table. This was recommended because these points were outside the network requiring snapping, which is a spatial operation with relatively high computational costs. Apart from indexing the geometry column and the id, the query is constrained to a buffer of 0.02º to reduce computational costs.\n\n\nShow the code\n--- Creating the function that sample regularly:\nCREATE OR REPLACE FUNCTION I_Grid_Point_Series(geom geometry, x_side decimal, y_side decimal, spheroid boolean default false)\nRETURNS SETOF geometry AS $BODY$\nDECLARE\nx_max decimal;\ny_max decimal;\nx_min decimal;\ny_min decimal;\nsrid integer := 4326;\ninput_srid integer;\nx_series DECIMAL;\ny_series DECIMAL;\nBEGIN\nCASE st_srid(geom) WHEN 0 THEN\n  geom := ST_SetSRID(geom, srid);\n  RAISE NOTICE 'SRID Not Found.';\n    ELSE\n        RAISE NOTICE 'SRID Found.';\n    END CASE;\n\n    CASE spheroid WHEN false THEN\n        RAISE NOTICE 'Spheroid False';\n    else\n        srid := 900913;\n        RAISE NOTICE 'Spheroid True';\n    END CASE;\n    input_srid:=st_srid(geom);\n    geom := st_transform(geom, srid);\n    x_max := ST_XMax(geom);\n    y_max := ST_YMax(geom);\n    x_min := ST_XMin(geom);\n    y_min := ST_YMin(geom);\n    x_series := CEIL ( @( x_max - x_min ) / x_side);\n    y_series := CEIL ( @( y_max - y_min ) / y_side );\nRETURN QUERY\nSELECT st_collect(st_setsrid(ST_MakePoint(x * x_side + x_min, y*y_side + y_min), srid)) FROM\ngenerate_series(0, x_series) as x,\ngenerate_series(0, y_series) as y\nWHERE st_intersects(st_setsrid(ST_MakePoint(x*x_side + x_min, y*y_side + y_min), srid), geom);\nEND;\n$BODY$ LANGUAGE plpgsql IMMUTABLE STRICT;\n--- Using this function to create the sampling table\nCREATE TABLE regular_point_od AS (\nWITH multipoint_regular AS(\nselect \n    I_Grid_Point_Series(geom, 0.01,0.01, false) AS geom\n    from porto_alegre_bbox as geom),\npoint_regular AS(\nSELECT \n    st_setsrid((st_dump(geom)).geom, 4326)::geometry(Point, 4326) AS geom\nFROM  multipoint_regular)\nSELECT \n    row_number() over() AS id,\n    geom\nFROM \n    point_regular);\n--- Applying indexes\nCREATE INDEX idx_regular_point_od_geom ON regular_point_od USING GIST (geom);\nCREATE INDEX idx_regular_point_od_id ON regular_point_od USING btree(id);\n\n\n\n\n2.4.1.2 Weighted on building volume\nIn R…\n\n\nShow the code\n# Import data\n## raster\nbuildings &lt;- terra::rast('/home/ricardo/HeiGIT-Github/do_not_push_too_large/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif')\npop_ghs &lt;- 2728\n## AoI\naoi_bbox &lt;- sf::read_sf('/home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_bbox_derived.geojson')\n## reproject for clipping\naoi_bbox_reproj &lt;- aoi_bbox |&gt; st_transform(crs(buildings)) |&gt; as_Spatial()\nbuild_cropped &lt;- crop(buildings, aoi_bbox_reproj)\n## reproject for sampling\nbuild_4326 &lt;- terra::project(build_cropped, crs(aoi_bbox))\n## weighted sampling\nod &lt;- spatSample(build_cropped, pop_ghs, \"weights\", as.points=TRUE, ) |&gt; st_as_sf() |&gt; st_transform(4326)\nnames(od)[1] &lt;- 'building'\nDBI::dbWriteTable(connection, \n                  DBI::Id(schema = \"public\", table = \"od_2728\"), \n                  od)\n\n\nSnap the points\n\n\nShow the code\n--- Create ID column\nALTER TABLE od_2728\nADD COLUMN id serial PRIMARY KEY;\n\n--- Snap all the Apoints based on building density to the closest vertice within the network\nCREATE TABLE od_2728_snapped AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_2728 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n--- Create 100 origin samples from the snapped points\n\nCREATE TABLE weight_sampling_100_origin  AS\nWITH porto_100_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_origin;  \n        \n--- Create 100 destination from the snapped points\nCREATE TABLE weight_sampling_100_destination  AS\nWITH porto_100_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_destination;  \n--- Indeces on origin\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);\n--- Indeces on destination\nCREATE INDEX weight_sampling_1000_destination_idx_pt_id ON weight_sampling_1000_destination USING btree(pt_id);\nCREATE INDEX weight_sampling_1000_destination_idx_net_id ON weight_sampling_1000_destination USING btree(net_id);\nCREATE INDEX weight_sampling_1000_destination_idx_the_geom ON weight_sampling_1000_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_100_origin;\nVACUUM(full, ANALYZE) weight_sampling_100_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n----\n\n---- add ID\nALTER TABLE od_77763\nADD COLUMN id serial;\n---- Rename the column name\nALTER TABLE od_77763\nrename \"GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0\" \nto \"build\";\n---- create spatial index\nCREATE INDEX idx_od_77763_geom ON od_77763\n       USING gist(geometry);\nCREATE INDEX idx_od_77763_id on weighted_sampling\n       USING btree(id);\nCREATE INDEX idx_od_77763_geom_build on weighted_sampling \n       USING btree(\"build\");\n---- The current total is 77763\n\n----\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n----\n\nCREATE TABLE od_40420_snapped_destination AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\n2.4.1.3 Hospitals\n\n\nShow the code\n--- Create a table with the bounding box that contains Porto Alegre + GHS\nCREATE TABLE porto_alegre_bbox AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs)\n SELECT * FROM porto_alegre_ghs_bbox \n---Use the bounding box to select hospitals\nCREATE TABLE hospital_rs_node_v2 AS\nWITH hospital_rs_porto AS (\nSELECT \n    h.*\nFROM \n    hospitals_bed_rs AS h,\n    porto_alegre_bbox bbox\nWHERE st_intersects(h.geom, bbox.geom_bbox))\nSELECT DISTINCT ON (h.cd_cnes)\n    cd_cnes,\n    ds_cnes,\n    f.id,\n    f.the_geom &lt;-&gt; h.geom AS distance,\n    h.geom AS geom_hospital,\n    f.the_geom AS geom_node\nFROM hospital_rs_porto h\nLEFT JOIN LATERAL\n(SELECT \n    id, \n    the_geom\nFROM porto_alegre_net_largest_vertices_pgr AS net\nORDER BY\n    net.the_geom &lt;-&gt; h.geom\nLIMIT 1) AS f ON true\n\n---- Create index to optimize fuerther queries\n\nCREATE INDEX idx_hospital_rs_node_v2 ON hospital_rs_node_v2 USING btree(id);\n\n\n\n\n\n\n\nShow the code\n## Crop and Reproject\n## gdalwarp -te -4850853.201784615 -3737074.296348413 -4616291.881935796 -3495378.804761388 GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V\n## gdalwarp -t_srs \"EPSG:4326\" GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0_RioGrandeDoSul.tif GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif\npal &lt;- mapview::mapviewPalette(\"mapviewTopoColors\")\nghs_build &lt;- stack(\"/home/ricardo/heigit_bookdown/data/GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif\")\nghs_smod &lt;- stack(\"/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif\")\nghs_smod_terra &lt;- terra::rast(\"/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif\")\nregular_sampling &lt;- st_read(\"/home/ricardo/heigit_bookdown/data/random_points_snapped.geojson\")\nregular_sampling_100 &lt;- sample_n(regular_sampling, size = 100)\nweighted_sampling_origin &lt;- st_read(eisenberg_connection,\"weight_sampling_100_origin\")\nweighted_sampling_destination &lt;- st_read(eisenberg_connection,\"weight_sampling_100_destination\")\n\nweighted_sampling_origin$sample &lt;- \"origin\"\nweighted_sampling_destination$sample &lt;- \"destination\"\nweighted_sampling_both &lt;- rbind(weighted_sampling_origin,\n                           weighted_sampling_destination )\n\nweighted_sampling &lt;- weighted_sampling_both |&gt; mutate(sample = as.factor(weighted_sampling$sample))\n\npoi_hospital &lt;- st_read(eisenberg_connection,\"hospital_rs_node_v2\")\nm &lt;- matrix(c(\n     0, 10, NA,   # Values &gt;= 0 and &lt;= 10 become 0\n     10, 13, 1,  # Values &gt; 10 and &lt;= 21 become 1\n     13, 29, 2,  # Values &gt; 21 and &lt;= 29 become 2\n     30, 30, 3   # Values == 30 become 3\n ), ncol = 3, byrow = TRUE)\n## Classify using the correct matrix\nrc2 &lt;- classify(ghs_smod_terra, m, include.lowest=TRUE)\nrc2_factor &lt;- as.factor(rc2)\nlevels(rc2_factor) &lt;- data.frame(\n  ID = 1:3,    # These should match the values in the classification\n  category = c(\"Rural: 10-13\", \"Suburban: 13-29\", \"Urban Center: 30\")\n)\ncategory_colors &lt;- c(\"#008f44\",\"#dedb96\", \"#cc9152\")\nmapview(ghs_build[[1]],\n        layer.name =\"Built-up volume\",\n        col.regions = pal(100),\n        alpha.regions= 0.45,\n        hide=TRUE) +\nmapview(ghs_smod[[1]],\n        layer.name = \"Settlement classification\",\n        col.regions = pal(100),\n        alpha.regions= 0.35,\n        hide=TRUE) +\nmapview(weighted_sampling,\n        layer.name=\"Weighted samples\",\n        zcol=\"sample\",\n        col.regions=c(\"#2D5CA4\",\"#00A3A0\"),\n        hide=TRUE,\n        cex= 3) +\nmapview(regular_sampling_100,\n        color = \"darkgray\",\n        col.regions=\"darkgray\",\n        cex= 3,\n        legend= FALSE,\n        hide=TRUE) +\nmapview(subset(poi_hospital,\n               select=c(\"cd_cnes\",\n                        \"ds_cnes\",\n                        \"id\",\n                        \"geom_hospital\")),\n                layer.name = \"POI - Hospitals\",\n                color= \"darkred\",\n        col.regions=\"#CA2334\",\n               popup=popupTable(poi_hospital, zcol=c(\"cd_cnes\",\"ds_cnes\",\"id\")))\n\n\n\n\n\n2.4.2 Edge betweenness\n\n\nShow the code\n--- Pre-scenario\n CREATE TABLE centrality_100_100_dijkstra AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_street_united AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n                      \n                     \n                     select * from porto_alegre_net_largest ;\n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_100_bidirect AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_100_100_dijkstra t1\nJOIN\n    porto_alegre_net_largest t2\nON\n    t1.id = t2.id;\n--- The final product requries  79941\nselect max(\"bidirectid\") from centrality_weighted_100_bidirect;\ncreate sequence bididirect_id_weight start 79941;\nupdate centrality_weighted_100_bidirect\nset \"bidirectid\" = nextval('bididirect_id_weight')\nwhere \"bidirectid\" is null ;\n---- verify\nselect count(*) \nfrom centrality_weighted_100_bidirect \nwhere \"bidirectid\" is null; --- 0\n----\ncreate table centrality_weighted_100_bidirect_group as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_100_bidirect\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---\n---- now recover the id\ncreate table centrality_weighted_100_bidirect_group_id as \nselect t1.*, t2.id\nfrom centrality_weighted_100_bidirect_group t1\njoin centrality_weighted_100_bidirect t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_100_bidirect_cleaned as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_100_bidirect_group_id t1\njoin\n    porto_alegre_net_largest t2\non\n    t1.id = t2.id;\n--- The final product is: centrality_weighted_100_bidirect_cleaned\n\n\n\n\nShow the code\n--- Post scenario\n\n----routing\n CREATE TABLE centrality_100_100_dijkstra_post AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM prueba_largest_network_post',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin_post  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination_post ),\n                      directed := TRUE) j\n                      left JOIN prueba_largest_network_post AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_100_bidirect_post AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_100_100_dijkstra_post t1\nJOIN\n    prueba_largest_network_post t2\nON\n    t1.id = t2.id;\n--- The final product requries  79918\nselect max(\"bidirectid\") from centrality_weighted_100_bidirect_post; ---79918\ncreate sequence bididirect_id_weight_post start 79918;\nupdate centrality_weighted_100_bidirect_post\nset \"bidirectid\" = nextval('bididirect_id_weight_post')\nwhere \"bidirectid\" is null ;\n   ---- verify\nselect count(*) \nfrom centrality_weighted_100_bidirect_post \nwhere \"bidirectid\" is null; --- 0              \n----\ncreate table centrality_weighted_100_bidirect_group_post as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_100_bidirect_post\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---        \n---- now recover the id\ncreate table centrality_weighted_100_bidirect_group_post_id as \nselect t1.*, t2.id\nfrom centrality_weighted_100_bidirect_group_post t1\njoin centrality_weighted_100_bidirect_post t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_100_bidirect_cleaned_post as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_100_bidirect_group_post_id t1\njoin\n    prueba_largest_network_post t2\non\n    t1.id = t2.id;       \n\n\n\n2.4.2.1 Para hospitales\n\n\nShow the code\n---- POST-EVENT\nCREATE TABLE weight_sampling_454_origin_post  AS\nWITH porto_454_origin_post AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post\n        ORDER BY random() \n        LIMIT 454)\nSELECT * FROM  porto_454_origin_post;   \n\n---- Create index for origin\nCREATE INDEX weight_sampling_454_origin_post_net_id ON weight_sampling_454_origin_post USING hash(net_id);\nCREATE INDEX weight_sampling_454_origin_post_geom ON weight_sampling_454_origin_post USING gist(the_geom);\n---- Running the query\n ---- Group by end_vid that represent destination\nEXPLAIN ANALYZE\n CREATE TABLE centrality_424_hospitals_porto_end_id_centrality_post AS\n SELECT   \n b.id,\n j.end_vid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM prueba_largest_network_post',\n                      ARRAY(SELECT net_id AS origin_id FROM weight_sampling_454_origin_post),\n                      ARRAY(SELECT id AS destination_id FROM hospital_rs_destination),\n                      directed := TRUE) j\n            LEFT JOIN prueba_largest_network_post AS b\n            ON j.edge = b.id\n            GROUP BY  b.id, j.end_vid, b.the_geom\n            ORDER BY centrality DESC;   \n\n\n\n\n2.4.2.2 Edge betweenness\n\n\nShow the code\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\nShow the code\nCREATE TABLE random_272_destination  AS\nwith random_272_destination AS (\n        SELECT\n            * \n        FROM \n            od_40420_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  random_272_destination;  \n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5 \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,\n         random_272_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\nCREATE TABLE porto_272_272_dijkstra AS\n WITH pgr_result AS (\n   SELECT pgr_dijkstra('SELECT id,\n           source,\n    target,\n     cost FROM porto_alegre_net_largest',\n     array_agg(fv), array_agg(tv), \n     directed := true\n   ) FROM vertices_lookup_v5\n )\nSELECT (pgr_dijkstra).*, a.fid, a.tid FROM pgr_result\nJOIN vertices_lookup_v5 a\nON (pgr_dijkstra).start_vid = a.fv\nAND (pgr_dijkstra).end_vid = a.tv;\n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\n\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\n2.4.2.3 Closeness\n\n\nShow the code\n---- Pre-Event\nCREATE TABLE clossness_hospital_porto AS \nWITH dijkstra_cost AS (\nSELECT * FROM pgr_dijkstraCostMatrix(\n  'SELECT id, source, target, cost FROM porto_alegre_net_largest',\n  (SELECT array_agg(id)\n    FROM porto_alegre_net_largest_vertices_pgr\n    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),\n  true)),\ncloseness AS (\n SELECT dc.start_vid,\n        sum(agg_cost)::int AS closeness\nFROM dijkstra_cost dc\nGROUP BY dc.start_vid\nORDER BY closeness DESC)\nSELECT \n    h.cd_cnes,\n    h.ds_cnes,\n    c.closeness,\n    h.distance,\n    h.geom_node,\n    h.geom_hospital\nFROM \n    closeness AS c\nLEFT JOIN\n    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;\n--- Post event\nCREATE TABLE clossness_hospital_porto_post AS \nWITH dijkstra_cost AS (\nSELECT * FROM pgr_dijkstraCostMatrix(\n  'SELECT id, source, target, cost FROM prueba_largest_network_post',\n  (SELECT array_agg(id)\n    FROM prueba_largest_network_post_vertices_pgr\n    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),\n  true)),\ncloseness AS (\n SELECT dc.start_vid,\n        sum(agg_cost)::int AS closeness\nFROM dijkstra_cost dc\nGROUP BY dc.start_vid\nORDER BY closeness DESC)\nSELECT \n    h.cd_cnes,\n    h.ds_cnes,\n    c.closeness,\n    h.distance,\n    h.geom_node,\n    h.geom_hospital\nFROM \n    closeness AS c\nLEFT JOIN\n    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;   \n    \n---- Add centrality from post-event to pre-event\nCREATE TABLE hospitals_closeness_both AS \nSELECT\n    pre.cd_cnes,\n    pre.ds_cnes,\n    pre.closeness  AS closeness_pre,\n    coalesce(post.closeness,0) AS closeness_post,\n    pre.geom_node,\n    pre.geom_hospital\nFROM \n    clossness_hospital_porto AS pre\nLEFT JOIN \n    clossness_hospital_porto_post AS post ON pre.cd_cnes  = post.cd_cnes\n\n\n\n\n2.4.2.4 Vulnerability: Socioeconomic indicators\n\n\n2.4.2.5 Resiliance:\n\n\n\n\n“Caracterização Da População Em Áreas de Risco No Brasil - PGI.” n.d. Accessed September 22, 2024. https://www.ibge.gov.br/apps/populacaoareasderisco/#/home/.\n\n\nObe, Regina O., and Leo S. Hsu. 2017. pgRouting: A Practical Guide. Chugiak: Locate Press."
  },
  {
    "objectID": "results.html#data-preparation",
    "href": "results.html#data-preparation",
    "title": "3  Results",
    "section": "3.1 Data preparation",
    "text": "3.1 Data preparation\n\n3.1.1 OSM & ORS data\nThe area of interest covered 1331 km² of surface including 9 municipalities with 20 healthcare facilities. On this area, the road network included 2463 components with a total length of 9039 km. The selected component for the centrality analysis represented the 96.53% of this total, with a length of 8720 km. The transformed graph network contained 75384 target nodes, 75184 source nodes, and 175608 edges. Classified as a large settlement (European Commission. Joint Research Centre. 2023), the estimated population from the GHSl-SMOD was 2728133 with a total built up area of 147 km².\nA total of 100 origin and 100 destination generated 9994 shortest paths. The initial naive approach using the regular sampling method generated these points in fields and non-populated areas, which would have skewed the representation of the population distribution. This would have resulted in increasing the centrality of roads that are less relevant to the population’s movement patterns during the flooding event. Therefore, using the weighted sampling based on the built-up areas provided a closer model representation of the flooding phenomena.\nAfter subdividing the flooding extent and adjusting it to the Area of Interest (AoI), the flooding covered 441 km² instead of the original 7103 km². This operation allowed to create the post-flooding road network with less computing costs by only selecting the flooding extent that intersected with the AoI. This post-flooding network with a length of 1926 km included the components 21, 14 and 5187, summing up a total of 63458 vertices against the 75384 vertices from the pre-flooding network. When compared to the network before the flooding, the post-network network reduced its length by -16 % , representing a loss of -2556 km.\n\n\nPostGIS: Result from the responsive code to import OSM\n# Osmium command\nosmium extract -b -51.2791,-30.1722,-50.9407,-29.8048 sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf\n\n\n\n\n\n\nR: Visualizing regular and weighted sampling\n## Crop and Reproject\n## gdalwarp -te -4850853.201784615 -3737074.296348413 -4616291.881935796 -3495378.804761388 GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V\n## gdalwarp -t_srs \"EPSG:4326\" GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0_RioGrandeDoSul.tif GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif\npal &lt;- mapview::mapviewPalette(\"mapviewTopoColors\")\nghs_build &lt;- stack(\"/home/ricardo/heigit_bookdown/data/GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif\")\nghs_smod &lt;- stack(\"/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif\")\nghs_smod_terra &lt;- terra::rast(\"/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif\")\nregular_sampling &lt;- st_read(\"/home/ricardo/heigit_bookdown/data/random_points_snapped.geojson\")\nregular_sampling_100 &lt;- sample_n(regular_sampling, size = 100)\nweighted_sampling_origin &lt;- st_read(eisenberg_connection,\"weight_sampling_100_origin\")\nweighted_sampling_destination &lt;- st_read(eisenberg_connection,\"weight_sampling_100_destination\")\nnuts &lt;- st_read(eisenberg_connection, \"nuts\")\n\nweighted_sampling_origin$sample &lt;- \"origin\"\nweighted_sampling_destination$sample &lt;- \"destination\"\nweighted_sampling_both &lt;- rbind(weighted_sampling_origin,\n                                weighted_sampling_destination )\n\nweighted_sampling &lt;- weighted_sampling_both |&gt; mutate(sample = as.factor(weighted_sampling_both$sample))\n\npoi_hospital &lt;- st_read(eisenberg_connection,\"hospital_rs_node_v2\")\nm &lt;- matrix(c(\n  0, 10, NA,   # Values &gt;= 0 and &lt;= 10 become 0\n  10, 13, 1,  # Values &gt; 10 and &lt;= 21 become 1\n  13, 29, 2,  # Values &gt; 21 and &lt;= 29 become 2\n  30, 30, 3   # Values == 30 become 3\n), ncol = 3, byrow = TRUE)\n## Classify using the correct matrix\nrc2 &lt;- classify(ghs_smod_terra, m, include.lowest=TRUE)\nrc2_factor &lt;- as.factor(rc2)\nlevels(rc2_factor) &lt;- data.frame(\n  ID = 1:3,    # These should match the values in the classification\n  category = c(\"Rural: 10-13\", \"Suburban: 13-29\", \"Urban Center: 30\")\n)\ncategory_colors &lt;- c(\"#008f44\",\"#dedb96\", \"#cc9152\")\nmapview(ghs_build[[1]],\n        layer.name =\"Built-up volume\",\n        col.regions = pal(100),\n        alpha.regions= 0.45,\n        hide=TRUE) +\n  mapview(ghs_smod[[1]],\n          layer.name = \"Settlement classification\",\n          col.regions = pal(100),\n          alpha.regions= 0.35,\n          hide=TRUE) +\n  mapview(weighted_sampling,\n          layer.name=\"Weighted samples\",\n          zcol=\"sample\",\n          col.regions=c(\"#2D5CA4\",\"#00A3A0\"),\n          hide=FALSE,\n          cex= 3) +\n  mapview(regular_sampling_100,\n          color = \"darkgray\",\n          col.regions=\"darkgray\",\n          cex= 3,\n          legend= FALSE,\n          hide=TRUE) +\n  mapview(subset(poi_hospital,\n                 select=c(\"cd_cnes\",\n                          \"ds_cnes\",\n                          \"id\",\n                          \"geom_hospital\")),\n          layer.name = \"POI - Hospitals\",\n          color= \"darkred\",\n          col.regions=\"#CA2334\",\n          popup=popupTable(poi_hospital, zcol=c(\"cd_cnes\",\"ds_cnes\",\"id\"))) +\nmapview(flooding,\n            color=\"darkblue\",\n            alpha.regions= 0.5,\n        hide =TRUE,\n            layer.name=\"Flooding layer\") +\n    mapview::mapview(centrality_post,\n                     lwd = 0.2,\n                     color=\"#cb2a32\",\n                     hide = TRUE,\n                     layer.name =\"Post-flooding centrality network\") +\n    mapview::mapview(centrality_pre,\n                     color = \"#00a4a4\",\n                     lwd= 0.2,\n                     hide=TRUE,\n                     layer.name =\"Pre-flooding centrality network\")"
  },
  {
    "objectID": "results.html#rq1-centrality-analysis",
    "href": "results.html#rq1-centrality-analysis",
    "title": "3  Results",
    "section": "3.2 RQ1: Centrality analysis",
    "text": "3.2 RQ1: Centrality analysis\n\nHow does road connectivity change after being impacted by flooding based on connectivity metrics?\n\n\n3.2.1 Sum Edge Betweenness\nThe average sum edge betweenness before the flooding was 96, being decreased to 58 after the flooding. In contrast, the flooding caused a -65 % decrease on the maximum values, reducing the sum edge betweenness from 1644 to 582. Prior to the event, the 30% of the network that extended over 579 km, accounted for 69 % of the overall connectivity adding 1448245 to the network. After the event, the upper quartile of the network, with a length of 363km, maintained 70% of the total centrality, however, it only contributed 701092 to the sum edge betweenness.\n\n\nR: Visualizing changes of centrality on the network before and after the flooding calculated with pgrouting\n### natural breaks\n#### For pre-event: 1644, 468, 142\ncentrality_pre$centrality_fct &lt;- cut(centrality_pre$centrality,\n                  breaks=c(0,142,468,1644),\n                  labels =c(\"low\",\"medium\",\"high\"),\n                  include.lowest= TRUE,\n                  right =FALSE)\n#### natural breaks:  81, 230, 582\ncentrality_post$centrality_fct &lt;- cut(centrality_post$centrality,\n                  breaks=c(0,81,230,582),\n                  labels =c(\"low\",\"medium\",\"high\"),\n                  include.lowest= TRUE,\n                  right =FALSE)\n### \ncentrality_pre_map &lt;- mapview::mapview(centrality_pre,\n                                       zcol=\"centrality_fct\",\n                                        lwd =\"centrality\",\n                                       layer.name =\"Centrality Pre-Event\",\n              popup=popupTable(centrality_pre, \n                               zcol=c(\"id\",\"centrality\",\"bidirectid\"))) \ncentrality_post_map &lt;- mapview::mapview(centrality_post,\n                                       zcol=\"centrality_fct\",\n                                        lwd = \"centrality\",\n                                       layer.name =\"Centrality Post-Event\",\n              popup=popupTable(centrality_pre, \n                               zcol=c(\"id\",\"centrality\",\"bidirectid\"))) \n\ncentrality_pre_map | centrality_post_map + mapview(flooding,\n          color=\"darkblue\",\n          alpha.regions= 0.5,\n          layer.name=\"Flooding layer\")\n\n\n\n\nThe most central road segments were georeferenced to reveal the name of these identified critical roads. It is found that urban arteries such as Freeway Anchieta or Avenida Presidente Castello Branco lost their entire centrality after the flooding. Other roads such as Avenida Teresópolis or Avenida Manoel Elias, Jadim Leopoldina, even being affected by the flooding, only lost their centrality partially.In contrast, roads which were not used before the flooding, such as those located nearby Costa do Morro, Itacolomi or Costa do Ipiranga, were used after the flooding event. These results are shown in the table 1 and mapped in the figure 1.\n\n\nR: Table of the most central road segments georeferenced\nlibrary(plyr)\n## Categories for pre-flooding or post-flooding\ndf_centrality_pre &lt;- centrality_pre |&gt;\n                      sf::st_drop_geometry() |&gt;\n                      mutate(event = \"pre-flooding\") \ndf_centrality_post &lt;- centrality_post |&gt;\n                        sf::st_drop_geometry() |&gt;\n                    mutate(event = \"post-flooding\")\n\n#### For pre-event: 1644, 468, 142\ndf_centrality_pre$centrality_fct &lt;- cut(df_centrality_pre$centrality,\n                  breaks=c(0,142,468,1644),\n                  labels =c(\"low\",\"medium\",\"high\"),\n                  include.lowest= TRUE,\n                  right =FALSE)\n#### natural perk:  81, 230, 582\ndf_centrality_post$centrality_fct &lt;- cut(df_centrality_post$centrality,\n                  breaks=c(0,81,230,582),\n                  labels =c(\"low\",\"medium\",\"high\"),\n                  include.lowest= TRUE,\n                  right =FALSE)\n## Join both in one dataframe\ndf_centrality_both &lt;- left_join(df_centrality_pre,\n                    df_centrality_post,\n                    by = 'id',\n                    suffix=c(\"_pre\",\"_post\")) |&gt;    dplyr::select(c(\"id\",\"bidirectid_pre\",\"bidirectid_post\",\"centrality_pre\",\"centrality_post\",\"centrality_fct_pre\",\"centrality_fct_post\",\"event_post\",\"event_pre\")) |&gt;\n  mutate(centrality_post = replace_na(centrality_post, 0)) ## Roads covered by the flooding appeared as NA in the post-scenario, replace tha NA value for 0\n\n## Calculate change on centrality after being impacted by flooding \ndf_centrality_both &lt;-  df_centrality_both |&gt;\n            mutate(centrality_diff = centrality_post-centrality_pre)\n### Obtain distribution using quantiles\ndf_centrality_both_quantiles &lt;- quantile(df_centrality_both$centrality_diff, na.rm =TRUE)\n### Filter the worst scenario, most negative values below second quartile\ndf_centrality_negative_outlier &lt;- df_centrality_both |&gt;\n                        filter( centrality_diff &lt;= df_centrality_both_quantiles[2])\n### Join with original data to obtain again geometry\ndf_centrality_negative_outlier_geom &lt;- df_centrality_negative_outlier |&gt;\n                        arrange(centrality_diff) |&gt;\n                        slice(1:1000) |&gt;\n                        left_join(centrality_pre, by = \"id\") |&gt;\n                    mutate(the_geom_centroid = st_centroid(the_geom),\n                           lon = st_coordinates(the_geom_centroid)[,1],\n                           lat = st_coordinates(the_geom_centroid)[,2])\n# Obtain a subset of the 500 observations oredered by lowest values on centrality difference\ndf_centrality_negative_outlier_geom_unique &lt;- df_centrality_negative_outlier_geom |&gt; \n  distinct(centrality_pre, .keep_all = TRUE) \n# Reverse geocoding to obtain the address based on the centroids\ndf_centrality_negative_outlier_distinct_address &lt;-  tidygeocoder::reverse_geocode(df_centrality_negative_outlier_geom_unique, lat=lat, lon=lon, method=\"osm\")\n## Create ID for the found addresses (97)\ndf_centrality_negative_outlier_distinct_address$address_number &lt;- seq(1, 167,1)\n\ndf_centrality_negative_outlier_distinct_address$short_address &lt;- str_extract(df_centrality_negative_outlier_distinct_address$address, \"^[^,]+, [^,]+\") \ndf_centrality_negative_outlier_distinct_address$centrality_diff_perc &lt;- round(((df_centrality_negative_outlier_distinct_address$centrality_post - df_centrality_negative_outlier_distinct_address$centrality_pre) / df_centrality_negative_outlier_distinct_address$centrality_pre *100),2)\n  \n## DT Table\nDT::datatable(subset(df_centrality_negative_outlier_distinct_address, select=c(\"address_number\", \"short_address\",\"centrality_pre\", \"centrality_post\",\"centrality_diff\",\"centrality_diff_perc\",\"id\",\"source\",\"target\",\"the_geom\")), \n              colnames= c(\"ID\",\"Address\",\"Pre-Centrality\", \"Post-Centrality\",\"Diff-Centrality\",\"Diff-Centrality(%)\",\"id_1\",\"source\",\"target\",\"the_geom\"),\n              filter=\"top\",\n              class='compact', rownames=FALSE, escape=FALSE, caption='Data description',\n              extensions=c(\"Buttons\",'RowGroup'),\n              options=list(\n                  order=list(list(5, 'desc'), list(2,'desc')),  # Sort by the first column (index 5)\n                  dom=\"Bfrtip\",\n                  columnDefs = list(list(visible=FALSE, targets= c(6,7,8,9))),\n                  buttons=c(\"copy\", \"csv\", \"pdf\"),\n                  initComplete = JS(\n                      \"function(settings, json) {\",\n                      \"$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});\",\n                      \"}\")\n              )\n) |&gt; \n      DT::formatStyle(\"centrality_pre\",\n     background=DT::styleColorBar(range(df_centrality_negative_outlier_distinct_address$centrality_pre), '#ee8b8b'),\n                    backgroundSize='98% 88%',\n                    backgroundRepeat='no-repeat',\n                    backgroundPosition='center') \n\n\n\n\nShow the code\ndf_centrality_bind &lt;- bind_rows(df_centrality_pre, df_centrality_post)\n\ndf_centrality_bind &lt;- df_centrality_bind |&gt; \n  mutate(centrality_fct = case_when(\n    centrality &gt;= 1 & centrality &lt; 53 ~ \"[1-53]\",  # Change &gt; to &gt;=\n    centrality &gt;= 53 & centrality &lt; 117 ~ \"[53-117]\",\n    centrality &gt;= 117 & centrality &lt; 700 ~ \"[117-700]\",\n    centrality &gt;= 700 & centrality &lt; 1644 ~ \"[700-1644]\",\n    centrality == 0 ~ \"[0]\",\n    TRUE ~ NA_character_  \n  ))\n\ndf_centrality_pre\n\nlibrary(metan)\nlibrary(ggsankey)\nlibrary(highcharter)\n\nd &lt;- select(df_centrality_bind, c(event, centrality_fct))\n\n\nhchart(data_to_sankey(d), \"sankey\", name = \"Centrality pre and post flooding\") |&gt; \nhc_title(text= \"Sankey Diagram: Porto Alegre\") %&gt;%\n  hc_subtitle(text= \"Centrality analysis\")  %&gt;%\n  hc_caption(text = \"&lt;b&gt;Change on the distribution edge betweenness&lt;b&gt;\")%&gt;%\n  hc_add_theme(hc_theme_economist())\n\n#### secon sankey diagram\nlibrary(\"ggalluvial\")\npre_network_largest\nsankey_pre_centrality &lt;- centrality_pre |&gt; dplyr::select(c(\"centrality\", \"the_geom\",\"event\")) |&gt;\n                                            mutate(length = st_length(the_geom))\nsankey_post_centrality &lt;- centrality_post |&gt; dplyr::select(c(\"centrality\", \"the_geom\",\"event\")) |&gt; \n                                              mutate(length = st_length(the_geom))\nsankey_na_centrality &lt;- pre_network_largest |&gt; mutate(centrality= rep(0, nrow(pre_network_largest)),\n                                               event = rep (\"not-used\", nrow(pre_network_largest)),\n                                               length = st_length(the_geom)) |&gt; \n                              dplyr::select(c(\"centrality\", \"the_geom\",\"event\",\"length\"))\nsankey_all_centrality &lt;- rbind(sankey_pre_centrality, sankey_post_centrality,sankey_na_centrality)\n\nsankey_all_centrality$centrality_fct &lt;- cut(sankey_all_centrality$centrality,\n                                  breaks = c(1,13,53,117,1644),\n                                  labels= c(\"[1-13]\",\"[13-53]\",\"[53-117]\",\"[117-1644]\"),\n                                  include.lowest =TRUE)\n\n# Summarize the total length for each event and centrality quartile\nsankey_post_pre &lt;-rbind(sankey_pre_centrality, sankey_post_centrality) \nsankey_post_pre$centrality_fct &lt;- cut(sankey_post_pre$centrality,\n                                  breaks = c(1,13,53,117,700,2000),\n                                  labels= c(\"[1-13]\",\"[13-53]\",\"[53-117]\",\"[117-300]\",\"[300-2000]\"),\n                                  include.lowest =TRUE)\n\nsankey_agg &lt;- sankey_post_pre %&gt;% sf::st_drop_geometry() |&gt; \n  dplyr::group_by(event, centrality_fct) %&gt;%\n  dplyr::summarise(total_length = sum(as.numeric(length))) %&gt;%\n  dplyr::ungroup() %&gt;%\n  dplyr::mutate(\n    total_length_all = sum(total_length),\n    percent = (total_length / total_length_all) * 100\n  )\n\n(unlist(dplyr::summarise(group_by(df_sankey_post_pre, event), t_length = sum(length))[1,2])/ unlist(dplyr::summarise(group_by(df_sankey_post_pre, event), t_length = sum(length))[2,2])) * unlist(dplyr::summarise(group_by(df_sankey_post_pre, event), t_length = sum(length))[1,2])\n#### oooootro intentooooo \nlibrary(forcats)\nlibrary(data.table)\nsankey_wider &lt;- sankey_post_pre |&gt; pivot_wider(names_from= event, values_from = centrality)\ndf_sankey_wider &lt;- sankey_wider |&gt; sf::st_drop_geometry() \n  \ndf_sankey_wider &lt;- df_sankey_wider |&gt; mutate(`pre-flooding` = \n                            case_when(is.na(df_sankey_wider$`pre-flooding`) ~ 0,\n                                       .default=df_sankey_wider$`pre-flooding`),\n                          `post-flooding`= case_when(is.na(df_sankey_wider$`post-flooding`) ~ 0,\n                                      .default=df_sankey_wider$`post-flooding`)\n                          )\ndf_sankey_wider_zero &lt;- df_sankey_wider |&gt; \n    mutate(pre_flooding_fct = cut(`pre-flooding`,\n                                  breaks = c(-Inf, 0, 1, 13, 53, 117, 700, 1645, Inf),\n                                  labels = c(\"[0]\", \"[1]\", \"[1-13]\", \"[13-53]\", \"[53-117]\", \"[117-300]\", \"[300-1645]\", \"[&gt;1645]\"),\n                                  include.lowest = TRUE),\n           pre_flooding_fct = forcats::fct_explicit_na(pre_flooding_fct, \"[0]\"),\n           post_flooding_fct = cut(`post-flooding`,\n                                   breaks = c(-Inf, 0, 1, 13, 53, 117, 700, 1645, Inf),\n                                   labels = c(\"[0]\", \"[1]\", \"[1-13]\", \"[13-53]\", \"[53-117]\", \"[117-300]\", \"[300-1645]\", \"[&gt;1645]\"),\n                                   include.lowest = TRUE),\n           post_flooding_fct = forcats::fct_explicit_na(post_flooding_fct, \"[0]\"))\n\n\n\nhchart(data_to_sankey(df_sankey_post_pre_wider_v4), \"sankey\", name = \"Centrality pre and post flooding\")\n\ndf_sankey_wider_zero |&gt; select(c(pre_flooding_fct, post_flooding_fct)) |&gt;  group_by(pre_flooding_fct, post_flooding_fct) |&gt; summary(n=n(), .groups= \"drop\")\n\nggplot(data = df_sankey_wider_zero,\n       aes(axis1 = pre_flooding_fct, axis2 = post_flooding_fct, \n           y = Freq)) +\n  scale_x_discrete(limits = c(\"Class\", \"Sex\", \"Age\"), expand = c(.2, .05)) +\n  xlab(\"Demographic\") +\n  geom_alluvium(aes(fill = Survived)) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  theme_minimal()\n\nlibrary(networkD3)\ndf_network &lt;- sankey_post_pre |&gt; sf::st_drop_geometry() |&gt; select(c(\"event\",\"centrality_fct\"))\ndf_network_v2 &lt;- df_network |&gt; group_by(centrality_fct, event) |&gt; summarise(n=n())\ndf_network_v2 &lt;- df_network_v2 %&gt;%\n  bind_rows(tibble(centrality_fct = \"[300-2000]\",\n                   event = \"post-flooding\",\n                   n = 0))\n\nlinks &lt;- data.frame(source=c(df_network_v2$event),\n                    target = c(df_network_v2$centrality_fct),\n                    value = c(df_network_v2$n))\n\nnodes &lt;- data.frame(\n  name=c(as.character(links$source), \n  as.character(links$target)) %&gt;% unique()\n)\n\nlinks$IDsource &lt;- match(links$source, nodes$name)-1 \nlinks$IDtarget &lt;- match(links$target, nodes$name)-1\n\n## nice gepeto\n\n  \n\ndf_long &lt;- df_network_v2 |&gt; \n    ungroup() |&gt; \n    add_row(centrality_fct = \"[300-2000]\", event = \"post-flooding\", n = 0) |&gt; \n    pivot_wider(names_from = event, values_from = n) |&gt; \n    mutate(alluvium = row_number())\n\nggplot(data = df_long,\n       aes(axis1 = centrality_fct, axis2 = event, y = n)) +\n  geom_alluvium(aes(fill = event)) +\n  geom_stratum() +\n geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_discrete(limits = c(\"Pre-Centrality\", \"Post-Centrality\")) +\n  theme_minimal() +\n  ggtitle(\"Centrality Analysis Before and After Flooding\",\n          \"Comparing centrality distributions across events\")\n\n\nggplot(df_long,\n       aes(axis1 = `pre-flooding`, axis2 = alluvium, y = `post-flooding`)) +\n  geom_alluvium(aes(fill = centrality_fct)) +\n  geom_stratum(width = 1/6) +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_discrete(limits = c(\"Centrality\", \"Alluvium\"),\n                   expand = c(.1, .1)) +\n  labs(title = \"Alluvial Plot of Centrality Pre- and Post-Flooding\",\n       y = \"Number of Roads\", x = \"Centrality Ranges\") +\n  theme_minimal()\n\ndf_long &lt;- df_long %&gt;%\n  pivot_longer(cols = c(`post-flooding`, `pre-flooding`), \n               names_to = \"event\", \n               values_to = \"n\")\n\ndf_long &lt;- df_sankey_wider_zero |&gt; \n  pivot_longer(cols = c(`pre-flooding`, `post-flooding`), \n               names_to = \"event\", \n               values_to = \"n\")\nggplot(df_long, aes(axis1 = pre_flooding_fct, \n                    axis2 = post_flooding_fct, \n                    y = n)) +\n  geom_alluvium(aes(fill = event), width = 1/12) +\n  geom_stratum() +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum)), \n            size = 3, color = \"white\") +\n  scale_x_discrete(limits = c(\"Pre-Flooding Centrality\", \"Post-Flooding Centrality\"),\n                   expand = c(0.05, 0.05)) +\n  labs(title = \"Alluvial Diagram of Centrality Changes\",\n       x = \"Centrality Categories\",\n       y = \"Number of Roads (n)\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n\n\n\n\n\nRegarding the distribution of the centrality, the histogram on the figure 1 represented the betweenness centrality on a log10 scale on the X-axis, while the Y axis shown the frequency of occurence for both scenarios. In both scenarios, the centrality distribution exibited strong positive skewness, with most of the values concentrated at the lower values of the edge betweenness. It is observed that the frequency of mid-range centrality values between 10 and 100 increased after the flooding event, however, the tail with larger values after 1000 was no longer present. The extreme positive centrality values found only in the pre-disaster network affected the general centrality lowering the mean.\n\n\nR: Histogram of betweenness centrality calculated with pgrouting\nlibrary(plyr)\n# Tidy data and wrangling\ncentrality_post$event &lt;- \"post-flooding\"\ncentrality_pre$event &lt;- \"pre-flooding\"  \ncentrality_both &lt;- rbind(centrality_post[,c(\"id\",\"centrality\",\"event\")] ,\n                                   centrality_pre[,c(\"id\",\"centrality\",\"event\")])\nmu &lt;- plyr::ddply(centrality_both, \"event\", summarise, grp.mean=mean(centrality))\n\n# Create histogram\nggplot(centrality_both, aes(x=centrality, fill=event)) +\n                    geom_histogram(alpha=0.4,) + scale_x_log10() +\n                  labs(title = \"Centrality analysis\",\n                      subtitle= \"Histogram on log10 scale\",\n                      x = \" Betweenness centrality (centrality)\",\n                      y = \"Frequency (count)\") +\n  theme(plot.title=element_text(family =\"bold\", hjust=0.5),\n        plot.subtitle = element_text(colour=\"#626262\", hjust=0.5),\n        legend.position='bottom') +\n    theme_minimal()\n\n\n\n\n\nThe following grouped bar plot illustrates the sum of the edge betweenness values classified by their quantiles for both events, before and after the flooding. The X-Axis is categorized in the four quantiles, while yhe y-axis summed up the betweenness values of these quartiles. In Both events a similar pattern is found, the roads belonging to the Q4 caused most of the edge beetweenness of the network. The 1344 km that comprised all the roads below the fourth quartile only contributed to 30 for the preflooding scenario. Therefore, the findings shown an positive skewed distribution, where a small proportion of roads were responsible for the roads held responsible for the 69% of the centrality of the network.\n\n\nR: Betweenness centrality values grouped by their quantiles calculated with pgrouting\ndf_centrality_pre &lt;- centrality_pre |&gt;\n                        sf::st_drop_geometry() |&gt;\n                        mutate(event = \"pre-flooding\") \ndf_centrality_post &lt;- centrality_post |&gt; \n                          sf::st_drop_geometry() |&gt;\n                              mutate(event = \"post-flooding\")\n##\ndf_centrality_both_barplot &lt;- dplyr::bind_rows(df_centrality_pre,\n                                               df_centrality_post)\n### percentiles\nq &lt;- quantile(df_centrality_both_barplot$centrality)\nq_pre &lt;- quantile(\n  df_centrality_pre[df_centrality_pre$event == 'pre-flooding',]$centrality)\nq_post &lt;- quantile(\n  df_centrality_post[df_centrality_post$event == 'post-flooding',]$centrality)\n### Centrality\ndf_centrality_both_barplot_cat &lt;- na.omit(df_centrality_both_barplot) |&gt; \n  mutate(centrality_cat=as.factor(case_when(\n    centrality &lt;= q_pre[2] ~ \"Q1\",\n    centrality &lt;= q_pre[3] ~ \"Q2\",\n    centrality &lt;= q_pre[4] ~ \"Q3\",\n    centrality &gt;= q_pre[4] ~ \"Q4\",\n    TRUE ~ \"missing\")))  \n## tidy data\nbarplot_event_cat_sum &lt;- df_centrality_both_barplot_cat |&gt; \n                          dplyr::group_by(event, centrality_cat) |&gt;\n                           dplyr::summarise(sum_centrality = sum(centrality))\n# basic plot\n barplot_event_cat_sum |&gt;  \n  ggplot(aes(x=centrality_cat,\n              y=sum_centrality,\n              fill=event)) +\n  geom_col(width=0.5, position=\"dodge\") +\n  labs(title = \"Centrality analysis\",\n       subtitle= \"Barplot grouped by the post and pre event\",\n       x = \" Quantiles\",\n       y = \"Edge betweenness\") +\n  theme(plot.title=element_text(family =\"bold\", hjust=0.5),\n        plot.subtitle = element_text(colour=\"#626262\", hjust=0.5),\n        legend.position='bottom') +\n     theme_minimal()"
  },
  {
    "objectID": "results.html#rq2-accessibility-analysis",
    "href": "results.html#rq2-accessibility-analysis",
    "title": "3  Results",
    "section": "3.3 RQ2: Accessibility analysis",
    "text": "3.3 RQ2: Accessibility analysis\n\nWhich healthcare facilities will be most affected by flooding based on accessibility metrics?\n\n\n3.3.1 Sum Edge Betweenness:\n\n\nR: Visualizing the accesibility most affected from healtchare facilities based on betweenness centrality\n## Eisenberg\nhospitals_betweenness_pre &lt;- st_read(eisenberg_connection, \"centrality_424_hospitals_porto_end_id_centrality\") \nhospitals_betweenness_post &lt;- st_read(eisenberg_connection,\"centrality_424_hospitals_porto_end_id_centrality_post\")\n### Locally\nhospitals_betweenness_pre_local &lt;- st_read(eisenberg_connection, Id(schema=\"heigit\", table = \"centrality_424_hospitals_porto_end_id_centrality_group\"))\nhospitals_betweenness_post_local &lt;- st_read(eisenberg_connection, Id(schema=\"heigit\", table = \"centrality_424_hospitals_porto_end_id_centrality_post_group\"))\nhospitals_betweenness_pre_local$event &lt;- \"pre-event\"\nhospitals_betweenness_post_local$event &lt;- \"post-event\"\n## Import\nhospitals_betweenness_both_long &lt;- hospitals_betweenness_pre_local |&gt; bind_rows(hospitals_betweenness_post_local) |&gt; subset(select=c(\"cd_cnes\",\"ds_cnes\",\"max_centrality\",\"event\")) |&gt; pivot_wider(\n    names_from = event,\n    values_from = max_centrality\n  ) |&gt;  rename( betwenness_pre= `pre-event`,\n                betwenness_post= `post-event`) \nhospitals_betweenness_both_long[is.na(hospitals_betweenness_both_long)] &lt;- 0    \n## pivot\nhospitals_betweenness_long &lt;- hospitals_betweenness_both_long |&gt;\n                                  mutate(diff_betweenness_percent =\n                                           (betwenness_post-betwenness_pre)/(betwenness_pre)*100,\n                                         diff = betwenness_post-betwenness_pre,\n                                         letter = LETTERS[1:22])  |&gt;\n  arrange(letter) |&gt; \n                             pivot_longer(\n                                     c(betwenness_pre,betwenness_post),\n                                      names_to=\"betweenness_type\",\n                                      values_to=\"betweenness_value\") \n                   \nhospitals_betweenness_long_pre &lt;- \n  hospitals_betweenness_long |&gt;\n            filter(betweenness_type == \"betwenness_pre\")\nhospitals_betweenness_long_post &lt;- \n  hospitals_betweenness_long |&gt;\n  filter(betweenness_type == \"betwenness_post\")\n##\n\ndf_labels_betweenness &lt;- hospitals_betweenness_long |&gt; \n  group_by(letter) |&gt; \n  summarize(midpoint = mean(betweenness_value), \n            diff_betweenness_percent = first(diff_betweenness_percent))\n##\np &lt;- ggplot(hospitals_betweenness_long, aes(x=betweenness_value, y=reorder(letter, desc(diff_betweenness_percent)))) +\n  geom_segment(data=hospitals_betweenness_long_pre,\n               aes(x=betweenness_value, y = letter,\n                   yend=hospitals_betweenness_long_post$letter, xend=hospitals_betweenness_long_post$betweenness_value)) +\n  geom_point(aes(x=betweenness_value, y = letter, color = betweenness_type, size=2.5))  +\n  geom_text(data = df_labels_betweenness, aes(x = midpoint, y = letter, label = paste0(round(diff_betweenness_percent,2), \" %\")), \n            vjust = 1.5, size = 3.5, color = \"red\") +\n    geom_text(aes(label = betweenness_value, color = betweenness_type), vjust = -1, size=3.5) +  # Label the points with centrality_value\n   scale_color_brewer(palette = \"Set1\", direction = 1) +\n  scale_y_discrete(expand=c(0.05,0.05)) +\n  theme_minimal() +\n  labs(title = \"Centrality analysis\",\n       subtitle= \"Change of centrality based on sum edge betweenness in hospitals\",\n       y = \" Hospitals\",\n       x = \"Sum Edge Betweenness\") +\n  theme(plot.title=element_text(family =\"bold\", hjust=0.5),\n        axis.text.y = element_text(size = 16),\n        plot.subtitle = element_text(colour=\"#626262\", hjust=0.5),\n        legend.position='bottom') \n\n### table\n\ndf_hospitals_betweenness_both_compare &lt;- hospitals_betweenness_both_long |&gt; \n                                              mutate(diff_betweenness_percent = \n                                                          round((betwenness_post-betwenness_pre)/(betwenness_pre)*100,2),\n                                                      diff = betwenness_post-betwenness_pre,\n                                                      letter = LETTERS[1:22],\n                                                      ds_cnes = stringr::str_to_title(ds_cnes))\n                                                  \n  \nDT::datatable(subset(df_hospitals_betweenness_both_compare,\n                     select=c(\"letter\",\"cd_cnes\",\"ds_cnes\",\"betwenness_pre\",\"betwenness_post\",\"diff_betweenness_percent\")),\n              colnames=c(\"ID\",\"Code\",\"Name\",\"Betweenness Pre\",\"Betweenness Post\",\"Diff(%)\"),\n              filter=\"top\",\n              extensions=\"Buttons\",\n                         options=list(\n                           dom=\"Bfrtip\",\n                           buttons=c(\"copy\",\"csv\",\"pdf\"),\n                            initComplete = JS(\n    \"function(settings, json) {\",\n    \"$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});\",\n    \"}\")\n                                  )\n              ) |&gt; \n    DT::formatStyle(\"betwenness_pre\",\n              background=DT::styleColorBar(range(df_hospitals_betweenness_both_compare$betwenness_pre),'#ee8b8b'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center') |&gt;\n    DT::formatStyle(\"betwenness_post\",\n              background=DT::styleColorBar(range(df_hospitals_betweenness_both_compare$betwenness_pre),'#8ba1b3'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center')\n\n\n\n\n\nR: Changes on the accesibiliy based on closeness\nhospitals_closeness_both &lt;- st_read(eisenberg_connection, \"hospitals_closeness_both\") \ndf_hospitals_closeness_both_compare &lt;- hospitals_closeness_both |&gt; \n                                              mutate(diff_closeness_percent = \n                                                          round((closeness_post-closeness_pre)/(closeness_pre)*100,2),\n                                                      diff = closeness_post-closeness_pre,\n                                                      letter = LETTERS[1:20],\n                                                      ds_cnes = stringr::str_to_title(ds_cnes)) |&gt; \n                                                  sf::st_drop_geometry() \n                                                  \n  \nDT::datatable(subset(df_hospitals_closeness_both_compare,\n                     select=c(\"letter\",\"cd_cnes\",\"ds_cnes\",\"closeness_pre\",\"closeness_post\",\"diff_closeness_percent\")),\n              colnames=c(\"ID\",\"Code\",\"Name\",\"Closeness Pre\",\"Closeness Post\",\"Diff(%)\"),\n              filter=\"top\",\n              extensions=\"Buttons\",\n                         options=list(\n                           dom=\"Bfrtip\",\n                           buttons=c(\"copy\",\"csv\",\"pdf\"),\n                            initComplete = JS(\n    \"function(settings, json) {\",\n    \"$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});\",\n    \"}\")\n                                  )\n              ) |&gt; \n    DT::formatStyle(\"closeness_pre\",\n              background=DT::styleColorBar(range(df_hospitals_closeness_both_compare$closeness_pre),'#ee8b8b'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center') |&gt;\n    DT::formatStyle(\"closeness_post\",\n              background=DT::styleColorBar(range(df_hospitals_closeness_both_compare$closeness_pre),'#8ba1b3'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center')\n\n\n\n\n\n\n\n\n\n3.3.2 Closeness:\n\n\nR: Visualizing closeness of the healthcare facilities\n# Import he data\n## hospital with closenesss values\nclosseness &lt;-sf::st_read(eisenberg_connection, \n                            layer = \"clossness_hospital_porto\")\nclosseness_df &lt;- closseness |&gt;  arrange(ds_cnes, closeness) |&gt;\n                    mutate(lng= \n                              unlist(map(geom_hospital,1)),\n                           lat=\n                              unlist(map(geom_hospital,2)),\n                           closeness_norm = \n                          (closseness$closeness - min(closseness$closeness)) / (max(closseness$closeness) - min(closseness$closeness)) * 100,\n                          position = rank(-closeness))\n## Create Color palette for visualization\npal &lt;- colorQuantile(palette = \"OrRd\",closseness_df$closeness, n=4 )\n\n## Create leaflet product\n\nicons &lt;- makeAwesomeIcon(\n  icon = 'fa-heartbeat',\n  iconColor = \"#FFFFFF\",\n  markerColor = \"#57142c\",\n  library = \"fa\"\n)\nleaflet(closseness_df) |&gt;\n    addProviderTiles(providers$OpenStreetMap.HOT) |&gt;\n    addCircles(data =closseness_df , radius = ~sqrt(closeness)*10, fillOpacity = .50, color =~pal(closeness)) |&gt;\n  addAwesomeMarkers(data=closseness_df,\n                          icon =icons,\n                          popup= ~paste0(\"&lt;b&gt;Código CNES: &lt;/b&gt;\", cd_cnes, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Nome: &lt;/b&gt;\", ds_cnes, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt; Closeness&lt;/b&gt;:\", closeness, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Longitude: &lt;/b&gt;\", lng, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Posição &lt;/b&gt;\", position, \"&lt;br/&gt;\"))\n\n\n\n\n\n\n\n\nR: Creating the table showing closeness values\nclosseness &lt;-sf::st_read(eisenberg_connection, \n                            layer = \"clossness_hospital_porto\")\nclosseness_no_geom &lt;- closseness |&gt;  \n                          arrange(ds_cnes, closeness) |&gt;\n                          mutate(lng= \n                              unlist(map(geom_hospital,1)),\n                           lat=\n                              unlist(map(geom_hospital,2)),\n                           closeness_norm = \n                          (closseness$closeness - min(closseness$closeness)) / (max(closseness$closeness) - min(closseness$closeness)) * 100,\n                          position = rank(-closeness),\n                          ds_cnes =stringr::str_to_title(ds_cnes)) |&gt;\n                  sf::st_drop_geometry()\n\nDT::datatable(subset(closseness_no_geom, select=c(\"position\",\"cd_cnes\",\"ds_cnes\",\"closeness\")),\n              extensions=\"Buttons\",\n                         options=list(\n                           dom=\"Bfrtip\",\n                           buttons=c(\"copy\",\"csv\",\"pdf\"),\n                            initComplete = JS(\n    \"function(settings, json) {\",\n    \"$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});\",\n    \"}\")\n                                  )\n              ) |&gt; \n    DT::formatStyle(\"closeness\",\n              background=DT::styleColorBar(range(closseness_no_geom$closeness),'#ee8b8b'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center')"
  },
  {
    "objectID": "results.html#rq3-critical-infrastructures",
    "href": "results.html#rq3-critical-infrastructures",
    "title": "3  Results",
    "section": "3.4 RQ3: Critical infrastructures",
    "text": "3.4 RQ3: Critical infrastructures\n\nWhere are the most critical infrastructures located for accessing health facilities to reinforce urban resilience against flooding?\n\n\n\n\n\nEuropean Commission. Joint Research Centre. 2023. GHSL Data Package 2023. LU: Publications Office. https://data.europa.eu/doi/10.2760/098587."
  },
  {
    "objectID": "conclussion.html#framework",
    "href": "conclussion.html#framework",
    "title": "4  Conclussion",
    "section": "4.1 Framework",
    "text": "4.1 Framework"
  },
  {
    "objectID": "conclussion.html#data",
    "href": "conclussion.html#data",
    "title": "4  Conclussion",
    "section": "4.2 Data",
    "text": "4.2 Data"
  },
  {
    "objectID": "conclussion.html#sampling-points",
    "href": "conclussion.html#sampling-points",
    "title": "4  Conclussion",
    "section": "4.3 Sampling points",
    "text": "4.3 Sampling points"
  },
  {
    "objectID": "conclussion.html#network",
    "href": "conclussion.html#network",
    "title": "4  Conclussion",
    "section": "4.4 Network",
    "text": "4.4 Network"
  },
  {
    "objectID": "future_work.html#point-of-interest-poi",
    "href": "future_work.html#point-of-interest-poi",
    "title": "5  Future work & suggestions",
    "section": "5.1 Point of interest (POI)",
    "text": "5.1 Point of interest (POI)"
  },
  {
    "objectID": "future_work.html#data",
    "href": "future_work.html#data",
    "title": "5  Future work & suggestions",
    "section": "5.2 Data",
    "text": "5.2 Data\n\n\nShow the code\ndata.frame(\n  analysis = c(\"Vulnerability index\"),\n  problem =c(\"Results and conclussions based on aggregated data can be limited and biased\"),\n  suggestion =c(\"Find non-spatial data that can be added using municipality, neighbor or sector ID\")\n)\n\n\n             analysis\n1 Vulnerability index\n                                                                      problem\n1 Results and conclussions based on aggregated data can be limited and biased\n                                                                         suggestion\n1 Find non-spatial data that can be added using municipality, neighbor or sector ID\n\n\n\n5.2.1 Bairros\n\n\nShow the code\n# import the data : shp2pgsql -D -I -s 4674 -W \"LATIN1\" '43SEE250GC_SIR.shp' barrios_brasil | psql -p 25432 -U docker -d gis -h localhost\n\nCREATE TABLE brasil_porto_barrios AS\nSELECT \n    nm_bairro,\n    st_union(brasil_barrios_4326.geom_4326)\nFROM\n    brasil_barrios_4326,\n    ghs_aoi_porto_alegre\nWHERE \n brasil_barrios_4326.nm_bairro IS NOT NULL\n AND\n    st_intersects(brasil_barrios_4326.geom_4326,\n                ghs_aoi_porto_alegre.geom)\nGROUP BY \n    nm_bairro;"
  },
  {
    "objectID": "future_work.html#perforamance",
    "href": "future_work.html#perforamance",
    "title": "5  Future work & suggestions",
    "section": "5.3 Perforamance",
    "text": "5.3 Perforamance\n\n5.3.1 Flood mask\nUsing the dilate and erode method described in the slide 95/187 (link) could remove small islands that increase the computational costs. A use case of this technique is observed when simplifying coastlines (link)\n\n\n5.3.2 Buildings\n\n5.3.2.1 Download from Overture\n\n\nImport buildings and roads from Overture\n## Import data\nlibrary(overturemapsr)\nghs &lt;- st_read(eisenberg_connection, \"ghs_aoi_porto_alegre\")\noverture_roads &lt;- record_batch_reader(overture_type = 'segment', bbox = sf::st_bbox(ghs))\noverture_buildings &lt;- record_batch_reader(overture_type = 'building', bbox = sf::st_bbox(ghs))\n## Subset data\noverture_roads_subset &lt;- overture_roads |&gt; dplyr::select(c(id, geometry,class,update_time, subtype))\n## Export data\n### Local file\nlibrary(arrow)\nlibrary(sfarrow)\nsfarrow::st_write_parquet(overture_roads, \"overture_roads.parquet\")\n### Database\nlibrary(DBI)\nlibrary(nanoarrow)\nDBI::dbWriteTable(eisenberg_connection, \"overture_roads_subset\", overture_roads_subset)\n\n\n\n\n5.3.2.2 Processing in duckdb\n\n\nShow the code\n--- Using Duckdb\nINSTALL spatial;\nLOAD spatial;\n----- Importing buildings & flood extent from file to duckdb\n\n---- Buildings\nCREATE TABLE porto_buildings AS\n    SELECT \n        id,\n        st_geomfromwkb(geometry) as geom\n    FROM \n        'porto_buildings_overture.parquet';\n        \n--- Floodig      \nCREATE TABLE  flood AS\n    SELECT\n        * \n    FROM \n      st_read('flooding_porto_cleaned.geojson');\n---- Filtering building in flood extent\nCREATE TABLE flood_building AS \n    SELECT \n      buildings.*\n    FROM\n      porto_buildings AS buildings,\n      flood \n    WHERE\n      st_intersects(buildings.geom, flood.geom);\n--- Export data\nCOPY flood_building TO\n  'flood_building_v2.gpkg' WITH (FORMAT GDAL, DRIVER 'GPKG');\nCOPY porto_buildings TO \n   'porto_buildings.gpkg' WITH (FORMAT GDAL, DRIVER 'GPKG');\n\n\n\n\n\n5.3.3 Alternative route\n\n\nShow the code\n\nCREATE TABLE second_path_net AS\nWITH first_path AS (\nSELECT * FROM pgr_dijkstra(\n            'SELECT id,\n                     source,\n                     target, \n                     cost \n            FROM porto_alegre_net_pre_component_one',\n            7249, 7268,\n            true))\nSELECT \n    * \nFROM \n    porto_alegre_net_pre_component_one\nWHERE\n    porto_alegre_net_pre_component_one.id NOT IN (select edge from first_path);\n\nSELECT * FROM pgr_dijkstra(\n            'SELECT id,\n                     source,\n                     target, \n                     cost \n            FROM second_path_net',\n            7249, 7268,\n            true)\n-----2\n\n\nCREATE TABLE pkt_2000 AS\nWITH hospital AS (SELECT \n    h.ds_cnes,\n    net.id,\n    h.geom\nFROM hospital_rs_geoportal as h\nLEFT JOIN LATERAL\n    (SELECT \n        id,\n        the_geom\n     FROM\n     porto_alegre_net_pre_component_one_vertices_pgr as net\n     ORDER BY h.geom &lt;-&gt; net.the_geom\n     LIMIT 1) AS net ON TRUE\nWHERE \n    h.ds_cnes = 'IRMANDADE DA SANTA CASA DE MISERICORDIA DE PORTO ALEGRE'),\norigin AS(SELECT * FROM porto_alegre_net_pre_component_one_vertices_pgr \nWHERE  id = 7268)\nSELECT j.path_id, j.node, j.edge, b.cost, b.the_geom FROM pgr_KSP(\n                        'SELECT id,\n                            source,\n                            target,\n                            cost \n                        FROM porto_alegre_net_pre_component_one',\n                        (SELECT id FROM hospital),\n                        (SELECT id FROM origin),\n                        2000) as j\n                        left JOIN porto_alegre_net_pre_component_one AS b\n                  ON j.edge = b.id;\n\nCREATE TABLE road_without_first AS \nSELECT \n    * \nFROM \n    porto_alegre_net_pre_component_one  \nWHERE \n    id NOT IN (SELECT edge FROM pkt_2000 WHERE path_id = 1);\n    \n    -----3\n    WITH paths AS (\n  SELECT \n    path_id,\n    edge,\n    agg_cost,\n    SUM(agg_cost) OVER (PARTITION BY path_id ORDER BY seq ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) AS cum_sum,\n    SUM(agg_cost) OVER (PARTITION BY path_id) AS total_cost\n  FROM \n    your_table\n),\nfiltered_paths AS (\n  SELECT \n    path_id,\n    edge,\n    agg_cost,\n    cum_sum,\n    total_cost\n  FROM \n    paths\n  WHERE \n    cum_sum &gt; 0.25 * total_cost -- remove the first 25%\n    AND cum_sum &lt; 0.75 * total_cost -- remove the last 25%\n)\nSELECT \n  path_id, \n  edge, \n  agg_cost \nFROM \n  filtered_paths;\n  \n  ----5\n  WITH hospital AS (SELECT \n    h.ds_cnes,\n    net.id,\n    h.geom\nFROM hospital_rs_geoportal as h\nLEFT JOIN LATERAL\n    (SELECT \n        id,\n        the_geom\n     FROM\n     porto_alegre_net_pre_component_one_vertices_pgr as net\n     ORDER BY h.geom &lt;-&gt; net.the_geom\n     LIMIT 1) AS net ON TRUE\nWHERE \n    h.ds_cnes = 'IRMANDADE DA SANTA CASA DE MISERICORDIA DE PORTO ALEGRE'),\norigin AS(SELECT\n                * \n          FROM \n                porto_alegre_net_pre_component_one_vertices_pgr \nWHERE  id = 7268),\ntable_ksp AS (\nSELECT j.seq, j.path_id, j.path_seq, j.node, j.edge, j.agg_cost,j.cost,\n    b.the_geom, b.target, b.source, b.id FROM pgr_KSP(\n                        'SELECT id,\n                            source,\n                            target,\n                            cost \n                        FROM porto_alegre_net_pre_component_one',\n                        (SELECT id FROM hospital),\n                        (SELECT\n                            id \n                        FROM \n                            origin,\n                            extract_ksp_segments\n                        WHERE\n                            origin.id NOT IN (\n                                SELECT \n                                    id \n                                FROM \n                                    extract_ksp_segments\n                                WHERE\n                                    path_id = 1)),\n                        3) as j\n                        left JOIN porto_alegre_net_pre_component_one AS b\n                  ON j.edge = b.id)\nSELECT  * FROM table_ksp;\n\n\nThis second one does work without creating two segments of networks. It just increased the costs dinamically. This is inspired by post and post\n\n\nShow the code\n---- Obtain the shortest path as usual\nCREATE TABLE first_path_dijkstra AS\nSELECT \n        seq,\n        path_seq,\n        start_vid,\n        end_vid,\n        node,\n        edge,\n        net.the_geom \nFROM  pgr_dijkstra('\n                SELECT \n                        id,\n                        source,\n                        target,\n                        cost\n                FROM porto_alegre_net_largest',\n                9372,\n                1084) AS path\n    LEFT JOIN porto_alegre_net_pre_component_one AS net\n    ON path.edge = net.id;\n---- detected the shortest path and then increased cost two fold\nCREATE TABLE dijkstra_second_route AS\nWITH pgr_ksp_table AS (\nSELECT \n    *\nFROM  pgr_ksp('\n                SELECT \n                        id,\n                        source,\n                        target,\n                        cost\n                FROM porto_alegre_net_largest',\n                9372,\n                1084,\n                3,\n                true)), ----node 9372, network = source, target)\nsecond_route AS (\n            SELECT \n            net.*,\n            CASE \n                WHEN route.node = net.source THEN net.cost * 2\n                ELSE net.cost\n            END AS cost_updated\n            FROM            \n                porto_alegre_net_largest AS net\n            LEFT JOIN \n                pgr_ksp_table  AS route \n            ON  \n                route.edge= net.id) \n        SELECT * FROM second_route;\n\n---- Create spatial index   \nCREATE INDEX idx_ ON dijkstra_second_route USING gist(the_geom);\nCREATE INDEX dijkstra_second_route_source_source ON dijkstra_second_route USING btree(source);\nCREATE INDEX dijkstra_second_route_source_target ON dijkstra_second_route USING btree(target);\n\n---- obtain the second shortest path, since the first-path has now higher \nCREATE TABLE second_path_dikstra AS\nSELECT \n            seq,\n            path_seq,\n            start_vid,\n            end_vid,\n            node,\n            edge,\n            net.the_geom \n        FROM \n            pgr_Dijkstra('\n                    SELECT\n                        id,\n                        source,\n                        target,\n                        cost_updated AS cost\n                    FROM\n                        dijkstra_second_route',\n                        9372,\n                        1084)  AS path\n        LEFT JOIN\n                dijkstra_second_route AS net ON\n            path.edge = net.id\n\n\nUsing this strategy, the following code create 3 alternative routes obtaining the final table “paths_linestring_table”.\n\n\nShow the code\nCREATE TABLE first_path_6211 AS\nSELECT \n        seq,\n        path_seq,\n        start_vid,\n        end_vid,\n        node,\n        edge,\n        net.the_geom,\n        1 AS path\nFROM  pgr_dijkstra('\n                SELECT \n                        id,\n                        source,\n                        target,\n                        cost\n                FROM porto_alegre_net_largest',\n                6211,\n                ARRAY(SELECT id FROM hospital_rs_node_v2),\n                directed := TRUE) AS path\n    LEFT JOIN porto_alegre_net_largest AS net\n    ON path.edge = net.id;\n---- detected the shortest path and then increased cost two fold\n\nDROP TABLE dijkstra_second_route_6211;\nCREATE TABLE dijkstra_second_route_6211 AS\nWITH pgr_doubled_dijkstra AS (\nSELECT \n    *\nFROM  pgr_dijkstra('\n                SELECT \n                        id,\n                        source,\n                        target,\n                        cost\n                FROM porto_alegre_net_largest',\n                6211,\n                ARRAY(SELECT id FROM hospital_rs_node_v2),\n                directed:=true)), ----node 9372, network = source, target)\nsecond_route AS (\n            SELECT \n            net.*,\n            CASE \n                WHEN route.node = net.source THEN net.cost * 20\n                ELSE net.cost\n            END AS cost_updated\n            FROM            \n                porto_alegre_net_largest AS net\n            LEFT JOIN \n                pgr_doubled_dijkstra  AS route \n            ON  \n                route.edge= net.id) \n        SELECT * FROM second_route;\n\n---- Create spatial index   \nCREATE INDEX idx_second_6211 ON dijkstra_second_route_6211 USING gist(the_geom);\nCREATE INDEX idx_second_6211_source ON dijkstra_second_route_6211 USING btree(source);\nCREATE INDEX idx_second_6211_target ON dijkstra_second_route_6211 USING btree(target);\n\n---- obtain the second shortest path, since the first-path has now higher \nDROP TABLE second_path_dikstra_6211;\nCREATE TABLE second_path_dikstra_6211 AS\nSELECT \n            seq,\n            path_seq,\n            start_vid,\n            end_vid,\n            node,\n            edge,\n            net.the_geom,\n            2 AS path\n        FROM \n            pgr_Dijkstra('\n                    SELECT\n                        id,\n                        source,\n                        target,\n                        cost_updated AS cost\n                    FROM\n                        dijkstra_second_route_6211',\n                    6211,\n                    ARRAY(SELECT id FROM hospital_rs_node_v2))  AS path\n        LEFT JOIN\n                dijkstra_second_route_6211 AS net ON\n            path.edge = net.id;\n            \n            select * from second_path_dikstra_6211;\n        \n        \n        \n----- 3 path\nSELECT * FROM dijkstra_second_route_6211;       \n\nDROP TABLE dijkstra_third_route_6211;\nCREATE TABLE dijkstra_third_route_6211 AS\nWITH pgr_doubled_dijkstra AS (\nSELECT \n    *\nFROM  pgr_dijkstra('\n                SELECT \n                        id,\n                        source,\n                        target,\n                        cost_updated AS cost\n                FROM dijkstra_second_route_6211',\n                6211,\n                ARRAY(SELECT id FROM hospital_rs_node_v2),\n                directed:=true)), ----node 9372, network = source, target)\nsecond_route AS (\n            SELECT \n            net.*,\n            CASE \n                WHEN route.node = net.source THEN net.cost_updated * 20\n                ELSE net.cost_updated\n            END AS cost_updated_nd\n            FROM            \n                dijkstra_second_route_6211 AS net\n            LEFT JOIN \n                pgr_doubled_dijkstra  AS route \n            ON  \n                route.edge= net.id) \n        SELECT * FROM second_route;\n    ----\n\nDROP TABLE third_path_dikstra_6211 ;\nCREATE TABLE third_path_dikstra_6211 AS\nSELECT \n            seq,\n            path_seq,\n            start_vid,\n            end_vid,\n            node,\n            edge,\n            net.the_geom,\n            3 AS path\n        FROM \n            pgr_Dijkstra('\n                    SELECT\n                        id,\n                        source,\n                        target,\n                        cost_updated_nd AS cost\n                    FROM\n                        dijkstra_third_route_6211',\n                    6211,\n                    ARRAY(SELECT id FROM hospital_rs_node_v2))  AS path\n        LEFT JOIN\n                dijkstra_third_route_6211 AS net ON\n            path.edge = net.id;\n            \n            select * from dijkstra_third_route_6211;    \n        \n\nCREATE TABLE three_alternative_paths AS \nSELECT * FROM first_path_6211\nUNION\nSELECT * FROM second_path_dikstra_6211\nUNION\nSELECT * FROM third_path_dikstra_6211\n\nCREATE TABLE three_alternative_paths_20 AS \nSELECT * FROM first_path_6211\nUNION\nSELECT * FROM second_path_dikstra_6211\nUNION\nSELECT * FROM third_path_dikstra_6211\n\nCREATE TABLE ernesto_three_alternatives AS\nSELECT\nalternative_paths.*,\nhospitals.ds_cnes,\nhospitals.cd_cnes\nFROM \nthree_alternative_paths AS alternative_paths\nJOIN hospital_rs_node_v2 AS hospitals\nON hospitals.id = alternative_paths.end_vid\nWHERE end_vid = 1743;\n\n\nCREATE TABLE ernesto_three_alternatives_20 AS\nSELECT\nalternative_paths.*,\nhospitals.ds_cnes,\nhospitals.cd_cnes\nFROM \nthree_alternative_paths_20 AS alternative_paths\nJOIN hospital_rs_node_v2 AS hospitals\nON hospitals.id = alternative_paths.end_vid\nWHERE end_vid = 1743;\n\n\nCREATE TABLE paths_linestring_table AS\nWITH paths_linestring AS (\nSELECT\n    start_vid,\n    end_vid,\n    path,\n    st_union(the_geom) AS line_geom\nFROM \n    three_alternative_paths_20\nGROUP BY path, start_vid, end_vid)\nSELECT \n    paths_linestring.*,\n    hospitals.ds_cnes,\n    round(st_length(line_geom::geography)::numeric/1000,2) AS distance\nFROM \n    paths_linestring\nJOIN hospital_rs_node_v2 AS hospitals\nON hospitals.id = paths_linestring.end_vid\n\n\nThis code created the table to represent these values\n\n\nShow the code\nalternative_routes &lt;- st_read(eisenberg_connection ,\"paths_linestring_table\")\n\n\nlibrary(gtExtras)\nalternative_routes |&gt; arrange(distance) |&gt; \n      st_drop_geometry() |&gt;\n      select(c(ds_cnes, path, distance)) |&gt; \n      mutate(ds_cnes = as_factor(str_to_title(ds_cnes))) |&gt;\n      group_by(ds_cnes) |&gt; \n      mutate(sd = sd(distance)) |&gt; \n    gt(groupname_col=\"ds_cnes\")"
  },
  {
    "objectID": "appendix.html#section",
    "href": "appendix.html#section",
    "title": "Appendix A — Performance tests",
    "section": "A.1 10",
    "text": "A.1 10\n\n\nShow the code"
  },
  {
    "objectID": "appendix.html#section-1",
    "href": "appendix.html#section-1",
    "title": "Appendix A — Performance tests",
    "section": "A.2 50",
    "text": "A.2 50\n\n\nShow the code"
  },
  {
    "objectID": "appendix.html#section-2",
    "href": "appendix.html#section-2",
    "title": "Appendix A — Performance tests",
    "section": "A.3 100",
    "text": "A.3 100\n\n\nShow the code"
  },
  {
    "objectID": "appendix.html#section-3",
    "href": "appendix.html#section-3",
    "title": "Appendix A — Performance tests",
    "section": "A.4 200",
    "text": "A.4 200\n\n\nShow the code\n---- Create 200 origin\nCREATE TABLE sampling_weight_200_origin  AS\nwith porto_200_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_origin;  \n---- Create 200 destination\nCREATE TABLE sampling_weight_200_destination  AS\nwith porto_200_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_destination;  \n---- Create index for origin\nCREATE INDEX idx_sampling_weight_200_origin_net_id ON sampling_weight_200_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_origin_geom ON sampling_weight_200_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_200_destination_net_id ON sampling_weight_200_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_destination_geom ON sampling_weight_200_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_200_origin;\nVACUUM(full, ANALYZE) sampling_weight_200_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n--- Run query\n\nEXPLAIN ANALYZE\n CREATE TABLE centrality_200_200_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM porto_200_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM porto_200_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;"
  },
  {
    "objectID": "appendix.html#section-4",
    "href": "appendix.html#section-4",
    "title": "Appendix A — Performance tests",
    "section": "A.5 300",
    "text": "A.5 300\n\n\nShow the code\n\n---- 300\n\n---- Create 300 origin\nCREATE TABLE sampling_weight_300_origin  AS\nwith porto_300_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 300)\n        SELECT * FROM  porto_300_origin;  \n---- Create 300 destination\nCREATE TABLE sampling_weight_300_destination  AS\nwith porto_300_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 300)\n        SELECT * FROM  porto_300_destination;  \n---- Create index for origin\nCREATE INDEX idx_sampling_weight_300_origin_net_id ON sampling_weight_300_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_300_origin_geom ON sampling_weight_300_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_300_destination_net_id ON sampling_weight_300_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_300_destination_geom ON sampling_weight_300_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_300_origin;\nVACUUM(full, ANALYZE) sampling_weight_300_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n--- Run query\n\n EXPLAIN ANALYZE\n CREATE TABLE centrality_300_300_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM sampling_weight_300_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM sampling_weight_300_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n                      \n--- check max centrality                     \nSELECT max(centrality) FROM centrality_300_300_porto ;\n--- check max rows\nSELECT  count(*) FROM centrality_10_10_porto;"
  },
  {
    "objectID": "appendix.html#method-array_agg",
    "href": "appendix.html#method-array_agg",
    "title": "Appendix A — Performance",
    "section": "A.4 Method array_agg()",
    "text": "A.4 Method array_agg()\n\n\nShow the code\n---- create origin_destination\nCREATE TEMP TABLE vertices_lookup_10\nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom AS fgeom,\n         t.net_id AS tid, t.the_geom AS tgeom\n    FROM random_10_origin AS f,\n         random_10_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) AS fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) AS tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n---- Number of OD\nSELECT count(*) FROM vertices_lookup_10;\n---- Create index\nCREATE INDEX idx_vertices_lookup_10_fid ON vertices_lookup_10 USING hash(fid);\nCREATE INDEX idx_vertices_lookup_10_tid ON vertices_lookup_10 USING hash(tid);\nCREATE INDEX idx_vertices_lookup_10_fv ON vertices_lookup_10 USING hash(fv);\nCREATE INDEX idx_vertices_lookup_10_tv ON vertices_lookup_10 USING hash(tv);\n---- Vacuum and clean\nVACUUM(full, ANALYZE) vertices_lookup_10;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n---- Run query using array_agg()\nEXPLAIN ANALYZE\nCREATE TABLE porto_100_dijkstra_agg AS\nWITH pgr_result AS (\n  SELECT pgr_dijkstra('SELECT ogc_fid AS id,\n             fromid AS source,\n            toid AS target,\n             weight AS cost FROM porto_alegre_net_largest',\n    array_agg(fv), array_agg(tv), \n    directed := true\n  ) FROM vertices_lookup_10 \n) \nSELECT \n b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \nFROM pgr_result\nLEFT JOIN porto_alegre_net_largest AS b\nON (pgr_dijkstra).edge = b.ogc_fid\nGROUP BY \n    the_geom, b.ogc_fid\nORDER BY \n    centrality DESC;\n---- Max centrality value\nselect max(centrality) FROM porto_100_dijkstra_agg ;\n---- Number of rows\nselect count(*) FROM porto_100_dijkstra_agg ;"
  },
  {
    "objectID": "appendix.html#pgr_astrar",
    "href": "appendix.html#pgr_astrar",
    "title": "Appendix A — Performance",
    "section": "A.5 pgr_astrar()",
    "text": "A.5 pgr_astrar()\n\n\nShow the code\nSELECT * FROM random_10_origin ro ;\n \nCREATE TABLE porto_alegre_net_largest_astar AS          \nWITH porto_alegre_net_astart AS (\nSELECT \n*,\nst_startpoint(the_geom) AS start_pt,\nst_endpoint(the_geom) AS end_pt\nFROM porto_alegre_net_largest AS net)\nSELECT *,\n    st_x(start_pt) AS x1,\n    st_y(start_pt) AS y1,\n    st_x(end_pt) AS x2,\n    st_y(end_pt) AS y2\nFROM porto_alegre_net_astart;\n---- adding spatial index\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_the_geom ON porto_alegre_net_largest_astar  USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_start ON porto_alegre_net_largest_astar  USING gist(start_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_end ON porto_alegre_net_largest_astar USING gist(end_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x1 ON porto_alegre_net_largest_astar  USING btree(x1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y1 ON porto_alegre_net_largest_astar USING btree(y1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x2 ON porto_alegre_net_largest_astar USING btree(x2);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y2 ON porto_alegre_net_largest_astar USING btree(y2);                      \n----- RUnning the query\nEXPLAIN ANALYZE\nCREATE TABLE centrality_10_10_porto_astrar AS\nSELECT  \n    b.ogc_fid,\n     b.the_geom,\n    count(the_geom) as centrality \nFROM pgr_astar(\n    'SELECT ogc_fid AS id,\n            fromid AS source,\n            toid AS target,\n            weight AS cost,\n            x1,\n            y1,\n            x2,\n            y2\n    FROM porto_alegre_net_largest_astar',\n    ARRAY(SELECT net_id FROM  random_10_origin),\n    ARRAY(SELECT net_id FROM  random_10_destination),\n             directed:=TRUE,\n             heuristic:=2) j\n                      left JOIN porto_alegre_net_largest_astar AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n\n--- check max centrality                     \nSELECT max(centrality) FROM centrality_10_10_porto_astrar ;\n--- check max rows\nSELECT  count(*) FROM centrality_10_10_porto_astrar;\n\n\n\n\nShow the code\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\nShow the code\nCREATE TABLE random_272_destination  AS\nwith random_272_destination AS (\n        SELECT\n            * \n        FROM \n            od_40420_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  random_272_destination;  \n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5 \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,\n         random_272_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\nCREATE TABLE porto_272_272_dijkstra AS\n WITH pgr_result AS (\n   SELECT pgr_dijkstra('SELECT id,\n           source,\n    target,\n     cost FROM porto_alegre_net_largest',\n     array_agg(fv), array_agg(tv), \n     directed := true\n   ) FROM vertices_lookup_v5\n )\nSELECT (pgr_dijkstra).*, a.fid, a.tid FROM pgr_result\nJOIN vertices_lookup_v5 a\nON (pgr_dijkstra).start_vid = a.fv\nAND (pgr_dijkstra).end_vid = a.tv;\n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\n\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;"
  },
  {
    "objectID": "methodology.html#selecting-the-area-of-interest-aoi-and-importing-data",
    "href": "methodology.html#selecting-the-area-of-interest-aoi-and-importing-data",
    "title": "2  Methodology",
    "section": "3.1 Selecting the Area of Interest (AoI) and importing data",
    "text": "3.1 Selecting the Area of Interest (AoI) and importing data\nPostGIS queries obtained the Area of Interest calculating the bounding box from the flooding extent provided by the Universidade Federal do Rio Grande do Sul. Likewise, PostGIS queries prepared the flooding extent for the analysis subdividing the layer and removing slivers. Having obtained the area of interest, the C++/Javascript framework osmium (Barron, Neis, and Zipf 2014) processed the OSM data obtaining the geometry of the network.\n\n3.1.1 Importing data into PostgreSQL\nThe tool ogr2ogr imported the data into a PostgreSQL database. The output file format parameter -f included the PostgreSQL connection components, while the parameters -nln and -lco assigned the name of the table and columns respectively. For the OSM network, the parameter -nlt adjusted the multilinestring geometry to linestring to process single features more efficiently as indicated in the bibliography (Obe and Hsu 2017).\n\n\nPostGIS: Importing data to PostgreSQL using ogr2ogr\n## OSM geometry: porto_alegre_net_pre.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_net_pre.geojson -nln porto_alegre_net_pre -lco GEOMETRY_NAME=the_geom -nlt LINESTRING -explodecollections \n\n## Administrative units: nuts.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/nuts.geojson -nln nuts -lco GEOMETRY_NAME=geom\n\n## Flooding extent: flooding_rio_grande_do_sul.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/flooding_rio_grande_do_sul.geojson -nln flooding_rio_grande_do_sul -lco GEOMETRY_NAME=the_geom \n\n## Building density: urban_center_4326.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/urban_center_4326.geojson -nln urban_center_4326 -lco GEOMETRY_NAME=geom \n\n## Hospitals\n### From Rio Grande do Sul Geoportal\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/Hospitais_com_Leitos_de_UTIs_no_RS.geojson -nln hospitals_bed_rs -lco GEOMETRY_NAME=geom\n\n\n\n\n3.1.2 Preparing the flooding mask\nThe downloaded flooding extent covered a larger area in Rio Grande do Sul. However, the area of interest that contained urban dense cities such as Porto Alegre was smaller. Therefore, a series of operations reduced the area focusing on Porto Alegre and at the same time improved the query performance.\nSubdividing the flooding mask to make the spatial indexes more efficient was the first step. The reason was the higher number of vertices of large objects and larges bounding boxes that hinder the spatial index performance (Link). After enabling the spatial indexes, selecting only the flooding subunits that intersected with the area of interest reduced the size of the region query. Previous studies reported that reducing the number of spatial polygons and points of the region saved processing time and computation costs (Zhao et al. 2017). Additionally, the following code dissolved the borders to break the multipolygon into simple polygons to improve the performance of functions such as st_difference() and removed slivers polygons. Based on size criteria, the calculated area of these undesirable polygons identified the slivers of the flooding extent (Grippa et al. 2018). (link).\n\n\nPostGIS: Subdiving flooding extent and removing slivers\n--- 1) Subdividing to make spatial indexes more efficent\n--- Select the GHS area that intersects with Porto Alegre\nCREATE TABLE flooding_subdivided_porto AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n---Subdivide the flood extent to increase spatial indexes performance \nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul) \n---- Select the flooding subunits that intersects with the previous bounding box\nSELECT \n    flooding_sul_subdivided.* \nFROM \n    flooding_sul_subdivided, \n    porto_alegre_ghs_bbox\nWHERE \n    st_intersects(flooding_sul_subdivided.the_geom, porto_alegre_ghs_bbox.geom_bbox);\n--- 2) Simplifying geometry\nCREATE TABLE flooding_cleaned_porto AS\n    SELECT\n        (ST_Dump(the_geom)).geom::geometry(polygon, 4326) geom \n    FROM \n        flooding_subdivided_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union AS\n    SELECT\n        ST_Union(geom)::geometry(multipolygon, 4326) geom \n    FROM \n        flooding_cleaned_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union_simple AS\n    SELECT \n        (ST_Dump(geom)).geom::geometry(polygon, 4326) geom \n    FROM \n        flooding_cleaned_porto_union;\n--- This allowed to calculate the area of each polygon finding slivers that were removed using the following code. \nSELECT COUNT(*)\n     FROM flooding_cleaned_porto_union_simple ; --- count= 54.\n\nDELETE FROM \n  flooding_cleaned_porto_union_simple\nWHERE \n  ST_Area(geom) &lt; 0.0001; ---count= 2.\n  \n--- Obtain Area\nSELECT \n  sum(st_area(geom::geography)/10000)::integer \nFROM  \n  flooding_cleaned_porto_union_simple;\n--- Obtain size in memory\nSELECT \n  pg_size_pretty(SUM(ST_MemSize(geom))) \nFROM  \n  flooding_cleaned_porto_union_simple;\n--- Obtain number of points\nSELECT \n  sum(ST_Npoints(geom)) \nFROM  \n  flooding_cleaned_porto_union_simple;"
  },
  {
    "objectID": "methodology.html#obtaining-pre-and-post-disaster-routable-networks",
    "href": "methodology.html#obtaining-pre-and-post-disaster-routable-networks",
    "title": "2  Methodology",
    "section": "3.2 Obtaining pre and post disaster routable networks",
    "text": "3.2 Obtaining pre and post disaster routable networks\nThe creation of the network after the disaster using the modified flooding mask and the creation of the origin-destination are considered in this section. Before generating these routable networks, the following code set configuration parameters, recommended to deliver faster results (Jhummarwala Abdul 2014).\n\n\nPostGIS: Adjusting configuration settings\n--- Set up the configuration\n----- Increase performance by using more max_parallel_workes_per_gather (https://blog.cleverelephant.ca/2019/05/parallel-postgis-4.html)\nSET max_parallel_workers_per_gather =4;\n----- Cluster the index (https://gis-ops.com/pgrouting-speedups/)\nCLUSTER porto_alegre_net_largest USING porto_alegre_net_largest;\n\n\n\n3.2.1 Importing OpenStreetMap (OSM) network as geometry\nThe following SQL code created the command to import OpenStreetMap (OSM) network using osmium. After creating the spatial index on the network table, the GHS-SMOD dataset the Global Human Settlement that intersected with Porto Alegre is selected as AOI. Secondly, this GHS is used to dynamically generate the bounding box and command line required by osmium. The OSM network obtained with this osmium command was the input required for OpenRouteService (ORS).\n\n\nPostGIS: Generating responsive code to import OSM\n--- 0) Add spatial index everywhere:\nCREATE INDEX idx_porto_alegre_net_largest_geom ON porto_alegre_net_largest USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_source ON porto_alegre_net_largest USING btree(fromid);\nCREATE INDEX idx_porto_alegre_net_largest_target ON porto_alegre_net_largest USING btree(toid);\nCREATE INDEX idx_porto_alegre_net_largest_id ON porto_alegre_net_largest USING btree(ogc_fid);\nCREATE INDEX idx_porto_alegre_net_largest_cost ON porto_alegre_net_largest USING btree(weight);\n--- 1) GHS urban area that intersected with Porto Alegre city\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n--- 2) The command used the Porto Alegre GHS and its Bounding Box to import the OpenStreet Network. \nporto_alegre_ghs_bbox_osmium_command AS (\n    SELECT \n        ST_XMin(ST_SnapToGrid(geom_bbox, 0.0001)) AS min_lon,\n        St_xmax(ST_SnapToGrid(geom_bbox,0.0001)) AS max_lon,\n        St_ymin(ST_SnapToGrid(geom_bbox,0.0001)) AS min_lat,\n        St_ymax(ST_SnapToGrid(geom_bbox, 0.0001)) AS max_lat\n    FROM porto_alegre_ghs_bbox)\nSELECT\n    'osmium extract -b ' \n    || min_lon || ',' || min_lat || ',' || max_lon || ',' || max_lat || \n    ' sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf' AS osmium_command\nFROM porto_alegre_ghs_bbox_osmium_command;\n\n\n\n\n3.2.2 Transforming geometric network into routable graph network using OpenRouteService (ORS).\nFirstly, running the docker for OpenRouteService transformed the OSM network from the GHS AoI into a graph. In the “ors-config.yml” from OpenRotueService (ORS), the “source_file” parameter is set to find the directory that contained the OSM geometry network. OpenRouteService (ORS) created a a routable network assigining costs and adding information for each node such as the origin, “fromId”, and the destination, “toId”. The R script “get_graph” from Marcel Reinmuth exported the ORS network named as “porto_alegre_net”.\nSecondly, the data type of the columns “fromid” and “toid” from the graph network generated by ORS is transform into bigint to run the algorithm pgr_dijkstra as the official documentation indicated.\n\n\nPostGIS: Adjusting ORS configuration & preparation for pgrouting\n--- 1) the source_file parameter in the ors.config.yml file is set to find the imported OSM network \nsource_file: /home/ors/files/puerto_alegre_urban_center.osm.pbf\n---- 2)  The exported ORS graph is prepared for pgrouting casting the right data type for the pgrouting functions.\nselect * from porto_alegre_net_pre; \nALTER TABLE porto_alegre_net_pre \n    ALTER COLUMN \"toid\" type bigint,\n    ALTER COLUMN \"fromid\" type bigint,\n    ALTER COLUMN \"ogc_fid\" type bigint;\n\n\n\n\nR: Visualizing OSM and ORS network with Mapview\n## Load data\nors_network &lt;- st_read(eisenberg_connection, layer=\"porto_alegre_net_pre\")\nosm_network &lt;- st_read(eisenberg_connection, layer=\"puerto_alegre_ghs_osm\")\nors_network_subset &lt;- ors_network |&gt; head()\n## Create subset using a bounding box\nors_subset &lt;- ors_network |&gt; filter(id ==140210) |&gt; st_bbox()\nxrange &lt;- ors_subset$xmax - ors_subset$xmin\nyrange &lt;- ors_subset$ymax - ors_subset$ymin\n## Expand the bounding box\nors_subset[1] &lt;- ors_subset[1] - (4 * xrange) # xmin - left\nors_subset[3] &lt;- ors_subset[3] + (4 * xrange) # xmax - right\nors_subset[2] &lt;- ors_subset[2] - (2 * yrange) # ymin - bottom\nors_subset[4] &lt;- ors_subset[4] + (2 * yrange) # ymax - top\n## Convert bounding box into polygon\nors_subset_bbox &lt;- ors_subset %&gt;%  # \n  st_as_sfc() \n## Use the polygon to subset the network\nintersection_ors &lt;- sf::st_intersection(ors_network, ors_subset_bbox)\nintersection_osm &lt;- sf::st_intersection(osm_network, ors_subset_bbox)\n## Mapview maps\nm1 &lt;- mapview(intersection_osm, \n              color=\"#6e93ff\",\n              layer.name =\"OpenStreetMap - Geometry\",\n              popup=popupTable(intersection_osm, \n                               zcol=c(\"id\",\"osm_id\",\"name\",\"geom\")))\nm2 &lt;- mapview(intersection_ors,\n              color =\"#d50038\",\n              layer.name=\"OpenRouteService -Graph\",\n              popup=popupTable(intersection_ors))\nsync(m1,m2) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n3.2.3 Exploring ORS graph network and components\nBefore using the graph network, a quick inspection using the pgrouting function pgr_strongComponents() revealed the number of components or isolated self-connecting networks within the graph. The largest network component is selected using its length. The following code created a table with the different components and another table containing the largest component. The reason to discard the rest of components was its relatively small size and based on the observation that some of these components appeared next to OSM objects categorized as “barriers”, which meant that they were part of private properties, such as “Villa’s Home Resot”, being not relevant for an emergency response.\n\n\nPostGIS: Obtaining components of the network and their length\n--- 1) Quick inspection of the graph to determine components\n---- Create a vertice table for pgr_dijkstra()\nSELECT \n  pgr_createVerticesTable('porto_alegre_net_pre', source:='fromid', target:='toid');\n  \n---- Add data to the vertices created table \nCREATE TABLE component_analysis_network_porto AS\nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\nporto_alegre_net_component_geom AS (\nSELECT \n    net.*,\n    net_geom.the_geom\nFROM \n    porto_alegre_net_component AS net\nJOIN \n    porto_alegre_net_pre  AS net_geom\nON \n    net.node = net_geom.fromid)\nSELECT \n    component,\n    st_union(the_geom) AS the_geom,\n    st_length(st_union(the_geom)::geography)::int AS length\nFROM  \n    porto_alegre_net_component_geom\nGROUP BY component\nORDER BY length DESC;\n--- 2) A table with the component of the network with the highest length\nCREATE TABLE porto_alegre_net_largest AS\n---- Obtain again table classifying nodes in different components           \nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\n--- Calculate the largest component from the network\nlargest_component_net AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto \n    LIMIT 1),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component,\n    largest_component_net\nWHERE \n    porto_alegre_net_component.component = largest_component_net.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_pre AS net_multi_component,\n    largeset_component_network_porto AS net_largest_component\nWHERE  \n    net_multi_component.fromid IN (net_largest_component.node);\n\n\n\n\n\nR: Visualizing the three longest component\ncomponent_analysis_network &lt;- st_read(eisenberg_connection, \"component_analysis_network_porto\")\n## Select top 3\ncomponent_analysis_network_3 &lt;-  component_analysis_network |&gt; arrange(desc(round(distance))) |&gt; slice(1:3) \n### Tidy component to be used as categorical variable\ncomponent_analysis_network_3$component  &lt;- component_analysis_network_3$component |&gt; as.character() |&gt; as_factor()\n## Visualization\nm1_net &lt;- component_analysis_network_3 |&gt;\n              filter(component==\"1\") |&gt;\n              mapview(layer.name = \"1st Longest component\",\n                      lwd= 0.5,\n                      color=\"#66c2a5\")\n\nm2_net &lt;- component_analysis_network_3 |&gt; \n              filter(component==\"3728\") |&gt;\n              mapview(layer.name = \"2nd Longest component\",\n                      color =\"#8da0cb\",\n                      lwd=3,\n                      popup= popupTable( component_analysis_network_3[2,],\n                                        zcol=(c(\"component\",\"distance\"))))\n\nm3_net &lt;- component_analysis_network_3 |&gt;\n              filter(component==\"40578\") |&gt;\n              mapview(layer.name = \"3rd Longest component\",\n                      color=\"#fc8d62\",\n                      popup= popupTable(component_analysis_network_3[3,]),\n                      lwd =3)\n\nm1_net + m2_net + m3_net\n\n\n\n\n3.2.4 Generating a routable pre-disaster network\nThe original porto_alegre_net_pre graph network is classified by its components and the largest component representing the network before the flooding is filtered.In the first step, the components are calculated and the vertices from the largest network stored in the table largeset_component_network_porto. Then, selecting the vertices from the original table where they also appeared in the largest component network generated the pre-disaster network.\n\n\nPostGIS: Creating the pre-disaster network filtering out components\nCREATE TABLE porto_alegre_net_largest AS\n---- 1) Classify the geometric OSM network in different components          \n  WITH porto_alegre_net_component AS (\n    SELECT \n      * \n    FROM \n      pgr_strongComponents('SELECT id,\n                               source,\n                               target,\n                               cost \n                            FROM \n                              porto_alegre_net_pre')),\n--- Obtaining the nodes from the largest component from the network\n  largest_component_net AS (\n      SELECT \n          component\n      FROM \n          component_analysis_network_porto \n      LIMIT 1),\n--- 2) Filtering the nodes from the original network to only those in the largest component\n  largeset_component_network_porto AS (\n    SELECT\n        *  \n    FROM\n        porto_alegre_net_component,\n      largest_component_net\n    WHERE \n        porto_alegre_net_component.component = largest_component_net.component)\n    SELECT \n        net_multi_component.*\n    FROM \n        porto_alegre_net_pre AS net_multi_component,\n        largeset_component_network_porto AS net_largest_component\n    WHERE  \n        net_multi_component.source IN (net_largest_component.node);\n\n\n\n\n3.2.5 Generating a routable post-disaster network\nA naive approach overlaying the flooding mask with the road network by the function st_difference() crashed the session. The follwing multi-step methodology reduced the processing cost making the query feasible using less resources by incorporating boolean operators in some steps. In other studies, decomposing difference function queries using boolean operators reported an increase on the performance of 223% in PostgreSQL (Lupa and Piórkowski 2014). Firstly the network inside the flooding mask is selected. Secondly, a subset of the network outside the flooding mask avoided using spatial operations saving computing resources by using the ID’s from the network inside the flooding to filter the data.Thirdly, to obtain the boundaries between the inside and outside network, the geometry is converted into its exterior ring. From this network on the boundary, only the exterior part that intersected with the outside boundary was selected.\n\n\nPostGIS: Multi-step methodology to create the post-disaster network\n--- 1) Network inside the flooding mask\nCREATE TABLE porto_alegre_street_in_v2 AS\nSELECT net.id,\n    CASE \n        WHEN ST_Contains(flood.the_geom, net.the_geom)\n        THEN net.the_geom\n        ELSE st_intersection(net.the_geom, flood.the_geom)\n    END AS  geom\nFROM porto_alegre_net_largest net\nJOIN flooding_subdivided_porto flood\nON st_intersects(net.the_geom, flood.the_geom);\n--- 2) Network outside the flooding mask\nCREATE TABLE porto_alegre_street_out_v2 AS\nSELECT net.*\nFROM porto_alegre_net_largest net\nWHERE net.id NOT IN (\n    SELECT net.id\n    FROM porto_alegre_street_in_v2 net);\n--- 3) Network on the boundaries\nCREATE TABLE porto_alegre_net_outside_v2 AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul),\nexterior_ring_porto_alegre_v2 AS (\nSELECT \n    ST_ExteriorRing((ST_Dump(union_geom)).geom) as geom\nFROM (\n    SELECT \n        ST_Union(flood.the_geom) as union_geom\n    FROM \n        porto_alegre_ghs_bbox as bbox\n    JOIN \n        flooding_sul_subdivided as flood\n    ON \n        ST_Intersects(flood.the_geom, bbox.geom_bbox)\n) AS subquery)\nSELECT net.id,\n    CASE\n        WHEN NOT ST_Contains(flood.geom, net.the_geom)\n        THEN net.the_geom\n            ELSE st_intersection(net.the_geom, flood.geom)\n    END AS  geom,\n    net.target,\n       net.source,\n       cost,\n       \"unidirectid\",\n       \"bidirectid\"\nFROM\nporto_alegre_net_largest AS net\nJOIN exterior_ring_porto_alegre_v2 flood ON\nst_intersects(net.the_geom, flood.geom);\n\n---- For the network\nCREATE INDEX idx_porto_alegre_net_outside_v2 ON porto_alegre_net_outside_v2 USING gist (geom);\n\nCLUSTER porto_alegre_net_outside_v2 USING idx_porto_alegre_net_outside_v2;\n\n--- For the flooding mask\nCREATE INDEX flooding_sul_subdivided_idx ON flooding_sul_subdivided USING gist (the_geom);\n\nCLUSTER flooding_sul_subdivided USING flooding_sul_subdivided_idx;\n---- Before doing difference\nVACUUM(FULL, ANALYZE) porto_alegre_net_outside_v2;\nVACUUM(FULL, ANALYZE) flooding_sul_subdivided;\n--- st_difference and uniting the external to the boundaries\n---  Unite the subunits of the flooding\nCREATE TABLE flooding_symple as \nSELECT st_union(geom) as the_geom FROM flooding_cleaned_porto_union_simple;\n---- Index the flooding\nCREATE INDEX flooding_symple_idx ON flooding_symple USING gist (the_geom);\nCLUSTER flooding_symple USING flooding_symple_idx;\n--- 4) Obtain the boundary network that intersect with the outside network\nCREATE TABLE difference_outside_flood_v3 AS\nSELECT net.id,\n        target,\n        source,\n        cost,\n        unidirectid,\n        bidirectid,\nst_difference(net.geom, flood.the_geom) AS the_geom\nFROM porto_alegre_net_outside_v2 AS net,\nflooding_symple  AS flood;\n\n--- 5) Unite outside network with the external part of the boundary network\nCREATE TABLE porto_alegre_net_post_v5 AS\nSELECT *\nFROM porto_alegre_street_out_v2\nUNION\nSELECT *\nFROM difference_outside_flood_v3;\n\n\n\n\nR: Visualizing the multi-step methodology\nporto_alegre_net_pre &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_net_largest\")\ncentrality_pre &lt;- sf::st_read(eisenberg_connection, \"centrality_weighted_100_bidirect_cleaned\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_cleaned_porto\")\nnet_in &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_in_v2\")\n## subset\nsubset_network_pre &lt;-  sf::st_read(eisenberg_connection, \"porto_alegre_net_largest_subset\")\nsubset_network_in &lt;- sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_in_v3_subset.geojson\") |&gt; select(\"geometry\")\nsubset_network_out &lt;- sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_out_subset.geojson\")  \nsubset_network_outside_flood &lt;-sf::st_read(\"/home/ricardo/heigit_bookdown/data/porto_alegre_street_outside_subset.geojson\") \nsubset_network_post &lt;- sf::st_read(eisenberg_connection, \"reet_united_v3\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_symple\") |&gt; st_as_sf() \n## centrality\nmapview(subset_network_pre,\n          color=\"#d4e7e7\",\n          lwd= 1,\n          layer.name=\"0.Pre-flooding network\",\n          popup=popupTable(subset_network_pre,\n          zcol=c(\"id\",\"source\",\"target\",\"bidirectid\"))) +\n  mapview(flooding,\n          color=\"darkblue\",\n          alpha.regions= 0.5,\n          layer.name=\"0.Flooding layer\") +\n  mapview(subset_network_in,\n          color=\"red\",\n          lwd= 1,\n          hide = TRUE,\n          layer.name=\"1.Network inside the flooding\") +\n  mapview(subset_network_out,\n          color=\"yellow\",\n          lwd= 1.2,\n          hide = TRUE,\n          layer.name=\"2.Flooding outside the flooding mask\",\n          popup=popupTable(subset_network_out)) +\n  mapview(subset_network_outside_flood,\n          color=\"lightgreen\",\n          lwd= 1.4,\n          hide = TRUE,\n          layer.name=\"3 & 4. Network on the boundaries\",\n          popup=popupTable(subset_network_outside_flood)) +\n    mapview(subset_network_post,\n          color=\"green\",\n          lwd= 1,\n          hide =TRUE,\n          layer.name=\"5. Uniting network external to the boundires and network from outside\",\n          popup=popupTable(subset_network_post))\n\n\n\nAfter obtaining this post-disaster network, pgrouting created the vertices table required to conduct the centrality analysis. Similarly to generating the pre-disaster network, an analysis of the component identified those components more relevant due to its length. Since some of the post-disaster network components had similar length, the table “component_analysis_network_porto_post” stored this information, which after a qualitative inspection determined the components and their nodes that finally composed the post-disaster network.\n\n\nPostGIS: Creating the post-disaster network\n---- 1)  Creating a vertices table required for centrality analysis\nSELECT pgr_createverticesTable('porto_alegre_net_post_v5');\n---- 2)  Obtaining components and their nodes to be used later as a filter.\nCREATE TABLE component_analysis_network_porto_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\nporto_alegre_net_component_geom_post AS (\nSELECT \n    net.*,\n    net_geom.the_geom\nFROM \n    porto_alegre_net_component_post AS net\nJOIN \n    porto_alegre_net_post_v5  AS net_geom\nON \n    net.node = net_geom.source)\nSELECT \n    component,\n    st_union(the_geom) AS the_geom,\n    st_length(st_union(the_geom)::geography)::int AS length\nFROM  \n    porto_alegre_net_component_geom_post\nGROUP BY component\nORDER BY length DESC;\n--- Summarising component\nCREATE TABLE components_network_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\n--- 3) Calculating the largest component from the network\nlargests_component_net_post AS (\n    SELECT \n        component,\n        the_geom,\n        length::int\n    FROM \n        component_analysis_network_porto_post)\nSELECT * FROM largests_component_net_post;\n\n---- Visualizing which are more important\nCREATE TABLE main_components_post AS\nSELECT \n    * \nFROM \n    components_network_post\nWHERE\n    component IN (21,14,5187);\n\n-------------- Remember to justify why 5\n\nCREATE TABLE prueba_largest_network_post AS\nWITH porto_alegre_net_component_post AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               id,\n                               source,\n                               target,\n                               cost \n                        FROM \n                              porto_alegre_net_post_v5')),\n--- Calculate the largest component from the network\nlargest_component_net_post AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto_post\n    LIMIT 5),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto_post AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component_post,\n    largest_component_net_post\nWHERE \n    porto_alegre_net_component_post.component = largest_component_net_post.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_post_v5 AS net_multi_component,\n    largeset_component_network_porto_post AS net_largest_component\nWHERE  \n    net_multi_component.source IN (net_largest_component.node);"
  },
  {
    "objectID": "methodology.html#measuring-centrality-on-networks",
    "href": "methodology.html#measuring-centrality-on-networks",
    "title": "2  Methodology",
    "section": "3.3 Measuring centrality on networks",
    "text": "3.3 Measuring centrality on networks\nThe centrality metrics used to determine the importance of each road segment or hospitals in the network were the edge betweenness centrality and closeness centrality respectively.\n\nThe closenesss centrality defined as a measure of how central to other vertices is a vertex (Kolaczyk and Csárdi 2014). It is obtained calculating the average shortest path from a node v to all other nodes in the network (Eq 1) (Florath, Chanussot, and Keller 2024).\n\n\\[\n\\text{CN}(v) = \\frac{n - 1}{\\sum_{w = 1}^{n - 1} d(w, v)}\n\\]\n      where d(w,v) is the shortest path distance and n-1 is | the number of nodes reachable from v. In this case, | instead of using the distance to determine how central | | each hospital in the AoI was,it was the cost provided by | ORS. A higher value indicated a faster access to the | hospital, while a large decrease after the flooding event | indicated high vulnerability.\n\nThe betweenness centrality assess the significance of roads in a network by counting the number of shortest paths that pass through this node. The most common definition of the betweenness centrality was introduced by Freeman (Eq 2) (Freeman 1977).\n\n\\[\nc_{B}(v) = \\sum_{\\substack{s \\neq t \\neq v \\\\ s,t \\in V}} \\frac{\\sigma(s,t|v)}{\\sigma(s,t)}\n\\]\n      where σ(s,t|v) is the total number of shortest paths between s and t that pass through v, and σ(s,t) is the total number of shortest paths between s and t (regardless of whether or not they pass through v). In the event that shortest paths are unique, cB(v) just counts the number of shortest paths going through v (Kolaczyk and Csárdi 2014). In this case, edges are used instead of vertices. Those roads that are frequently part of shortest paths had higher values. This metric was used before to assess the accessibility of critical amenities before and during floodings (Phua et al. 2024 ; Petricola et al. 2022 ; Gangwal et al. 2023).\n\n3.3.1 Generating Origin-Destination matrix\nThe volume of people is modeled using a traffic matrix or origin-destination (OD) matrix denoted by \\(Z = [Z_{ij}]\\) , where \\(Z_{ij}\\) represents the total volume of people flowing from a starting point \\(i\\) to a ending point \\(j\\) in a given period of time. For this study case, these generated points are then snapped into the ORS graph network where the variables “fromid” and “toid” are the origin and destination respectively. A naive approach sampled these OD using a regular distribution, while another approach used the built-up density as weight to take more samples where there were more buildings. The motivation of choice were previous studies that suggested using to consider origin-destination distributions to avoid being overly simplistic (Coles et al. 2017).\n\n3.3.1.1 Sampling using regular distribution\nThe function “I_Grid_Point_Series” created a series of regular points that represented the OD matrix given an area geometry, and the distance for the X-axis and Y-axis in angle degrees. After this, a table stored 1258 sample points generated on the AoI with a distance of 0.01 º between each sample. Lastly, the application of two indexes improved further queries on this new table. This was recommended because these points were outside the network requiring snapping, which is a spatial operation with relatively high computational costs. Apart from indexing the geometry column and the id, the query is constrained to a buffer of 0.01º to reduce computational costs and the one multipoint geometry is converted into 1258 simple points.\n\n\nPostGIS: Creating a function to sample regularly\n--- 1) Creating the function that sample OD regularly:\nCREATE OR REPLACE FUNCTION I_Grid_Point_Series(geom geometry, x_side decimal, y_side decimal, spheroid boolean default false)\nRETURNS SETOF geometry AS $BODY$\nDECLARE\nx_max decimal;\ny_max decimal;\nx_min decimal;\ny_min decimal;\nsrid integer := 4326;\ninput_srid integer;\nx_series DECIMAL;\ny_series DECIMAL;\nBEGIN\nCASE st_srid(geom) WHEN 0 THEN\n  geom := ST_SetSRID(geom, srid);\n  RAISE NOTICE 'SRID Not Found.';\n    ELSE\n        RAISE NOTICE 'SRID Found.';\n    END CASE;\n\n    CASE spheroid WHEN false THEN\n        RAISE NOTICE 'Spheroid False';\n    else\n        srid := 900913;\n        RAISE NOTICE 'Spheroid True';\n    END CASE;\n    input_srid:=st_srid(geom);\n    geom := st_transform(geom, srid);\n    x_max := ST_XMax(geom);\n    y_max := ST_YMax(geom);\n    x_min := ST_XMin(geom);\n    y_min := ST_YMin(geom);\n    x_series := CEIL ( @( x_max - x_min ) / x_side);\n    y_series := CEIL ( @( y_max - y_min ) / y_side );\nRETURN QUERY\nSELECT st_collect(st_setsrid(ST_MakePoint(x * x_side + x_min, y*y_side + y_min), srid)) FROM\ngenerate_series(0, x_series) as x,\ngenerate_series(0, y_series) as y\nWHERE st_intersects(st_setsrid(ST_MakePoint(x*x_side + x_min, y*y_side + y_min), srid), geom);\nEND;\n$BODY$ LANGUAGE plpgsql IMMUTABLE STRICT;\n--- 2) Using this function to create the sampling table\nCREATE TABLE regular_point_od AS (\nWITH multipoint_regular AS(\nselect \n    I_Grid_Point_Series(geom, 0.037,0.037, false) AS geom\n    from porto_alegre_bbox as geom),\npoint_regular AS(\nSELECT \n    st_setsrid((st_dump(geom)).geom, 4326)::geometry(Point, 4326) AS geom\nFROM  multipoint_regular)\nSELECT \n    row_number() over() AS id,\n    geom\nFROM \n    point_regular);\n--- 3) Applying indexes to increase performance\nCREATE INDEX idx_regular_point_od_geom ON regular_point_od USING GIST (geom);\nCREATE INDEX idx_regular_point_od_id ON regular_point_od USING btree(id);\n\n\n\n\n3.3.1.2 Sampling using weighted distribution\nCurrently sampling the OD matrix using a weighted distribution is carried out using R. An aliquot of 2728 points is taken from the total size of the sample population of 2.728.133, which represented a 1% of this total. The function spatSample from the terra library took this sample based on the GHS-Built-V raster and the results are stored directly in the PostgreSQL database using the DBI library.\n\n\nR: Weighted sampling based on the built up density\n# Import data\n## raster\nbuildings &lt;- terra::rast('/home/ricardo/HeiGIT-Github/do_not_push_too_large/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif')\npop_ghs &lt;- 2728\n## AoI\naoi_bbox &lt;- sf::read_sf('/home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_bbox_derived.geojson')\n## reproject for clipping\naoi_bbox_reproj &lt;- aoi_bbox |&gt; st_transform(crs(buildings)) |&gt; as_Spatial()\nbuild_cropped &lt;- crop(buildings, aoi_bbox_reproj)\n## reproject for sampling\nbuild_4326 &lt;- terra::project(build_cropped, crs(aoi_bbox))\n## weighted sampling\nod &lt;- spatSample(build_cropped, pop_ghs, \"weights\", as.points=TRUE, ) |&gt; st_as_sf() |&gt; st_transform(4326)\nnames(od)[1] &lt;- 'building'\nDBI::dbWriteTable(connection, \n                  DBI::Id(schema = \"public\", table = \"od_2728\"), \n                  od)\n\n\nAfter creating an ID for the samples, a neighbor nearest function snapped these points into the closest node of the network. Subsequently, two table subset stored a subset of 100 points for origin and destination due to the path computation costs, since the costs fom the Dijkstra algorithm used in pgrouting are reported as very high (Brandes 2001). Indexes applied on these created tables that contained the OD matrix speed up further queries. This sampling process repeated twice for the pre-flooding and post-flooding scenario generated the OD matrix required to assess the centrality of both networks.\n\n\nPostGIS: Snapping samples to the closest node\n--- 0) Creating an ID column\nALTER TABLE od_2728\nADD COLUMN id serial PRIMARY KEY;\n----1: Pre flooding event\n--- 1.1) Snapping all the points based on building density to the closest vertices within the network\nCREATE TABLE od_2728_snapped AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_2728 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n--- 1.2) Limiting to 100 origin points in the pre flooding event\n\nCREATE TABLE weight_sampling_100_origin  AS\nWITH porto_100_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_origin;  \n        \n--- 1.3) Limiting to 100 destination in the pre flooding event\nCREATE TABLE weight_sampling_100_destination  AS\nWITH porto_100_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_destination;  \n--- Indeces on origin\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_idx_net_id ON weight_sampling_100_origin USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);\n----- A) Indexes, clustering and vacuum\n---  Indexes on destination\nCREATE INDEX weight_sampling_100_destination_idx_pt_id ON weight_sampling_100_destination USING btree(pt_id);\nCREATE INDEX weight_sampling_100_destination_idx_net_id ON weight_sampling_100_destination USING btree(net_id);\nCREATE INDEX weight_sampling_100_destination_idx_the_geom ON weight_sampling_100_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_100_origin;\nVACUUM(full, ANALYZE) weight_sampling_100_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n----\n---- 2) Post flooding scenario:\n---  2.1) Snapping 2728 points to post-scenario network\nCREATE TABLE od_2728_snapped_post AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_2728 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM prueba_largest_network_post_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n--- 2.2) Creating 100 origin samples from the snapped points post-scenario\nCREATE TABLE weight_sampling_100_origin_post  AS\nWITH porto_100_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_origin;  \n--- 2.3) Creating 100 destination from the snapped points post-scenario\nCREATE TABLE weight_sampling_100_destination_post  AS\nWITH porto_100_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_destination;  \n--- B) Indeces on origin\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);\n--- Indeces on destination\nCREATE INDEX weight_sampling_100_destination_post_idx_pt_id ON weight_sampling_100_destination_post USING btree(pt_id);\nCREATE INDEX weight_sampling_100_destination_post_idx_net_id ON weight_sampling_100_destination_post USING btree(net_id);\nCREATE INDEX weight_sampling_100_destination_post_idx_the_geom ON weight_sampling_100_destination_post USING gist(the_geom);\n---- Indexes on origin\nCREATE INDEX weight_sampling_100_origin_post_idx_pt_id ON weight_sampling_100_origin_post USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_post_idx_net_id ON weight_sampling_100_origin_post USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_post_idx_the_geom ON weight_sampling_100_origin_post USING gist(the_geom);\n---- Index on the new network\nCREATE INDEX prueba_largest_network_post_idx_GEOM ON prueba_largest_network_post USING gist(the_geom);\nCREATE INDEX prueba_largest_network_post_idx_id ON prueba_largest_network_post USING btree(id);\nCREATE INDEX prueba_largest_network_post_idx_target ON prueba_largest_network_post USING btree(target);\nCREATE INDEX prueba_largest_network_post_idx_source ON prueba_largest_network_post USING btree(source);\nCREATE INDEX prueba_largest_network_post_idx_cost ON prueba_largest_network_post USING btree(cost);\n---- Cluster \nCLUSTER prueba_largest_network_post USING prueba_largest_network_post_idx_GEOM;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_100_origin;\nVACUUM(full, ANALYZE) weight_sampling_100_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;    \n\n\n\n\n3.3.1.3 Selecting point of interest (Hospitals)\nThe bounding box containing the GHS settlement that intersected with Porto Alegre filtered the Hospitals from Rio Grande do Sul. Similarly to previous sampling, a closest neighbor query snapped the location of these hospitals to the closest vertices of the network. Hospitals and other healthcare facilities were the objects of previous studies focused on accessibility (Geldsetzer et al. 2020 ; Sushma and Reddy 2021). The following query provided the table of healthcare facilities affected by the flooding, which were the destination of the OD matrix and required to assess their accessibility.\n\n\nPostGIS: Selecting hospistals in the AoI and snapping\n--- Creating a table with the bounding box that contains Porto Alegre + GHS\nCREATE TABLE porto_alegre_bbox AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs)\n SELECT * FROM porto_alegre_ghs_bbox \n---Use the bounding box to select hospitals\nCREATE TABLE hospital_rs_node_v2 AS\nWITH hospital_rs_porto AS (\nSELECT \n    h.*\nFROM \n    hospitals_bed_rs AS h,\n    porto_alegre_bbox bbox\nWHERE st_intersects(h.geom, bbox.geom_bbox))\nSELECT DISTINCT ON (h.cd_cnes)\n    cd_cnes,\n    ds_cnes,\n    f.id,\n    f.the_geom &lt;-&gt; h.geom AS distance,\n    h.geom AS geom_hospital,\n    f.the_geom AS geom_node\nFROM hospital_rs_porto h\n---- Snapping the hospitals to the closest vertices in the network\nLEFT JOIN LATERAL\n(SELECT \n    id, \n    the_geom\nFROM porto_alegre_net_largest_vertices_pgr AS net\nORDER BY\n    net.the_geom &lt;-&gt; h.geom\nLIMIT 1) AS f ON true\n\n---- Create index to optimize fuerther queries\n\nCREATE INDEX idx_hospital_rs_node_v2 ON hospital_rs_node_v2 USING btree(id);\n\n\n\n\n\n3.3.2 Carrying out centrality analysis\nThe graph network obtained from OpenRouteService provided the vertices, edges and weight required for the function pgr_dijkstra. The variable toid and fromid represented the source and target, while weight was the cost. After including this information in the function, the OD matrix included as two arrays completed the required inputs to run the dijkstra algorithm. The table 1 shown the data from this graph network without including the geometry.\n\n\n\n\n\n\n\nThe table generated included the ogc_fid variable making possible to join original geometry from the graph network table. This table contained all the shortest paths, where on the one hand, the variables start_vid and end_vid were the source (fromid) and destination (toid) from the original OD matrix. On the other hand, edge indicated the edge id from the graph network.\n\n\n\n\n\n\n\n\n3.3.2.1 Assessing network centrality\nThe aggregated count of these shortest path determined the centrality of each segment. A further processing required was to use group these values by the bidirectid, obtaining the final betweenness centrality value. This process applied to the pre-disaster and post-disaster network using the two OD matrix allowed to compare the change of the betweenness centrality.\n\n\nPostGIS: Calculating edge betweenness centrality on the pre-disaster network\n--- 1) Pre-disaster network\n--- 1.1) Creating centrality \n CREATE TABLE centrality_100_100_dijkstra AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_street_united AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_100_bidirect AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_100_100_dijkstra t1\nJOIN\n    porto_alegre_net_largest t2\nON\n    t1.id = t2.id;\n--- The final product requries  79941\nselect max(\"bidirectid\") from centrality_weighted_100_bidirect;\ncreate sequence bididirect_id_weight start 79941;\nupdate centrality_weighted_100_bidirect\nset \"bidirectid\" = nextval('bididirect_id_weight')\nwhere \"bidirectid\" is null ;\n---- verify\nselect count(*) \nfrom centrality_weighted_100_bidirect \nwhere \"bidirectid\" is null; --- 0\n----\ncreate table centrality_weighted_100_bidirect_group as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_100_bidirect\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---\n---- now recover the id\ncreate table centrality_weighted_100_bidirect_group_id as \nselect t1.*, t2.id\nfrom centrality_weighted_100_bidirect_group t1\njoin centrality_weighted_100_bidirect t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_100_bidirect_cleaned as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_100_bidirect_group_id t1\njoin\n    porto_alegre_net_largest t2\non\n    t1.id = t2.id;\n--- The final product is: centrality_weighted_100_bidirect_cleaned\n\n--- 2) Post scenario\n\n----routing\n CREATE TABLE centrality_100_100_dijkstra_post AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM prueba_largest_network_post',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin_post  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination_post ),\n                      directed := TRUE) j\n                      left JOIN prueba_largest_network_post AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_100_bidirect_post AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_100_100_dijkstra_post t1\nJOIN\n    prueba_largest_network_post t2\nON\n    t1.id = t2.id;\n--- The final product requries  79918\nselect max(\"bidirectid\") from centrality_weighted_100_bidirect_post; ---79918\ncreate sequence bididirect_id_weight_post start 79918;\nupdate centrality_weighted_100_bidirect_post\nset \"bidirectid\" = nextval('bididirect_id_weight_post')\nwhere \"bidirectid\" is null ;\n   ---- verify\nselect count(*) \nfrom centrality_weighted_100_bidirect_post \nwhere \"bidirectid\" is null; --- 0              \n----\ncreate table centrality_weighted_100_bidirect_group_post as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_100_bidirect_post\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---        \n---- now recover the id\ncreate table centrality_weighted_100_bidirect_group_post_id as \nselect t1.*, t2.id\nfrom centrality_weighted_100_bidirect_group_post t1\njoin centrality_weighted_100_bidirect_post t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_100_bidirect_cleaned_post as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_100_bidirect_group_post_id t1\njoin\n    prueba_largest_network_post t2\non\n    t1.id = t2.id;       \n\n\n\n\n\n\n\n\n\n\n\n3.3.2.2 Assessing hospitals centrality\nA subset of 474 points from the weighted sampling was the origin table (source or fromid), while 22 hospitals within the AoI were the destination table (target or toid). Similarly to the network, counting the number of geometries returned the centrality value. However, for the hospital centrality centrality values were also grouped by the destination, named as end_vid. Applying this process to the pre-network and post-network with their respective OD matrix allowed us to assess the change of centrality on the healthcare facilities.\n\n\nShow the code\n------ PRE-EVENT\n---- Based on a max of 100*100 / 22 = 454   \nSELECT count(*) FROM hospital_rs_node_v2;  --- 22 POI: Hospitals \n----- Create origin   \nCREATE TABLE weight_sampling_454_origin  AS\nWITH porto_454_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() \n        LIMIT 454)\nSELECT * FROM  porto_454_origin;\n\n--- Create destinations in this case hospitals\nCREATE TABLE hospital_rs_destination AS\nSELECT \n    cd_cnes,\n    ds_cnes,\n    id,\n    geom_node\nFROM \n    hospital_rs_node_v2;  --- 22 POI: Hospitals    \n    \n---- Create hospitald destinations from closeness \nCREATE TABLE hospital_rs_destination AS\nWITH clossness_hospital_porto_id AS (\nSELECT \n    n.*,\n    v.id\nFROM \n    clossness_hospital_porto AS n\nJOIN\n    porto_alegre_net_largest_vertices_pgr AS v\n    ON  st_intersects(n.geom_node, v.the_geom))\nSELECT \n    cd_cnes,\n    ds_cnes,\n    id,\n    closeness,\n    geom_node\nFROM \n    clossness_hospital_porto_id;    \n---- Create index for origin\nCREATE INDEX weight_sampling_454_origin_net_id ON weight_sampling_454_origin USING hash(net_id);\nCREATE INDEX weight_sampling_454_origin_geom ON weight_sampling_454_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX hospital_rs_destination_net_id ON hospital_rs_destination USING btree(id);\nCREATE INDEX hospital_rs_destination_geom ON hospital_rs_destination USING gist(geom_node);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_454_origin;\nVACUUM(full, ANALYZE) hospital_rs_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n---- Running the query\nEXPLAIN ANALYZE\n CREATE TABLE centrality_424_hospitals_porto_end_id_centrality AS\n SELECT   \n b.vid,\n j.end_id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS origin_id FROM weight_sampling_454_origin),\n                      ARRAY(SELECT id AS destination_id FROM hospital_rs_destination),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, j.end_vid, b.the_geom\n                      ORDER BY centrality DESC;  \n ---- Group by end_vid that represent destination\nselect * from hospital_rs_destination ;\n                     \nSELECT * FROM centrality_424_hospitals_porto_end_id_centrality;                      \nSELECT \n    centrality.end_vid,\n    hospitals.cd_cnes,\n    hospitals.ds_cnes,\n    count(centrality.the_geom)::int AS sum_centrality,\n    max(st_length(centrality.the_geom::geography))::int AS length\nFROM \n    centrality_424_hospitals_porto_end_id_centrality AS centrality\nLEFT JOIN \n    hospital_rs_destination AS hospitals ON  centrality.end_vid = hospitals.id \nGROUP BY end_vid, cd_cnes, ds_cnes;\n### Post-scenario\n\nSELECT count(*) FROM hospital_rs_node_v2;  --- 22 POI: Hospitals \n----- Create origin   \nCREATE TABLE weight_sampling_454_origin_post  AS\nWITH porto_454_origin_post AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post \n        ORDER BY random() \n        LIMIT 454)\nSELECT * FROM  porto_454_origin_post;\n\n--- Create destinations in this case hospitals\nCREATE TABLE hospital_rs_destination AS\nSELECT \n    cd_cnes,\n    ds_cnes,\n    id,\n    geom_node\nFROM \n    hospital_rs_node_v2;  --- 22 POI: Hospitals    \n    \n---- Create hospitald destinations from closeness \nCREATE TABLE hospital_rs_destination AS\nWITH clossness_hospital_porto_id AS (\nSELECT \n    n.*,\n    v.id\nFROM \n    clossness_hospital_porto AS n\nJOIN\n    porto_alegre_net_largest_vertices_pgr AS v\n    ON  st_intersects(n.geom_node, v.the_geom))\nSELECT \n    cd_cnes,\n    ds_cnes,\n    id,\n    closeness,\n    geom_node\nFROM \n    clossness_hospital_porto_id;    \n---- Create index for origin\n\nCREATE INDEX weight_sampling_454_origin_net_id ON weight_sampling_454_origin_post USING hash(net_id); \nCREATE INDEX weight_sampling_454_origin_geom ON weight_sampling_454_origin_post USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX hospital_rs_destination_net_id ON hospital_rs_destination USING btree(id);\nCREATE INDEX hospital_rs_destination_geom ON hospital_rs_destination USING gist(geom_node);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_454_origin_post;\nVACUUM(full, ANALYZE) hospital_rs_destination;\nVACUUM(full, ANALYZE) prueba_largest_network_post;\n---- Running the query\nEXPLAIN ANALYZE\n CREATE TABLE centrality_424_hospitals_porto_end_id_centrality_post AS\n SELECT   \n b.ogc_fid,\n j.end_vid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                             fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM prueba_largest_network_post',\n                      ARRAY(SELECT net_id AS origin_id FROM weight_sampling_454_origin_post),\n                      ARRAY(SELECT id AS destination_id FROM hospital_rs_destination),\n                      directed := TRUE) j\n                      left JOIN prueba_largest_network_post AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, j.end_vid, b.the_geom\n                      ORDER BY centrality DESC;  \n ---- Group by end_vid that represent destination\nCREATE TABLE centrality_424_hospitals_porto_end_id_centrality_post_group AS\nSELECT \n    centrality.end_vid,\n    hospitals.cd_cnes,\n    hospitals.ds_cnes,\n    max(centrality.centrality)::int AS max_centrality,\n    max(st_length(centrality.the_geom::geography))::int AS length\nFROM \n    centrality_424_hospitals_porto_end_id_centrality_post AS centrality\nLEFT JOIN \n    hospital_rs_destination AS hospitals ON  centrality.end_vid = hospitals.id \nGROUP BY end_vid, cd_cnes, ds_cnes;\n\n\n\n\n\n3.3.3 Carrying out accessibility analysis\nClosenness\n\n3.3.3.1 Asessing hospitals accessibility\nThe function pgr_dijkstraCostMatrix calculated the sum of the costs of the shortest path for pair combination of nodes in the graph. Grouping the aggregated costs of these cost matrix by origin returned the closeness metrics. Lastly, using the ID of the vertices from the hospital tables the closeness values are included obtaining the most affected healthcare facilities\n\n\nPostGIS: Calculating closeness for each hospital\n--- POST-EVENT\nCREATE TABLE weight_sampling_454_origin_post  AS\nWITH porto_454_origin_post AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_post\n        ORDER BY random() \n        LIMIT 454)\nSELECT * FROM  porto_454_origin_post;   \n\n---- Create index for origin\nCREATE INDEX weight_sampling_454_origin_post_net_id ON weight_sampling_454_origin_post USING hash(net_id);\nCREATE INDEX weight_sampling_454_origin_post_geom ON weight_sampling_454_origin_post USING gist(the_geom);\n---- Running the query\n ---- Group by end_vid that represent destination\nEXPLAIN ANALYZE\n CREATE TABLE centrality_424_hospitals_porto_end_id_centrality_post AS\n SELECT   \n b.id,\n j.end_vid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM prueba_largest_network_post',\n                      ARRAY(SELECT net_id AS origin_id FROM weight_sampling_454_origin_post),\n                      ARRAY(SELECT id AS destination_id FROM hospital_rs_destination),\n                      directed := TRUE) j\n            LEFT JOIN prueba_largest_network_post AS b\n            ON j.edge = b.id\n            GROUP BY  b.id, j.end_vid, b.the_geom\n            ORDER BY centrality DESC;   \n\n\n\n---- Pre-Event\nCREATE TABLE clossness_hospital_porto AS \nWITH dijkstra_cost AS (\nSELECT * FROM pgr_dijkstraCostMatrix(\n  'SELECT id, source, target, cost FROM porto_alegre_net_largest',\n  (SELECT array_agg(id)\n    FROM porto_alegre_net_largest_vertices_pgr\n    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),\n  true)),\ncloseness AS (\n SELECT dc.start_vid,\n        sum(agg_cost)::int AS closeness\nFROM dijkstra_cost dc\nGROUP BY dc.start_vid\nORDER BY closeness DESC)\nSELECT \n    h.cd_cnes,\n    h.ds_cnes,\n    c.closeness,\n    h.distance,\n    h.geom_node,\n    h.geom_hospital\nFROM \n    closeness AS c\nLEFT JOIN\n    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;\n--- Post event\nCREATE TABLE clossness_hospital_porto_post AS \nWITH dijkstra_cost AS (\nSELECT * FROM pgr_dijkstraCostMatrix(\n  'SELECT id, source, target, cost FROM prueba_largest_network_post',\n  (SELECT array_agg(id)\n    FROM prueba_largest_network_post_vertices_pgr\n    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),\n  true)),\ncloseness AS (\n SELECT dc.start_vid,\n        sum(agg_cost)::int AS closeness\nFROM dijkstra_cost dc\nGROUP BY dc.start_vid\nORDER BY closeness DESC)\nSELECT \n    h.cd_cnes,\n    h.ds_cnes,\n    c.closeness,\n    h.distance,\n    h.geom_node,\n    h.geom_hospital\nFROM \n    closeness AS c\nLEFT JOIN\n    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;   \n    \n---- Add centrality from post-event to pre-event\nCREATE TABLE hospitals_closeness_both AS \nSELECT\n    pre.cd_cnes,\n    pre.ds_cnes,\n    pre.closeness  AS closeness_pre,\n    coalesce(post.closeness,0) AS closeness_post,\n    pre.geom_node,\n    pre.geom_hospital\nFROM \n    clossness_hospital_porto AS pre\nLEFT JOIN \n    clossness_hospital_porto_post AS post ON pre.cd_cnes  = post.cd_cnes\n\n\n\n\n\n3.3.4 Assesing vulnerability based on socioeconomic indicators"
  },
  {
    "objectID": "conclussion.html",
    "href": "conclussion.html",
    "title": "4  Conclussion",
    "section": "",
    "text": "Key variables: - Centrality —&gt; vulnerability-resilience - Accessibility —-&gt; vulnerability-resilience"
  },
  {
    "objectID": "methodology.html#verifying-results",
    "href": "methodology.html#verifying-results",
    "title": "2  Methodology",
    "section": "3.4 Verifying results",
    "text": "3.4 Verifying results\n\n\nNicht wichtig, vieliecht weg lassen.\n\n---- add ID\nALTER TABLE od_77763\nADD COLUMN id serial;\n---- Rename the column name\nALTER TABLE od_77763\nrename \"GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0\" \nto \"build\";\n---- create spatial index\nCREATE INDEX idx_od_77763_geom ON od_77763\n       USING gist(geometry);\nCREATE INDEX idx_od_77763_id on weighted_sampling\n       USING btree(id);\nCREATE INDEX idx_od_77763_geom_build on weighted_sampling \n       USING btree(\"build\");\n---- The current total is 77763\n\n----\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n----\n\nCREATE TABLE od_40420_snapped_destination AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\n\n\nBarron, Christopher, Pascal Neis, and Alexander Zipf. 2014. “A Comprehensive Framework for Intrinsic OpenStreetMap Quality Analysis.” Transactions in GIS 18 (6): 877–95. https://doi.org/10.1111/tgis.12073.\n\n\nBrandes, Ulrik. 2001. “A Faster Algorithm for Betweenness Centrality*.” The Journal of Mathematical Sociology 25 (2): 163–77. https://doi.org/10.1080/0022250X.2001.9990249.\n\n\nChoosumrong, Sittichai, Chingchai Humhong, Venkatesh Raghavan, and Gérald Fenoy. 2019. “Development of Optimal Routing Service for Emergency Scenarios Using pgRouting and FOSS4G.” Spatial Information Research 27 (4): 465–74. https://doi.org/10.1007/s41324-019-00248-2.\n\n\nColes, Daniel, Dapeng Yu, Robert L. Wilby, Daniel Green, and Zara Herring. 2017. “Beyond ‘Flood Hotspots’: Modelling Emergency Service Accessibility During Flooding in York, UK.” Journal of Hydrology 546 (March): 419–36. https://doi.org/10.1016/j.jhydrol.2016.12.013.\n\n\nFlorath, Janine, Jocelyn Chanussot, and Sina Keller. 2024. “Road Accessibility During Natural Hazards Based on Volunteered Geographic Information Data and Network Analysis.” ISPRS International Journal of Geo-Information 13 (4): 107. https://doi.org/10.3390/ijgi13040107.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35. https://doi.org/10.2307/3033543.\n\n\nGangwal, Utkarsh, A. R. Siders, Jennifer Horney, Holly A. Michael, and Shangjia Dong. 2023. “Critical Facility Accessibility and Road Criticality Assessment Considering Flood-Induced Partial Failure.” Sustainable and Resilient Infrastructure 8 (January): 337–55. https://doi.org/10.1080/23789689.2022.2149184.\n\n\nGeldsetzer, Pascal, Marcel Reinmuth, Paul O Ouma, Sven Lautenbach, Emelda A Okiro, Till Bärnighausen, and Alexander Zipf. 2020. “Mapping Physical Access to Health Care for Older Adults in Sub-Saharan Africa and Implications for the COVID-19 Response: A Cross-Sectional Analysis.” The Lancet Healthy Longevity 1 (1): e32–42. https://doi.org/10.1016/S2666-7568(20)30010-6.\n\n\nGrippa, Taïs, Stefanos Georganos, Soukaina Zarougui, Pauline Bognounou, Eric Diboulo, Yann Forget, Moritz Lennert, Sabine Vanhuysse, Nicholus Mboga, and Eléonore Wolff. 2018. “Mapping Urban Land Use at Street Block Level Using OpenStreetMap, Remote Sensing Data, and Spatial Metrics.” ISPRS International Journal of Geo-Information 7 (7): 246. https://doi.org/10.3390/ijgi7070246.\n\n\nHataitara, Rhutairat, Kampanart Piyathamrongchai, and Sittichai Choosumrong. 2024. “Development of an Emergency Notification System to Analyze the Access Route for Emergency Medical Services Using Geo-IoT and pgRouting.” Applied Geomatics 16 (2): 441–49. https://doi.org/10.1007/s12518-024-00557-8.\n\n\nJhummarwala Abdul, Prashant Chauhan, M.b. Potdar. 2014. “Parallel and Distributed GIS for Processing Geo-Data: An Overview.” International Journal of Computer Applications 106 (16): 9–16. https://doi.org/ 10.5120/18602-9881 .\n\n\nKolaczyk, Eric D., and Gábor Csárdi. 2014. Statistical Analysis of Network Data with r. Vol. 65. Use r! New York, NY: Springer New York. https://doi.org/10.1007/978-1-4939-0983-4.\n\n\nLupa, Michał, and Adam Piórkowski. 2014. “Spatial Query Optimization Based on Transformation of Constraints.” In Man-Machine Interactions 3, edited by Dr. Aleksandra Gruca, Tadeusz Czachórski, and Stanisław Kozielski, 242:621–29. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-02309-0_67.\n\n\nObe, Regina O., and Leo S. Hsu. 2017. pgRouting: A Practical Guide. Chugiak: Locate Press.\n\n\nPetricola, Sami, Marcel Reinmuth, Sven Lautenbach, Charles Hatfield, and Alexander Zipf. 2022. “Assessing Road Criticality and Loss of Healthcare Accessibility During Floods: The Case of Cyclone Idai, Mozambique 2019.” International Journal of Health Geographics 21 (1): 14. https://doi.org/10.1186/s12942-022-00315-2.\n\n\nPhua, Shin Zert, Markus Hofmeister, Yi-Kai Tsai, Oisín Peppard, Kok Foong Lee, Seán Courtney, Sebastian Mosbach, Jethro Akroyd, and Markus Kraft. 2024. “Fostering Urban Resilience and Accessibility in Cities: A Dynamic Knowledge Graph Approach.” Sustainable Cities and Society 113 (October): 105708. https://doi.org/10.1016/j.scs.2024.105708.\n\n\nSingh, Puyam S., Rosly B. Lyngdoh, Dibyajyoti Chutia, Victor Saikhom, Bhargav Kashyap, and S. Sudhakar. 2015. “Dynamic Shortest Route Finder Using pgRouting for Emergency Management.” Applied Geomatics 7 (4): 255–62. https://doi.org/10.1007/s12518-015-0161-4.\n\n\nSushma, M. B., and Veera Reddy. 2021. “Finding an Optimal Path with Hospital Information System Using GIS-Based Network Analysis.” WSEAS TRANSACTIONS ON INFORMATION SCIENCE AND APPLICATIONS 18 (March): 1–6. https://doi.org/10.37394/23209.2021.18.1.\n\n\nZhao, J. H., X. Z. Wang, F. Y. Wang, Z. H. Shen, Y. C. Zhou, and Y. L. Wang. 2017. “A NOVEL APPROACH OF INDEXING AND RETRIEVING SPATIAL POLYGONS FOR EFFICIENT SPATIAL REGION QUERIES.” ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences IV-4/W2 (October): 131–38. https://doi.org/10.5194/isprs-annals-IV-4-W2-131-2017."
  },
  {
    "objectID": "methodology.html#workflow",
    "href": "methodology.html#workflow",
    "title": "2  Methodology",
    "section": "2.1 Workflow",
    "text": "2.1 Workflow\nThree major lanes constituted the workflow of the methodology.\n\nThe first lane defined an area of interest (AOI) based on the flooding extent reported by the Universidade Federal do Rio Grande do Sul.\nThe second lane transformed this AOI into a network and graph using OpenRoutSservice. Additionally, in this phase called “routable network” lane, the post-disaster network is derived through spatial overlay with the extent of flooding.\nLastly, centrality analysis, which included connectivity and accessibility metrics, is conducted on the routable network.PostgreSQL currently done in R and the technical goal is the implementation of PostgreSQL."
  },
  {
    "objectID": "methodology.html#data",
    "href": "methodology.html#data",
    "title": "2  Methodology",
    "section": "2.1 Data",
    "text": "2.1 Data"
  },
  {
    "objectID": "methodology.html#technical-design---workflow",
    "href": "methodology.html#technical-design---workflow",
    "title": "2  Methodology",
    "section": "2.2 Technical design - workflow",
    "text": "2.2 Technical design - workflow\nThree major lanes constituted the technical design of this research.\n\nThe first lane defined an area of interest (AOI) based on the flooding extent reported by the Universidade Federal do Rio Grande do Sul.\nThe second lane transformed this AOI into a network and graph using OpenRoutService. Additionally, in this phase called “routable network” lane, the post-disaster network is derived through spatial overlay with the extent of flooding.\nLastly, centrality analysis, which included connectivity and accessibility metrics, is conducted on the routable network.\n\nThe method is based on PostgreSQL using the extensions PostGIS and Pgrouting. In the context of emergency management, the spatial database PostGIS and the library of functions Pgrouting proved to be useful developing emergency notifications (Hataitara, Piyathamrongchai, and Choosumrong 2024) or finding dynamic shortest simulating obstruction of the network by floods (Singh et al. 2015), (Choosumrong et al. 2019). Having obtained the area of interest, the C++/Javascript framework osmium (Barron, Neis, and Zipf 2014) processed the OSM data obtaining the geometry of the network.\nWHERE GHS…"
  },
  {
    "objectID": "intro.html#case",
    "href": "intro.html#case",
    "title": "1  Introduction",
    "section": "1.2 Case",
    "text": "1.2 Case\nThe different study areas were in Rio Grande do Sul, which is located in the southernmost region of Brazil bordered by Uruguay in the south and Argentina to the west. Its total extension of 281.707 km² includes 497 municipalities with 274,390 population exposed to risk areas (“Caracterização Da População Em Áreas de Risco No Brasil - PGI” n.d.).. The first study area lies between 51º 27’91’‘W and 50º94’07’‘W latitude and 30º17’22’‘S and 29º80’48’’S being identified as the dense urban centre 1210 according to the GHL-SMOD. This area contains the municipalities of Porto Alegre, Canoas, Cachoeirinha,Alvorada, Gravataí, Esteio,Sapucaia do Sul and Viamão."
  },
  {
    "objectID": "methodology.html#technical-design",
    "href": "methodology.html#technical-design",
    "title": "2  Methodology",
    "section": "2.1 Technical design",
    "text": "2.1 Technical design\n\n2.1.1 Research strategy\nA quantitative analysis of the road connectivity and access to healthcare facilities before and after a flooding event identified the temporal patterns of changes on centrality metrics. This was based on the case study of Rio Grande do Sul floodings that took place in late April 2024. The spatial pattern of the flooding shown that some regions were more vulnerable than others. Lastly, an assessment of different pathways to the healthcare facilities contributed to take an informed decision on which areas are more critical to avoid the disruption of healthcare access facilitating urban resilience.\nTherefore, this practical-orientated research aim to make a diagnosis of the occurred flooding in Rio Grande do Sul to identifying the potential critical infrastructures necessary for accessing healthcare facilities that could improve the urban resilience against future floodings.\n\n\n2.1.2 Research planning\nTo address the diagnosis of the loss of centrality by the flooding, the methodological approach is segmented into three major lanes that constituted the workflow of this research.\n\nThe first lane, selection of the Area Of Interested (AOI), defined the study area based on the flooding extent reported by the Universidade Federal do Rio Grande do Sul and imported the OSM network geometry using osmium.\nThe second lane, routable network, transformed the OSM network geometry into a routable graph network using OpenRoutService. Additionally, the post-disaster network is derived through spatial overlay with the extent of flooding.\nThe third lane,centrality analysis , connectivity and accessibility metrics identified critical infrastructure.\n\nThe method is based on PostgreSQL using the extensions PostGIS and Pgrouting. In the context of emergency management, the spatial database PostGIS and the library of functions Pgrouting proved to be useful developing emergency notifications (Hataitara, Piyathamrongchai, and Choosumrong 2024) or finding dynamic shortest simulating obstruction of the network by floods (Singh et al. 2015), (Choosumrong et al. 2019).\n\n\n\n2.1.3 Research material\nThe before and after flooding networks are obtained using the OSM network from Geofabrik and the flooding extent from Universidad de Rio Grande do Sul. Regarding the location of the hospitals, the Geoportal de Rio Grande do Sul and OSM offered this data. From this material, the PostGIS and OpenRouteService Docker set up the environment required to conduct the centrality analysis. Moreover, the Global Settlement Layer-SMOD classified the different cities to select the AOI, while the Global Settlement -Bult-V added different weights used in the origin-destination sampling. This research material is shown in the table."
  },
  {
    "objectID": "methodology.html",
    "href": "methodology.html",
    "title": "2  Methodology",
    "section": "",
    "text": "3 Workflow"
  },
  {
    "objectID": "methodology.html#research-strategy",
    "href": "methodology.html#research-strategy",
    "title": "2  Technical design",
    "section": "2.1 Research strategy",
    "text": "2.1 Research strategy\nA quantitative analysis of the road connectivity and access to healthcare facilities before and after a flooding event identified the temporal patterns of changes on centrality metrics. This was based on the case study of Rio Grande do Sul floodings that took place in late April 2024. The spatial pattern of the flooding shown that some regions were more vulnerable than others. Lastly, an assessment of different pathways to the healthcare facilities contributed to take an informed decision on which areas are more critical to avoid the disruption of healthcare access facilitating urban resilience.\nTherefore, this practical-orientated research aim to make a diagnosis of the occurred flooding in Rio Grande do Sul to identifying the potential critical infrastructures necessary for accessing healthcare facilities that could improve the urban resilience against future floodings."
  },
  {
    "objectID": "methodology.html#research-planning",
    "href": "methodology.html#research-planning",
    "title": "2  Technical design",
    "section": "2.2 Research planning",
    "text": "2.2 Research planning\nTo address the diagnosis of the loss of centrality by the flooding, the methodological approach is segmented into three major lanes that constituted the workflow of this research.\n\nThe first lane, selection of the Area Of Interested (AOI), defined the study area based on the flooding extent reported by the Universidade Federal do Rio Grande do Sul and imported the OSM network geometry using osmium.\nThe second lane, routable network, transformed the OSM network geometry into a routable graph network using OpenRoutService. Additionally, the post-disaster network is derived through spatial overlay with the extent of flooding.\nThe third lane,centrality analysis , connectivity and accessibility metrics identified critical infrastructure.\n\nThe method is based on PostgreSQL using the extensions PostGIS and Pgrouting. In the context of emergency management, the spatial database PostGIS and the library of functions Pgrouting proved to be useful developing emergency notifications (Hataitara, Piyathamrongchai, and Choosumrong 2024) or finding dynamic shortest simulating obstruction of the network by floods (Singh et al. 2015), (Choosumrong et al. 2019).\n\n\n2.2.1 Research material\nThe before and after flooding networks are obtained using the OSM network from Geofabrik and the flooding extent from Universidad de Rio Grande do Sul. Regarding the location of the hospitals, the Geoportal de Rio Grande do Sul and OSM offered this data. From this material, the PostGIS and OpenRouteService Docker set up the environment required to conduct the centrality analysis. Moreover, the Global Settlement Layer-SMOD classified the different cities to select the AOI, while the Global Settlement -Bult-V added different weights used in the origin-destination sampling. This research material is shown in the table."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Barron, Christopher, Pascal Neis, and Alexander Zipf. 2014. “A\nComprehensive Framework for Intrinsic OpenStreetMap Quality\nAnalysis.” Transactions in GIS 18 (6):\n877–95. https://doi.org/10.1111/tgis.12073.\n\n\nBrandes, Ulrik. 2001. “A Faster Algorithm for Betweenness\nCentrality*.” The Journal of Mathematical Sociology 25\n(2): 163–77. https://doi.org/10.1080/0022250X.2001.9990249.\n\n\nChoosumrong, Sittichai, Chingchai Humhong, Venkatesh Raghavan, and\nGérald Fenoy. 2019. “Development of Optimal Routing Service for\nEmergency Scenarios Using pgRouting and\nFOSS4G.” Spatial Information Research 27\n(4): 465–74. https://doi.org/10.1007/s41324-019-00248-2.\n\n\nColes, Daniel, Dapeng Yu, Robert L. Wilby, Daniel Green, and Zara\nHerring. 2017. “Beyond ‘Flood Hotspots’: Modelling\nEmergency Service Accessibility During Flooding in York,\nUK.” Journal of Hydrology 546 (March):\n419–36. https://doi.org/10.1016/j.jhydrol.2016.12.013.\n\n\nEuropean Commission. Joint Research Centre. 2023. GHSL\nData Package 2023. LU: Publications Office. https://data.europa.eu/doi/10.2760/098587.\n\n\nFlorath, Janine, Jocelyn Chanussot, and Sina Keller. 2024. “Road\nAccessibility During Natural Hazards Based on Volunteered Geographic\nInformation Data and Network Analysis.” ISPRS\nInternational Journal of Geo-Information 13 (4): 107. https://doi.org/10.3390/ijgi13040107.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on\nBetweenness.” Sociometry 40 (1): 35. https://doi.org/10.2307/3033543.\n\n\nGangwal, Utkarsh, A. R. Siders, Jennifer Horney, Holly A. Michael, and\nShangjia Dong. 2023. “Critical Facility Accessibility and Road\nCriticality Assessment Considering Flood-Induced Partial\nFailure.” Sustainable and Resilient Infrastructure 8\n(January): 337–55. https://doi.org/10.1080/23789689.2022.2149184.\n\n\nGeldsetzer, Pascal, Marcel Reinmuth, Paul O Ouma, Sven Lautenbach,\nEmelda A Okiro, Till Bärnighausen, and Alexander Zipf. 2020.\n“Mapping Physical Access to Health Care for Older Adults in\nSub-Saharan Africa and Implications for the COVID-19\nResponse: A Cross-Sectional Analysis.” The Lancet Healthy\nLongevity 1 (1): e32–42. https://doi.org/10.1016/S2666-7568(20)30010-6.\n\n\nGrippa, Taïs, Stefanos Georganos, Soukaina Zarougui, Pauline Bognounou,\nEric Diboulo, Yann Forget, Moritz Lennert, Sabine Vanhuysse, Nicholus\nMboga, and Eléonore Wolff. 2018. “Mapping Urban Land Use at Street\nBlock Level Using OpenStreetMap, Remote Sensing Data, and\nSpatial Metrics.” ISPRS International Journal of\nGeo-Information 7 (7): 246. https://doi.org/10.3390/ijgi7070246.\n\n\nHataitara, Rhutairat, Kampanart Piyathamrongchai, and Sittichai\nChoosumrong. 2024. “Development of an Emergency Notification\nSystem to Analyze the Access Route for Emergency Medical Services Using\nGeo-IoT and pgRouting.”\nApplied Geomatics 16 (2): 441–49. https://doi.org/10.1007/s12518-024-00557-8.\n\n\nJhummarwala Abdul, Prashant Chauhan, M.b. Potdar. 2014. “Parallel\nand Distributed GIS for Processing Geo-Data: An Overview.”\nInternational Journal of Computer Applications 106 (16): 9–16.\nhttps://doi.org/\n10.5120/18602-9881 .\n\n\nKolaczyk, Eric D., and Gábor Csárdi. 2014. Statistical Analysis of\nNetwork Data with r. Vol. 65. Use r! New York, NY:\nSpringer New York. https://doi.org/10.1007/978-1-4939-0983-4.\n\n\nLupa, Michał, and Adam Piórkowski. 2014. “Spatial Query\nOptimization Based on Transformation of Constraints.” In\nMan-Machine Interactions 3, edited by Dr. Aleksandra Gruca,\nTadeusz Czachórski, and Stanisław Kozielski, 242:621–29. Cham: Springer\nInternational Publishing. https://doi.org/10.1007/978-3-319-02309-0_67.\n\n\nObe, Regina O., and Leo S. Hsu. 2017. pgRouting: A Practical Guide. Chugiak: Locate\nPress.\n\n\nPetricola, Sami, Marcel Reinmuth, Sven Lautenbach, Charles Hatfield, and\nAlexander Zipf. 2022. “Assessing Road Criticality and Loss of\nHealthcare Accessibility During Floods: The Case of Cyclone Idai,\nMozambique 2019.” International Journal of Health\nGeographics 21 (1): 14. https://doi.org/10.1186/s12942-022-00315-2.\n\n\nPhua, Shin Zert, Markus Hofmeister, Yi-Kai Tsai, Oisín Peppard, Kok\nFoong Lee, Seán Courtney, Sebastian Mosbach, Jethro Akroyd, and Markus\nKraft. 2024. “Fostering Urban Resilience and Accessibility in\nCities: A Dynamic Knowledge Graph Approach.” Sustainable\nCities and Society 113 (October): 105708. https://doi.org/10.1016/j.scs.2024.105708.\n\n\nSingh, Puyam S., Rosly B. Lyngdoh, Dibyajyoti Chutia, Victor Saikhom,\nBhargav Kashyap, and S. Sudhakar. 2015. “Dynamic Shortest Route\nFinder Using pgRouting for Emergency\nManagement.” Applied Geomatics 7 (4): 255–62. https://doi.org/10.1007/s12518-015-0161-4.\n\n\nSushma, M. B., and Veera Reddy. 2021. “Finding an Optimal Path\nwith Hospital Information System Using GIS-Based Network\nAnalysis.” WSEAS TRANSACTIONS\nON INFORMATION SCIENCE\nAND APPLICATIONS 18 (March): 1–6. https://doi.org/10.37394/23209.2021.18.1.\n\n\nZhao, J. H., X. Z. Wang, F. Y. Wang, Z. H. Shen, Y. C. Zhou, and Y. L.\nWang. 2017. “A NOVEL APPROACH\nOF INDEXING AND\nRETRIEVING SPATIAL POLYGONS\nFOR EFFICIENT SPATIAL\nREGION QUERIES.” ISPRS\nAnnals of the Photogrammetry, Remote Sensing and Spatial Information\nSciences IV-4/W2 (October): 131–38. https://doi.org/10.5194/isprs-annals-IV-4-W2-131-2017."
  },
  {
    "objectID": "methodology.html#validating-results",
    "href": "methodology.html#validating-results",
    "title": "2  Methodology",
    "section": "3.4 Validating results",
    "text": "3.4 Validating results\n\n\nNicht wichtig, vieliecht weg lassen.\n---- add ID\nALTER TABLE od_77763\nADD COLUMN id serial;\n---- Rename the column name\nALTER TABLE od_77763\nrename \"GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0\" \nto \"build\";\n---- create spatial index\nCREATE INDEX idx_od_77763_geom ON od_77763\n       USING gist(geometry);\nCREATE INDEX idx_od_77763_id on weighted_sampling\n       USING btree(id);\nCREATE INDEX idx_od_77763_geom_build on weighted_sampling \n       USING btree(\"build\");\n---- The current total is 77763\n\n----\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n----\n\nCREATE TABLE od_40420_snapped_destination AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\n\n\nBarron, Christopher, Pascal Neis, and Alexander Zipf. 2014. “A Comprehensive Framework for Intrinsic OpenStreetMap Quality Analysis.” Transactions in GIS 18 (6): 877–95. https://doi.org/10.1111/tgis.12073.\n\n\nBrandes, Ulrik. 2001. “A Faster Algorithm for Betweenness Centrality*.” The Journal of Mathematical Sociology 25 (2): 163–77. https://doi.org/10.1080/0022250X.2001.9990249.\n\n\nChoosumrong, Sittichai, Chingchai Humhong, Venkatesh Raghavan, and Gérald Fenoy. 2019. “Development of Optimal Routing Service for Emergency Scenarios Using pgRouting and FOSS4G.” Spatial Information Research 27 (4): 465–74. https://doi.org/10.1007/s41324-019-00248-2.\n\n\nColes, Daniel, Dapeng Yu, Robert L. Wilby, Daniel Green, and Zara Herring. 2017. “Beyond ‘Flood Hotspots’: Modelling Emergency Service Accessibility During Flooding in York, UK.” Journal of Hydrology 546 (March): 419–36. https://doi.org/10.1016/j.jhydrol.2016.12.013.\n\n\nFlorath, Janine, Jocelyn Chanussot, and Sina Keller. 2024. “Road Accessibility During Natural Hazards Based on Volunteered Geographic Information Data and Network Analysis.” ISPRS International Journal of Geo-Information 13 (4): 107. https://doi.org/10.3390/ijgi13040107.\n\n\nFreeman, Linton C. 1977. “A Set of Measures of Centrality Based on Betweenness.” Sociometry 40 (1): 35. https://doi.org/10.2307/3033543.\n\n\nGangwal, Utkarsh, A. R. Siders, Jennifer Horney, Holly A. Michael, and Shangjia Dong. 2023. “Critical Facility Accessibility and Road Criticality Assessment Considering Flood-Induced Partial Failure.” Sustainable and Resilient Infrastructure 8 (January): 337–55. https://doi.org/10.1080/23789689.2022.2149184.\n\n\nGeldsetzer, Pascal, Marcel Reinmuth, Paul O Ouma, Sven Lautenbach, Emelda A Okiro, Till Bärnighausen, and Alexander Zipf. 2020. “Mapping Physical Access to Health Care for Older Adults in Sub-Saharan Africa and Implications for the COVID-19 Response: A Cross-Sectional Analysis.” The Lancet Healthy Longevity 1 (1): e32–42. https://doi.org/10.1016/S2666-7568(20)30010-6.\n\n\nGrippa, Taïs, Stefanos Georganos, Soukaina Zarougui, Pauline Bognounou, Eric Diboulo, Yann Forget, Moritz Lennert, Sabine Vanhuysse, Nicholus Mboga, and Eléonore Wolff. 2018. “Mapping Urban Land Use at Street Block Level Using OpenStreetMap, Remote Sensing Data, and Spatial Metrics.” ISPRS International Journal of Geo-Information 7 (7): 246. https://doi.org/10.3390/ijgi7070246.\n\n\nHataitara, Rhutairat, Kampanart Piyathamrongchai, and Sittichai Choosumrong. 2024. “Development of an Emergency Notification System to Analyze the Access Route for Emergency Medical Services Using Geo-IoT and pgRouting.” Applied Geomatics 16 (2): 441–49. https://doi.org/10.1007/s12518-024-00557-8.\n\n\nJhummarwala Abdul, Prashant Chauhan, M.b. Potdar. 2014. “Parallel and Distributed GIS for Processing Geo-Data: An Overview.” International Journal of Computer Applications 106 (16): 9–16. https://doi.org/ 10.5120/18602-9881 .\n\n\nKolaczyk, Eric D., and Gábor Csárdi. 2014. Statistical Analysis of Network Data with r. Vol. 65. Use r! New York, NY: Springer New York. https://doi.org/10.1007/978-1-4939-0983-4.\n\n\nLupa, Michał, and Adam Piórkowski. 2014. “Spatial Query Optimization Based on Transformation of Constraints.” In Man-Machine Interactions 3, edited by Dr. Aleksandra Gruca, Tadeusz Czachórski, and Stanisław Kozielski, 242:621–29. Cham: Springer International Publishing. https://doi.org/10.1007/978-3-319-02309-0_67.\n\n\nObe, Regina O., and Leo S. Hsu. 2017. pgRouting: A Practical Guide. Chugiak: Locate Press.\n\n\nPetricola, Sami, Marcel Reinmuth, Sven Lautenbach, Charles Hatfield, and Alexander Zipf. 2022. “Assessing Road Criticality and Loss of Healthcare Accessibility During Floods: The Case of Cyclone Idai, Mozambique 2019.” International Journal of Health Geographics 21 (1): 14. https://doi.org/10.1186/s12942-022-00315-2.\n\n\nPhua, Shin Zert, Markus Hofmeister, Yi-Kai Tsai, Oisín Peppard, Kok Foong Lee, Seán Courtney, Sebastian Mosbach, Jethro Akroyd, and Markus Kraft. 2024. “Fostering Urban Resilience and Accessibility in Cities: A Dynamic Knowledge Graph Approach.” Sustainable Cities and Society 113 (October): 105708. https://doi.org/10.1016/j.scs.2024.105708.\n\n\nSingh, Puyam S., Rosly B. Lyngdoh, Dibyajyoti Chutia, Victor Saikhom, Bhargav Kashyap, and S. Sudhakar. 2015. “Dynamic Shortest Route Finder Using pgRouting for Emergency Management.” Applied Geomatics 7 (4): 255–62. https://doi.org/10.1007/s12518-015-0161-4.\n\n\nSushma, M. B., and Veera Reddy. 2021. “Finding an Optimal Path with Hospital Information System Using GIS-Based Network Analysis.” WSEAS TRANSACTIONS ON INFORMATION SCIENCE AND APPLICATIONS 18 (March): 1–6. https://doi.org/10.37394/23209.2021.18.1.\n\n\nZhao, J. H., X. Z. Wang, F. Y. Wang, Z. H. Shen, Y. C. Zhou, and Y. L. Wang. 2017. “A NOVEL APPROACH OF INDEXING AND RETRIEVING SPATIAL POLYGONS FOR EFFICIENT SPATIAL REGION QUERIES.” ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences IV-4/W2 (October): 131–38. https://doi.org/10.5194/isprs-annals-IV-4-W2-131-2017."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "In progress"
  },
  {
    "objectID": "appendix.html#routing",
    "href": "appendix.html#routing",
    "title": "Appendix A — Performance",
    "section": "A.2 Routing",
    "text": "A.2 Routing"
  },
  {
    "objectID": "appendix.html#naive",
    "href": "appendix.html#naive",
    "title": "Appendix A — Performance",
    "section": "A.2 Naive",
    "text": "A.2 Naive\n\n\nShow the code\n---- Create 200 origin\nCREATE TABLE sampling_weight_200_origin  AS\nwith porto_200_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_origin;  \n---- Create 200 destination\nCREATE TABLE sampling_weight_200_destination  AS\nwith porto_200_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_destination;  \n---- Create index for origin\nCREATE INDEX idx_sampling_weight_200_origin_net_id ON sampling_weight_200_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_origin_geom ON sampling_weight_200_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_200_destination_net_id ON sampling_weight_200_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_destination_geom ON sampling_weight_200_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_200_origin;\nVACUUM(full, ANALYZE) sampling_weight_200_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n--- Run query\n\nEXPLAIN ANALYZE\n CREATE TABLE centrality_200_200_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM porto_200_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM porto_200_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;"
  },
  {
    "objectID": "appendix.html#method-bounding-box",
    "href": "appendix.html#method-bounding-box",
    "title": "Appendix A — Performance",
    "section": "A.3 Method bounding box",
    "text": "A.3 Method bounding box\n\n\nShow the code\nEXPLAIN ANALYZE \nCREATE TABLE centrality_200_200_porto_astrar_bbox AS \nSELECT    \n    b.id,\n    b.the_geom,\n    count(the_geom) as centrality\nFROM \n    pgr_astar(  \n        'SELECT id,\n                source,\n                target,\n                cost,\n                x1,\n                y1,\n                x2,\n                y2  \n            FROM \n                porto_alegre_net_pre_component_one_star\n            WHERE the_geom && (SELECT box FROM bbox)',\n             ARRAY(SELECT net_id FROM weight_sampling_200_origin),\n             ARRAY(SELECT net_id FROM weight_sampling_200_destination),\n            directed:=TRUE,\n            heuristic:=2) j\n            LEFT JOIN \n                porto_alegre_net_pre_component_one_star AS b\n            ON \n                j.edge = b.id\n            GROUP BY  \n                b.id,\n                b.the_geom\n            ORDER BY \n                centrality DESC;"
  },
  {
    "objectID": "appendix.html#data",
    "href": "appendix.html#data",
    "title": "Appendix A — Performance",
    "section": "A.1 Data",
    "text": "A.1 Data\n\nA.1.1 Direct: Download files\n\n\n\n\n\n\n\n\n\nA.1.2 Indirect: Build from queries\nmeter tambien el od_2722 que sin eso no puedes construir\nOrigin-Destination matrix\n\n\nCreate OD matrix weighted sampling\n\n---- Create 200 origin\nCREATE TABLE sampling_weight_200_origin  AS\nwith porto_200_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_origin;  \n---- Create 200 destination\nCREATE TABLE sampling_weight_200_destination  AS\nwith porto_200_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_destination;  \n\n\nCreating table with x,y for pgr_astar()\n\n\nCreate network table with x,y for pgr_astar()\nCREATE TABLE porto_alegre_net_largest_astar AS          \nWITH porto_alegre_net_astart AS (\nSELECT \n  *,\n  st_startpoint(the_geom) AS start_pt,\n  st_endpoint(the_geom) AS enstart_pt\nFROM \n  porto_alegre_net_largest AS net)\nSELECT *,\n    st_x(start_pt) AS x1,\n    st_y(start_pt) AS y1,\n    st_x(end_pt) AS x2,\n    st_y(end_pt) AS y2\nFROM \n  porto_alegre_net_astart;\n\n\nApplying spatial index\n\n\nApplying index to weighted OD matrix\n---- Create index for origin\nCREATE INDEX idx_sampling_weight_200_origin_net_id ON sampling_weight_200_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_origin_geom ON sampling_weight_200_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_200_destination_net_id ON sampling_weight_200_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_destination_geom ON sampling_weight_200_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_200_origin;\nVACUUM(full, ANALYZE) sampling_weight_200_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n\n\n\n\nApplying spatial index in the network\nCREATE INDEX idx_porto_alegre_net_largest_geom ON porto_alegre_net_largest  USING GIST(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_source ON porto_alegre_net_largest  USING btree(source);\nCREATE INDEX idx_porto_alegre_net_largest_target ON porto_alegre_net_largest  USING btree(target);\nCREATE INDEX idx_porto_alegre_net_largest_cost ON porto_alegre_net_largest  USING btree(cost);\nCREATE INDEX idx_porto_alegre_net_largest_bidirectid ON porto_alegre_net_largest  USING btree(bidirectid);\n\n\n\n\nCreate OD matrix regular sampling for agg_array()\n---- create origin_destination\nCREATE TEMP TABLE vertices_lookup_10\nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom AS fgeom,\n         t.net_id AS tid, t.the_geom AS tgeom\n    FROM random_10_origin AS f,\n         random_10_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) AS fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) AS tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n---- Number of OD\nSELECT count(*) FROM vertices_lookup_10;\n\n\n\n\nApplying index to network for astrar()\n---- adding spatial index\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_the_geom ON porto_alegre_net_largest_astar  USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_start ON porto_alegre_net_largest_astar  USING gist(start_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_end ON porto_alegre_net_largest_astar USING gist(end_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x1 ON porto_alegre_net_largest_astar  USING btree(x1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y1 ON porto_alegre_net_largest_astar USING btree(y1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x2 ON porto_alegre_net_largest_astar USING btree(x2);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y2 ON porto_alegre_net_largest_astar USING btree(y2);                      \n\n\n\n\nApplying index on vertices_lookup\n---- Create index\nCREATE INDEX idx_vertices_lookup_10_fid ON vertices_lookup_10 USING hash(fid);\nCREATE INDEX idx_vertices_lookup_10_tid ON vertices_lookup_10 USING hash(tid);\nCREATE INDEX idx_vertices_lookup_10_fv ON vertices_lookup_10 USING hash(fv);\nCREATE INDEX idx_vertices_lookup_10_tv ON vertices_lookup_10 USING hash(tv);\n---- Vacuum and clean\nVACUUM(full, ANALYZE) vertices_lookup_10;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;"
  },
  {
    "objectID": "appendix.html#method",
    "href": "appendix.html#method",
    "title": "Appendix A — Performance",
    "section": "A.3 Method",
    "text": "A.3 Method\n\n\nShow the code\ndata_dijkstra_method &lt;- read.csv(\"/home/ricardo/heigit_bookdown/metrics_method_dijkstra.csv\",sep=\",\")\nggplot(data_dijkstra_method, aes(x=number_od, y=time, group = method)) +\n  labs(x=\"Number of Origin-Destination (OD)\",\n       y=\"time (ms)\",\n       title=\"Pgrouting pgr_dijkstra() performance\",\n       subtitle=\"using naive, bbox, array_ag() methods\") +\n  geom_line(aes(color=method)) +\n   geom_point(aes(color=method)) +\n  theme_minimal()\n\n\n\n\n\n\nA.3.1 Naive\nCentrality based on\n\nBlog from Daniel I. Patterson\nBook by Matt Forrest “Listing 3.26: Final query”,page 435\n\n\n\nShow the code\n--- Run query\nEXPLAIN ANALYZE\n CREATE TABLE centrality_200_200_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM porto_200_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM porto_200_destination ),\n                      directed := TRUE) j\n                      LEFT JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n\n\n\n\nA.3.2 Method bounding box\nCode based on:\n\nStackoverflow from “simpleuser001”\n\n\n\nShow the code\nEXPLAIN ANALYZE \nCREATE TABLE centrality_200_200_porto_astrar_bbox AS \nSELECT    \n    b.id,\n    b.the_geom,\n    count(the_geom) as centrality\nFROM \n    pgr_astar(  \n        'SELECT id,\n                source,\n                target,\n                cost,\n                x1,\n                y1,\n                x2,\n                y2  \n            FROM \n                porto_alegre_net_pre_component_one_star\n            WHERE the_geom && (SELECT box FROM bbox)',\n             ARRAY(SELECT net_id FROM weight_sampling_200_origin),\n             ARRAY(SELECT net_id FROM weight_sampling_200_destination),\n            directed:=TRUE,\n            heuristic:=2) j\n            LEFT JOIN \n                porto_alegre_net_pre_component_one_star AS b\n            ON \n                j.edge = b.id\n            GROUP BY  \n                b.id,\n                b.the_geom\n            ORDER BY \n                centrality DESC;   \n\n\n\n\nA.3.3 Method array_agg()\nCode based on:\n\nStackoverflow from “Timothy Dalton”\n\n\n\nShow the code\n---- Run query using array_agg()\nEXPLAIN ANALYZE\nCREATE TABLE porto_100_dijkstra_agg AS\nWITH pgr_result AS (\n  SELECT pgr_dijkstra('SELECT ogc_fid AS id,\n             fromid AS source,\n            toid AS target,\n             weight AS cost FROM porto_alegre_net_largest',\n    array_agg(fv), array_agg(tv), \n    directed := true\n  ) FROM vertices_lookup_10 \n) \nSELECT \n b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \nFROM pgr_result\nLEFT JOIN porto_alegre_net_largest AS b\nON (pgr_dijkstra).edge = b.ogc_fid\nGROUP BY \n    the_geom, b.ogc_fid\nORDER BY \n    centrality DESC;\n---- Max centrality value\nselect max(centrality) FROM porto_100_dijkstra_agg ;\n---- Number of rows\nselect count(*) FROM porto_100_dijkstra_agg ;"
  },
  {
    "objectID": "appendix.html#algorithm",
    "href": "appendix.html#algorithm",
    "title": "Appendix A — Performance",
    "section": "A.4 Algorithm",
    "text": "A.4 Algorithm\n\nA.4.1 pgr_dijkstra\n\n\nShow the code\ndata_algorithms_naive &lt;- read.csv(\"/home/ricardo/heigit_bookdown/metrics_algorithm_naive.csv\", sep=\",\")\nggplot(data_algorithms_naive,aes(x=number_od, y=time, group = algorithm)) +\n  labs(x=\"Number of Origin-Destination (OD)\",\n       y=\"time (ms)\",\n       title=\"Pgrouting performance\",\n       subtitle=\"Naive method on dijkstra,bddijkstra, astar, bdastar\") +\n  geom_line(aes(color=algorithm)) +\n   geom_point(aes(color=algorithm)) +\n  theme_minimal()\n\n\n\n\n\n\n\nA.4.2 pgr_astrar()\n\n\nShow the code\nEXPLAIN ANALYZE\nCREATE TABLE centrality_10_10_porto_astrar AS\nSELECT  \n    b.ogc_fid,\n     b.the_geom,\n    count(the_geom) as centrality \nFROM pgr_astar(\n    'SELECT ogc_fid AS id,\n            fromid AS source,\n            toid AS target,\n            weight AS cost,\n            x1,\n            y1,\n            x2,\n            y2\n    FROM porto_alegre_net_largest_astar',\n    ARRAY(SELECT net_id FROM  random_10_origin),\n    ARRAY(SELECT net_id FROM  random_10_destination),\n             directed:=TRUE,\n             heuristic:=2) j\n                      left JOIN porto_alegre_net_largest_astar AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n\n--- check max centrality                     \nSELECT max(centrality) FROM centrality_10_10_porto_astrar ;\n--- check max rows\nSELECT  count(*) FROM centrality_10_10_porto_astrar;\n\n\n\n\nA.4.3 pgr_bdijkstra\n\n\nShow the code\n\n\n\n\n\nA.4.4 pgr_bdastar()\n\n\nShow the code\nEXPLAIN ANALYZE\nCREATE TABLE centrality_100_100_porto_bdastrar AS \nSELECT    b.ogc_fid,\n          b.the_geom,\n          count(the_geom) as centrality\nFROM \n  pgr_bdAstar(\n      'SELECT\n          ogc_fid AS id,\n          fromid AS source,\n          toid AS target,\n          weight AS cost,\n          x1,\n          y1,\n          x2,\n          y2  \n  FROM\n    porto_alegre_net_largest_astar',\n    ARRAY(SELECT net_id FROM  random_100_origin),\n    ARRAY(SELECT net_id FROM  random_100_destination),\n    directed:=TRUE,\n    heuristic:=2) j\n    LEFT JOIN \n      porto_alegre_net_largest_astar AS b\n      ON j.edge = b.ogc_fid\n      GROUP BY  b.ogc_fid, b.the_geom\n      ORDER BY centrality DESC;  \n\n\n\n\nA.4.5 Other code\n\n\nShow the code\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\nShow the code\nCREATE TABLE random_272_destination  AS\nwith random_272_destination AS (\n        SELECT\n            * \n        FROM \n            od_40420_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  random_272_destination;  \n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5 \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,\n         random_272_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\nCREATE TABLE porto_272_272_dijkstra AS\n WITH pgr_result AS (\n   SELECT pgr_dijkstra('SELECT id,\n           source,\n    target,\n     cost FROM porto_alegre_net_largest',\n     array_agg(fv), array_agg(tv), \n     directed := true\n   ) FROM vertices_lookup_v5\n )\nSELECT (pgr_dijkstra).*, a.fid, a.tid FROM pgr_result\nJOIN vertices_lookup_v5 a\nON (pgr_dijkstra).start_vid = a.fv\nAND (pgr_dijkstra).end_vid = a.tv;\n\n\n\n\nShow the code\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\n\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;"
  },
  {
    "objectID": "future_work.html#visualization",
    "href": "future_work.html#visualization",
    "title": "5  Future work & suggestions",
    "section": "5.4 Visualization",
    "text": "5.4 Visualization\n\n5.4.1 Data\n\n\nShow the code\nCREATE TABLE centrality_both_sankey  AS\nWITH centrality_both_sankey AS (\nSELECT  centrality_pre.bidirectid,\n        centrality_pre.centrality AS centrality_pre,\n        centrality_post.centrality AS centrality_post,\n        centrality_pre.the_geom AS pre_geom,\n        centrality_post.the_geom AS post_geom\nFROM centrality_weighted_100_bidirect_cleaned AS centrality_pre\nFULL OUTER JOIN centrality_weighted_100_bidirect_cleaned_post AS centrality_post\n  ON centrality_pre.bidirectid = centrality_post.bidirectid)\nSELECT \n    bidirectid,\n    centrality_pre,\n    centrality_post,\n    COALESCE(ST_Length(pre_geom::geography)/100, ST_Length(post_geom::geography)/1000) AS geom_length\nFROM \n    centrality_both_sankey;\n\n\n\n\n5.4.2 plot\n\n\nShow the code\nlibrary(tidyverse)\nlibrary(ggsankey)\n\n### Flow Chart\ncentrality_both_sankey &lt;- DBI::dbReadTable(eisenberg_connection, \"centrality_both_sankey\")\n\ndf_centrality_both_sankey &lt;- centrality_both_sankey |&gt; \n  mutate(pre_scenario=\n           as.factor(case_when(\n             centrality_pre == 0 ~ \"[0]\",\n             centrality_pre &gt; 0 & centrality_pre &lt;= 146 ~ \"[1-146]\",\n             centrality_pre &gt;= 146 & centrality_pre &lt;=345 ~ \"[146-345]\",\n             centrality_pre &gt;= 345 & centrality_pre &lt;= 633  ~ \"[345-633]\",\n             centrality_pre &gt;= 633 ~ \"[633-1644]\",\n             TRUE ~ \"[0]\")),\n         post_scenario=\n           as.factor(case_when(\n             centrality_post == 0 ~ \"[0]\",\n             centrality_post &gt; 0 & centrality_post &lt;= 146 ~ \"[1-146]\",\n             centrality_post &gt;= 146 & centrality_post &lt;=345 ~ \"[146-345]\",\n             centrality_post &gt;= 345 & centrality_post &lt;= 633  ~ \"[345-633]\",\n             centrality_post &gt;= 633 ~ \"[633-1644]\",\n             TRUE ~ \"[0]\"))) \n\n# HeiGIt --- Create a long format dataset for ggsankey\ndf_centrality_both_sankey_long &lt;- df_centrality_both_sankey |&gt; \n  mutate(\n    pre_scenario = as.character(pre_scenario),\n    post_scenario = as.character(post_scenario)) |&gt; \n  ggsankey::make_long(5:6) \n\n# Create an initial Sankey diagram (not used in final plot, but used to extract data)\nmycolour &lt;- c(\"#edf8fb\",\"#b3cde3\",\"#8c96c6\",\"#8856a7\",\"#810f7c\")\nnames(mycolour) &lt;- c(unique(df_centrality_both_sankey_long$node)[3],\n                     unique(df_centrality_both_sankey_long$node)[1],\n                     unique(df_centrality_both_sankey_long$node)[2],\n                     unique(df_centrality_both_sankey_long$node)[4],\n                     unique(df_centrality_both_sankey_long$node)[5])\nauxiliar_data &lt;-df_centrality_both_sankey_long |&gt; group_by(node, x) |&gt; count()\ndf_centrality_both_sankey_long$node &lt;- factor(df_centrality_both_sankey_long$node, levels=c(\"[0]\",\n                                                                                            \"[1-146]\",\n                                                                                            \"[146-345]\",\n                                                                                            \"[345-633]\",\n                                                                                            \"[633-1644]\")) \np &lt;- ggplot(df_centrality_both_sankey_long, aes(x = x,\n                                                next_x = next_x,\n                                                node = node,\n                                                next_node = next_node,\n                                                fill = factor(node),\n                                                label = node)) +\n  geom_sankey(flow.alpha = 0.6) +\n  geom_sankey_text() +\n  scale_fill_manual(values = mycolour) + \n  geom_sankey_label( fill=\"white\", alpha=.7) +\n  theme_minimal() +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"grey99\", color = NA)\n  )\n\n# Extract flow, node, and label data from the Sankey diagram\nmoves_flows &lt;- layer_data(p, 1) %&gt;% \n  mutate(height = flow_end_ymax - flow_end_ymin)\n\nmoves_nodes &lt;- layer_data(p, 2)\n\nmoves_labels &lt;- layer_data(p, 3)\n\nf1 &lt;- \"Graphik\"\nf1b &lt;- \"Graphik Compact\"\nf2 &lt;- \"Publico Headline\"\n\n# Apply factor levels to ensure matching with mycolour\nmoves_nodes$label &lt;- factor(moves_nodes$label, levels = names(mycolour))\nmoves_flows$label &lt;- factor(moves_flows$label, levels = names(mycolour))\n\np1 &lt;- ggplot() +\n  # Add flow polygons, highlighting the most common moves and using the correct fill mapping\n  geom_polygon(data = moves_flows, aes(x, y, group = group, fill = label)) +\n  # Add node rectangles with color based on the label\n  geom_rect(data = moves_nodes, aes(xmin = xmin, xmax = xmax, ymin = ymin, ymax = ymax, fill = \"white\"), fill=\"white\") +\n  # Manually scale the fill using the mycolour palette\n  scale_fill_manual(values = mycolour) +\n  # Add labels for the most common moves\n  geom_text(data = moves_labels,\n            aes(x, y, label = label),\n            family = f1b,\n            color = \"black\",\n            size = 6) +\n  labs(\n    title = \"Changes on Connectivity\",\n    subtitle = \"Porto Alegre Settlement\",\n    caption = \"Source: OpenStreetMap · Graphic: Ricardo Ruiz Sánchez inspired by Georgios Karamanis\"\n  ) +\n  theme_void(base_family = f1) +\n  theme(\n    legend.position = \"none\",\n    plot.background = element_rect(fill = \"white\", color = NA),\n    plot.margin = margin(10, 10, 10, 10),\n    plot.title = element_text(size = 30, face = \"bold\", family = f2),\n    plot.subtitle = element_text(size = 15, lineheight = 1, color = \"#676162\"),\n    plot.caption = element_text(hjust = 0, size = 12, \"#676162\")\n  )\n\nggsave(\"plot_flow_chart.jpg\", plot= p1, dpi= 300, width= 10, height= 15)\n\n## Length roads\n### pre-length\ndf_length_pre &lt;- centrality_both_sankey |&gt; select(bidirectid, centrality_pre, geom_length) |&gt;\n  rename(centrality = centrality_pre) |&gt; \n  mutate(\n  category_centrality=\n    as.factor(case_when(\n      centrality == 0 ~ \"[0]\",\n      centrality &gt; 0 & centrality &lt;= 146 ~ \"[1-146]\",\n      centrality &gt;= 146 & centrality &lt;=345 ~ \"[146-345]\",\n      centrality &gt;= 345 & centrality &lt;= 633  ~ \"[345-633]\",\n      centrality &gt;= 633 ~ \"[633-1644]\",\n      TRUE ~ \"[0]\"))) |&gt; \n  dplyr::select(c(geom_length, category_centrality)) |&gt; \n  group_by(category_centrality) |&gt;\n  summarise(total_lenth=sum(geom_length))\ndf_length_pre$event &lt;- \"Road length (%)\"\ndf_length_pre$pct &lt;- as.numeric(round(df_length_pre$total_lenth/(sum(df_length_pre$total_lenth)),4))\n\n## pre-length-plot\nggplot(df_length_pre, aes(x = event, y = pct, fill = category_centrality)) +\n  geom_col() +\n  coord_flip() +\n  scale_fill_manual(values = mycolour) +\n  #theme_minimal() +\n  ggthemes::theme_clean() +\n  labs(x=\"\", y=\"\") +\n  theme(\n    legend.position = \"none\",\n    legend.text = element_text(size = 7),\n    legend.title = element_text(size = 7),\n    legend.background = element_blank(),\n    plot.background = element_blank(),\n    plot.caption = element_text(face = \"italic\",\n                                size = 8,\n                                color = \"grey30\"\n    )\n  ) +\n  guides (\n    fill = guide_legend(nrow =2, byrow = TRUE)\n  ) \n\n## Centralities values\ndf_barplots_sankey &lt;- df_centrality_both_sankey |&gt;\n  pivot_longer(cols=c(\"post_scenario\",\"pre_scenario\"),\n               names_to =\"event\",\n               values_to = \"quartile\")\ncentrality_both_longer &lt;- centrality_both |&gt; pivot_longer(cols=c(pre_scenario, post_scenario),\n                                names_to = \"quartile\",\n                                values_to = \"category\") \ndf_barplots_sankey &lt;- centrality_both_longer |&gt; \n  dplyr::select(c(centrality, quartile, category)) |&gt;\n  group_by(quartile, category) |&gt; summarise(centrality=sum(centrality)) |&gt;\n  mutate(total_perc = centrality/sum(centrality))\n\nggplot(df_barplots_sankey, aes(x = quartile, y = total_perc, fill = category)) +\n  geom_col() +\n  scale_x_discrete(labels=c(\"centrality_post\" = \"Post-flooding\", \"centrality_pre\" = \"Pre-flooding\")) +\n  coord_flip() +\n  scale_fill_manual(values = mycolour) +\n  labs(\n    x = \"Event\",\n    y = \"Centrality (%)\",\n    title = \"Share of centrality per range in the two scenarios\"\n  ) +\n  #theme_minimal() +\n  ggthemes::theme_clean() +\n  theme(\n    legend.position = \"bottom\",\n    legend.text = element_text(size = 7),\n    legend.title = element_text(size = 7),\n    legend.background = element_blank(),\n    plot.background = element_blank(),\n    plot.caption = element_text(face = \"italic\",\n                                size = 8,\n                                color = \"grey30\"\n    )\n  ) +\n  guides (\n    fill = guide_legend(nrow = 1, byrow = TRUE)\n  )"
  }
]