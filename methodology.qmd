```{r}
#| echo: false
#| output: false
#| warning: false

lapply(c("tidyverse","DT","leaflet","sf","htmltools", "DBI", "mapview","leafpop","RPostgres","dplyr","leaflet","leafsync","terra","raster","stars", "lwgeom","leaflet.extras2","RColorBrewer","tidygeocoder"),
       require,
       character.only =T)
eisenberg_connection <- DBI::dbConnect(RPostgres::Postgres(),
                          user= "docker",
                          password = "docker",
                          host = "localhost",
                          dbname="gis",
                          port = 25432)

```

# Methodology

## Framework
Three major research phases or lanes constituted the methodology.

1. The first lane defined an area of interest (AOI) 
can be identified using sources such as Copernicus EMS Rapid Mapping.
2. The second lane transforms this AOI into a network and graph using
openrouteservice. Additionally, in this phase called "routable network" lane, the
post-disaster network is derived through spatial overlay with the extent of flooding.
3. Lastly, centrality analysis, which includes connectivity and accessibility metrics, is
conducted on the routable network. This analysis is currently done in R and the
technical goal is the implementation of PostgreSQL.

![](/media/workflow_methodology.png)

## Selecting the Area of Interest (AoI) and importing data

The different study areas were in Rio Grande do Sul, which is located in the southernmost region of Brazil bordered by Uruguay in the south and Argentina to the west. Its total extension of 281.707 km² includes 497 municipalities with 274,390 population exposed to risk areas [@noauthor_caracterizacao_nodate].. The first study area lies between 51º 27'91''W and 50º94'07''W latitude and 30º17'22''S and 29º80'48''S being identified as the dense urban centre 1210 according to the GHL-SMOD. This area contains the municipalities of Porto Alegre, Canoas, Cachoeirinha,Alvorada, Gravataí, Esteio,Sapucaia do Sul and Viamão.

###  Data

```{r}
#| echo: false
#| warning: false
#| message: false

data_df <- read_csv('/home/ricardo/HeiGIT-Github/data_required_porto_alegre/requried_data.csv') 

data_df$Link <-  paste0('<a href="', data_df$Link ,'">', 'Link </a>')

DT::datatable(data_df, class='compact', rownames=FALSE, escape= FALSE, caption = 'Data description',
              extensions="Buttons",
                         options=list(
                           dom="Bfrtip",
                           buttons=c("copy","csv","pdf"),
                            initComplete = JS(
    "function(settings, json) {",
    "$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});",
    "}")
                                  )
              ) 
```


#### Importing data into PostgreSQL

The tool ogr2ogr imported the data into a PostgreSQL database. The output file format parameter -f included the PostgreSQL connection components, while the  parameters -nln and -lco assigned the name of the table and columns respectively. For the OSM network, the parameter -nlt adjusted the multilinestring geometry to linestring to process single features more efficiently as indicated in the bibliography [@obe_pgrouting_2017].

```{r}
#| eval: false
#| code-summary: "PostGIS: Importing data to PostgreSQL using ogr2ogr"

## OSM geometry: porto_alegre_net_pre.geojson
ogr2ogr -f PostgreSQL PG:"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_net_pre.geojson -nln porto_alegre_net_pre -lco GEOMETRY_NAME=the_geom -nlt LINESTRING -explodecollections 

## Administrative units: nuts.geojson
ogr2ogr -f PostgreSQL PG:"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/nuts.geojson -nln nuts -lco GEOMETRY_NAME=geom

## Flooding extent: flooding_rio_grande_do_sul.geojson
ogr2ogr -f PostgreSQL PG:"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/flooding_rio_grande_do_sul.geojson -nln flooding_rio_grande_do_sul -lco GEOMETRY_NAME=the_geom 

## Building density: urban_center_4326.geojson
ogr2ogr -f PostgreSQL PG:"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/urban_center_4326.geojson -nln urban_center_4326 -lco GEOMETRY_NAME=geom 

## Hospitals
### From Rio Grande do Sul Geoportal
ogr2ogr -f PostgreSQL PG:"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/Hospitais_com_Leitos_de_UTIs_no_RS.geojson -nln hospitals_bed_rs -lco GEOMETRY_NAME=geom
```


### Preparing the flooding mask

The downloaded flooding extent covered a larger area in Rio Grande do Sul. However, the area of interest that contained urban dense cities such as Porto Alegre was smaller. Therefore, a series of operations reduced the area focusing on Porto Alegre and at the same time improved the query performance.

Subdividing the flooding mask to make the spatial indexes more efficient was the first step. The reason was the higher number of vertices of large objects and larges bounding boxes that hinder the spatial index performance ([Link](https://blog.cleverelephant.ca/2019/11/subdivide.html)). After enabling the spatial indexes, selecting only the flooding subunits that intersected with the area of interest reduced the size of the region query. Previous studies reported that reducing the number of spatial polygons and points of the region saved processing time and computation costs [@zhao_novel_2017]. Additionally, the following code dissolve the borders to break the multipolygon into simple polygons to improve the performance of functions such as st_difference() ([link](https://icarto.es/en/improve-performance-making-the-difference-between-two-layers-with-postgis/)).

```{sql}
#| eval: false
#| code-summary: "PostGIS: Subdiving flooding extent and removing slivers"

--- 1) Subdividing to make spatial indexes more efficent
--- Select the GHS area that intersects with Porto Alegre
CREATE TABLE flooding_subdivided_porto AS
WITH porto_alegre_ghs AS(
    SELECT 
       ghs.*
    FROM
       urban_center_4326 AS ghs
    JOIN
       nuts 
    ON 
       st_intersects(nuts.geom, ghs.geom)
    WHERE 
        nuts.shapename = 'Porto Alegre'),
--- Bounding Box that contained the GHS in Porto Alegre
porto_alegre_ghs_bbox AS(
    SELECT 
        st_setsrid(st_extent(geom),4326) as geom_bbox
    FROM 
        porto_alegre_ghs),
---Subdivide the flood extent to increase spatial indexes performance 
flooding_sul_subdivided AS (
        SELECT 
            st_subdivide(geom) as the_geom
        FROM
            flooding_rio_grande_do_sul) 
---- Select the flooding subunits that intersects with the previous bounding box
SELECT 
    flooding_sul_subdivided.* 
FROM 
    flooding_sul_subdivided, 
    porto_alegre_ghs_bbox
WHERE 
    st_intersects(flooding_sul_subdivided.the_geom, porto_alegre_ghs_bbox.geom_bbox);
--- 2) Simplifying geometry
CREATE TABLE flooding_cleaned_porto AS
    SELECT
        (ST_Dump(the_geom)).geom::geometry(polygon, 4326) geom 
    FROM 
        flooding_subdivided_porto;
    
CREATE TABLE flooding_cleaned_porto_union AS
    SELECT
        ST_Union(geom)::geometry(multipolygon, 4326) geom 
    FROM 
        flooding_cleaned_porto;
    
CREATE TABLE flooding_cleaned_porto_union_simple AS
    SELECT 
        (ST_Dump(geom)).geom::geometry(polygon, 4326) geom 
    FROM 
        flooding_cleaned_porto_union;
--- This allowed to calculate the area of each polygon finding slivers that were removed using the following code. 
SELECT COUNT(*)
     FROM flooding_cleaned_porto_union_simple ; --- count= 54.

DELETE FROM 
  flooding_cleaned_porto_union_simple
WHERE 
  ST_Area(geom) < 0.0001; ---count= 2.
  
--- Obtain Area
SELECT 
  sum(st_area(geom::geography)/10000)::integer 
FROM  
  flooding_cleaned_porto_union_simple;
--- Obtain size in memory
SELECT 
  pg_size_pretty(SUM(ST_MemSize(geom))) 
FROM  
  flooding_cleaned_porto_union_simple;
--- Obtain number of points
SELECT 
  sum(ST_Npoints(geom)) 
FROM  
  flooding_cleaned_porto_union_simple;

```

```{r}
#| echo: false
#| code-summary: "R: Representing results in dinamic table "

df_flooding <- data.frame(
    table = c("flooding_rio_grande_do_sul","flooding_subdivided_porto","flooding_cleaned_porto","flooding_cleaned_porto_union","flooding_cleaned_porto_union_simple"),
    st_npoints=c(810007,11732,11732,11352,11352),
    st_memsize=c("13312 kB","191 kB","191 kB","179 kB","181 kB"),
    st_area=c(709518,44071,44071, 44071,44071)
)
DT::datatable(df_flooding, 
              colnames= c("Table","Nº of points","Size in memory", "Area"),
              filter="top",
              class='compact', rownames=FALSE, escape=FALSE, caption='Simplification of the flooding extent',
              extensions=c("Buttons",'RowGroup'),
              options=list(
                  order=list(list(1, 'desc'), list(3,'desc')),
                  dom="Bfrtip",
                  initComplete = JS(
                      "function(settings, json) {",
                      "$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});",
                      "}")
              )
) |> 
      DT::formatStyle("st_npoints",
     background=DT::styleColorBar(range(df_flooding$st_npoints), '#ee8b8b'),
                    backgroundSize='98% 88%',
                    backgroundRepeat='no-repeat',
                    backgroundPosition='center') 

```


## Obtaining pre and post disaster routable networks

The creation of the network after the disaster using the modified flooding mask and the creation of the origin-destination are considered in this section. The following code set the paremeters before carrying out these transformations following [post 1](https://blog.cleverelephant.ca/2019/05/parallel-postgis-4.html) and [post2](https://gis-ops.com/pgrouting-speedups/).

```{sql}
#| eval: false
#| code-summary: "PostGIS: Adjusting configuration settings"

--- Firstly, add spatial index everywhere:
CREATE INDEX idx_porto_alegre_net_largest_geom ON porto_alegre_net_largest USING gist(the_geom);
CREATE INDEX idx_porto_alegre_net_largest_source ON porto_alegre_net_largest USING btree(fromid);
CREATE INDEX idx_porto_alegre_net_largest_target ON porto_alegre_net_largest USING btree(toid);
CREATE INDEX idx_porto_alegre_net_largest_id ON porto_alegre_net_largest USING btree(ogc_fid);
CREATE INDEX idx_porto_alegre_net_largest_cost ON porto_alegre_net_largest USING btree(weight);
--- Set up the configuration
----- Increase performance by using more max_parallel_workes_per_gather
SET max_parallel_workers_per_gather =4;
----- Cluster the index
CLUSTER porto_alegre_net_largest USING porto_alegre_net_largest;
```


### Importing OpenStreetMap (OSM) network as geometry

The following SQL code created the command to import OpenStreetMap (OSM) network using osmium. Firstly using the [GHS-SMOD dataset](https://human-settlement.emergency.copernicus.eu/download.php?ds=smod) the Global Human Settlement in Porto Alegre is chosen. Secondly, this GHS selected is used to create the bounding box that it is lastly used to generate the osmium code. The OSM network obtained with this osmium command was the input required for OpenRouteService ([ORS](https://heigit.org/introducing-openrouteservice-version-8-0-a-dedication-to-wilfried-juling/)). 

```{sql}
#| eval: false
#| code-summary: "PostGIS: Generating responsive code to import OSM"
--- GHS urban area that intersected with Porto Alegre city
WITH porto_alegre_ghs AS(
	SELECT 
	   ghs.*
	FROM
	   urban_center_4326 AS ghs
	JOIN
	   nuts 
	ON 
	   st_intersects(nuts.geom, ghs.geom)
	WHERE 
		nuts.shapename = 'Porto Alegre'),
--- Bounding Box that contained the GHS in Porto Alegre
porto_alegre_ghs_bbox AS(
	SELECT 
		st_setsrid(st_extent(geom),4326) as geom_bbox
	FROM 
		porto_alegre_ghs),
--- The command used the Porto Alegre GHS and its Bounding Box to import the OpenStreet Network. 
porto_alegre_ghs_bbox_osmium_command AS	(
	SELECT 
		ST_XMin(ST_SnapToGrid(geom_bbox, 0.0001)) AS min_lon,
		St_xmax(ST_SnapToGrid(geom_bbox,0.0001)) AS max_lon,
		St_ymin(ST_SnapToGrid(geom_bbox,0.0001)) AS min_lat,
		St_ymax(ST_SnapToGrid(geom_bbox, 0.0001)) AS max_lat
	FROM porto_alegre_ghs_bbox)
SELECT
    'osmium extract -b ' 
	|| min_lon || ',' || min_lat || ',' || max_lon || ',' || max_lat || 
    ' sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf' AS osmium_command
FROM porto_alegre_ghs_bbox_osmium_command;

```


### Transforming geometric network into routable graph network using OpenRouteService (ORS).

Firstly, a [docker for OpenRouteService](https://giscience.github.io/openrouteservice/run-instance/running-with-docker) transformed the OSM network that covered the GHS in Porto Alegre into a graph. In the "ors-config.yml" from OpenRotueService (ORS), the "source_file" parameter is set with the following directory. OpenRouteService (ORS) created a a routable network, assigining costs and adding information for each node, namely, "fromId" and "toId". The R script "get_graph" from Marcel Reinmuth exported the ORS network named as "porto_alegre_net".

Secondly, the parameters start_vid, end_vid named "fromid" and "toid" in the network dataset from openrouteservice are transform into bigint data type to run the algorithm pgr_dijkstra as the [official documentation](https://docs.pgrouting.org/latest/en/pgr_dijkstra.html) indicates.  

```{sql}
#| eval: false
#| code-summary: "PostGIS: Adjusting ORS configuration & preparation for pgrouting"
--- Firstly, the source_file parameter in the ors.config.yml file is set to the OSM network previously obtained 
source_file: /home/ors/files/puerto_alegre_urban_center.osm.pbf
---- Secondly the exported ORS graph is prepared for pgrouting casting the right data type for the pgrouting functions.
select * from porto_alegre_net_pre; 
ALTER TABLE porto_alegre_net_pre 
	ALTER COLUMN "toid" type bigint,
	ALTER COLUMN "fromid" type bigint,
	ALTER COLUMN "ogc_fid" type bigint;
```


```{r}
#| warning: false
#| message: false
#| code-summary: "R: Visualizing OSM and ORS network"

## Load data
ors_network <- st_read(eisenberg_connection, layer="porto_alegre_net_pre")
osm_network <- st_read(eisenberg_connection, layer="puerto_alegre_ghs_osm")
ors_network_subset <- ors_network |> head()
## Create subset using a bounding box
ors_subset <- ors_network |> filter(id ==140210) |> st_bbox()
xrange <- ors_subset$xmax - ors_subset$xmin
yrange <- ors_subset$ymax - ors_subset$ymin
## Expand the bounding box
ors_subset[1] <- ors_subset[1] - (4 * xrange) # xmin - left
ors_subset[3] <- ors_subset[3] + (4 * xrange) # xmax - right
ors_subset[2] <- ors_subset[2] - (2 * yrange) # ymin - bottom
ors_subset[4] <- ors_subset[4] + (2 * yrange) # ymax - top
## Convert bounding box into polygon
ors_subset_bbox <- ors_subset %>%  # 
  st_as_sfc() 
## Use the polygon to subset the network
intersection_ors <- sf::st_intersection(ors_network, ors_subset_bbox)
intersection_osm <- sf::st_intersection(osm_network, ors_subset_bbox)
## Mapview maps
m1 <- mapview(intersection_osm, 
              color="#6e93ff",
              layer.name ="OpenStreetMap - Geometry",
              popup=popupTable(intersection_osm, 
                               zcol=c("id","osm_id","name","geom")))
m2 <- mapview(intersection_ors,
              color ="#d50038",
              layer.name="OpenRouteService -Graph",
              popup=popupTable(intersection_ors))
sync(m1,m2) 
```

### Exploring ORS graph network and components

Before using the graph, a quick inspection of the graph using the pgrouting function [*pgr_strongComponents()*](https://docs.pgrouting.org/dev/en/pgr_strongComponents.html) revealed how many components or isolated self-connecting network the graph had.  The largest network component is selected using the length. The following code created a table with the different components and another table containing the largest component ruling out the rest of the components with relatively small networks.

```{sql}
#| eval: false
#| code-summary: "PostGIS: Obtaining components of the network and their length"
--- 1) Quick inspection of the graph to determine components
---- Create a vertice table for pgr_dijkstra()
SELECT 
  pgr_createVerticesTable('porto_alegre_net_pre', source:='fromid', target:='toid');
  
---- Add data to the vertices created table 
CREATE TABLE component_analysis_network_porto AS
WITH porto_alegre_net_component AS (
SELECT 
  * 
FROM 
  pgr_strongComponents('SELECT ogc_fid AS id,
                               fromid AS source,
                               toid AS target,
                               weight AS cost 
                        FROM 
                              porto_alegre_net_pre')),
porto_alegre_net_component_geom AS (
SELECT 
    net.*,
    net_geom.the_geom
FROM 
    porto_alegre_net_component AS net
JOIN 
    porto_alegre_net_pre  AS net_geom
ON 
    net.node = net_geom.fromid)
SELECT 
    component,
    st_union(the_geom) AS the_geom,
    st_length(st_union(the_geom)::geography)::int AS length
FROM  
    porto_alegre_net_component_geom
GROUP BY component
ORDER BY length DESC;
--- 2) A table with the component of the network with the highest length
CREATE TABLE porto_alegre_net_largest AS
---- Obtain again table classifying nodes in different components           
WITH porto_alegre_net_component AS (
SELECT 
  * 
FROM 
  pgr_strongComponents('SELECT
                               ogc_fid AS id,
                               fromid AS source,
                               toid AS target,
                               weight AS cost 
                        FROM 
                              porto_alegre_net_pre')),
--- Calculate the largest component from the network
largest_component_net AS (
    SELECT 
        component
    FROM 
        component_analysis_network_porto 
    LIMIT 1),
--- Using the largest component from the network to filter
largeset_component_network_porto AS (
SELECT
    *  
FROM
    porto_alegre_net_component,
    largest_component_net
WHERE 
    porto_alegre_net_component.component = largest_component_net.component)
SELECT 
    net_multi_component.*
FROM 
    porto_alegre_net_pre AS net_multi_component,
    largeset_component_network_porto AS net_largest_component
WHERE  
    net_multi_component.fromid IN (net_largest_component.node);
```


```{=html}
<iframe width="760" height="500" src="/media/network_components_three.html" title = "Inspect of the components in the graph network "></iframe>
```


```{r}
#| eval: false
#| code-summary: "R: Visualizing the three longest component"

component_analysis_network <- st_read(eisenberg_connection, "component_analysis_network_porto")
## Select top 3
component_analysis_network_3 <-  component_analysis_network |> arrange(desc(distance)) |> slice(1:3) 
### Tidy component to be used as categorical variable
component_analysis_network_3$component  <- component_analysis_network_3$component |> as.character() |> as_factor()
## Visualization
m1_net <- component_analysis_network_3 |>
              filter(component=="1") |>
              mapview(layer.name = "1st Longest component",
                      lwd= 0.5,
                      color="#66c2a5")

m2_net <- component_analysis_network_3 |> 
              filter(component=="3728") |>
              mapview(layer.name = "2nd Longest component",
                      color ="#8da0cb",
                      lwd=0.5)

m3_net <- component_analysis_network_3 |>
              filter(component=="40578") |>
              mapview(layer.name = "3rd Longest component",
                      color="#fc8d62",
                      lwd =0.5)

m1_net + m2_net + m3_net
 
```



#### Pre-Disaster network

```{sql}
#| eval: false
#| code-summary: "PostGIS: Creating the pre-disaster network filtering out components"

CREATE TABLE porto_alegre_net_largest AS
---- Obtain again table classifying nodes in different components		    
  WITH porto_alegre_net_component AS (
    SELECT 
      * 
    FROM 
      pgr_strongComponents('SELECT id,
                               source,
                               target,
                               cost 
                            FROM 
                              porto_alegre_net_pre')),
--- Calculate the largest component from the network
  largest_component_net AS (
	  SELECT 
		  component
	  FROM 
	 	  component_analysis_network_porto 
	  LIMIT 1),
--- Using the largest component from the network to filter
  largeset_component_network_porto AS (
    SELECT
	    *  
    FROM
	    porto_alegre_net_component,
	  largest_component_net
    WHERE 
	    porto_alegre_net_component.component = largest_component_net.component)
    SELECT 
	    net_multi_component.*
    FROM 
	    porto_alegre_net_pre AS net_multi_component,
	    largeset_component_network_porto AS net_largest_component
    WHERE  
	    net_multi_component.source IN (net_largest_component.node);
```

#### Post-Disaster network

A naive approach overlaying the flooding mask with the road network by the function *st_difference()* crashed the session.  The follwing multi-step methodology reduced the processing cost making the query feasible using less resources by incorporating boolean operators in some steps. In other studies, decomposing difference function queries using boolean operators reported an increase on the performance of 223% in PostgreSQL [@gruca_spatial_2014]. Firstly the network inside the flooding mask is selected. Secondly, a subset of the network outside the flooding mask avoided using spatial operations saving computing resources by using the ID's from the network inside the flooding to filter the data.Thirdly, to obtain the boundaries between the inside and outside network, the geometry is converted into its exterior ring. From this network on the boundary, only the exterior part that intersected with the outside boundary was selected.   

```{sql}
#| eval: false
#| code-summary: "PostGIS: Multi-step methodology to create the post-disaster network"

--- 1) Network inside the flooding mask
CREATE TABLE porto_alegre_street_in_v2 AS
SELECT net.id,
    CASE 
        WHEN ST_Contains(flood.the_geom, net.the_geom)
        THEN net.the_geom
        ELSE st_intersection(net.the_geom, flood.the_geom)
    END AS  geom
FROM porto_alegre_net_largest net
JOIN flooding_subdivided_porto flood
ON st_intersects(net.the_geom, flood.the_geom);
--- 2) Network outside the flooding mask
CREATE TABLE porto_alegre_street_out_v2 AS
SELECT net.*
FROM porto_alegre_net_largest net
WHERE net.id NOT IN (
    SELECT net.id
    FROM porto_alegre_street_in_v2 net);
--- 3) Network on the boundaries
CREATE TABLE porto_alegre_net_outside_v2 AS
WITH porto_alegre_ghs AS(
    SELECT 
       ghs.*
    FROM
       urban_center_4326 AS ghs
    JOIN
       nuts 
    ON 
       st_intersects(nuts.geom, ghs.geom)
    WHERE 
        nuts.shapename = 'Porto Alegre'),
--- Bounding Box that contained the GHS in Porto Alegre
porto_alegre_ghs_bbox AS(
    SELECT 
        st_setsrid(st_extent(geom),4326) as geom_bbox
    FROM 
        porto_alegre_ghs),
flooding_sul_subdivided AS (
        SELECT 
            st_subdivide(geom) as the_geom
        FROM
            flooding_rio_grande_do_sul),
exterior_ring_porto_alegre_v2 AS (
SELECT 
    ST_ExteriorRing((ST_Dump(union_geom)).geom) as geom
FROM (
    SELECT 
        ST_Union(flood.the_geom) as union_geom
    FROM 
        porto_alegre_ghs_bbox as bbox
    JOIN 
        flooding_sul_subdivided as flood
    ON 
        ST_Intersects(flood.the_geom, bbox.geom_bbox)
) AS subquery)
SELECT net.id,
    CASE
        WHEN NOT ST_Contains(flood.geom, net.the_geom)
        THEN net.the_geom
            ELSE st_intersection(net.the_geom, flood.geom)
    END AS  geom,
    net.target,
       net.source,
       cost,
       "unidirectid",
       "bidirectid"
FROM
porto_alegre_net_largest AS net
JOIN exterior_ring_porto_alegre_v2 flood ON
st_intersects(net.the_geom, flood.geom);

---- For the network
CREATE INDEX idx_porto_alegre_net_outside_v2 ON porto_alegre_net_outside_v2 USING gist (geom);

CLUSTER porto_alegre_net_outside_v2 USING idx_porto_alegre_net_outside_v2;

--- For the flooding mask
CREATE INDEX flooding_sul_subdivided_idx ON flooding_sul_subdivided USING gist (the_geom);

CLUSTER flooding_sul_subdivided USING flooding_sul_subdivided_idx;
---- Before doing difference
VACUUM(FULL, ANALYZE) porto_alegre_net_outside_v2;
VACUUM(FULL, ANALYZE) flooding_sul_subdivided;
--- st_difference and uniting the external to the boundaries
---  Unite the subunits of the flooding
CREATE TABLE flooding_symple as 
SELECT st_union(geom) as the_geom FROM flooding_cleaned_porto_union_simple;
---- Index the flooding
CREATE INDEX flooding_symple_idx ON flooding_symple USING gist (the_geom);
CLUSTER flooding_symple USING flooding_symple_idx;
--- 4) Obtain the boundary network that intersect with the outside network
CREATE TABLE difference_outside_flood_v3 AS
SELECT net.id,
        target,
        source,
        cost,
        unidirectid,
        bidirectid,
st_difference(net.geom, flood.the_geom) AS the_geom
FROM porto_alegre_net_outside_v2 AS net,
flooding_symple  AS flood;

--- 5) Unite outside network with the external part of the boundary network
CREATE TABLE porto_alegre_net_post_v5 AS
SELECT *
FROM porto_alegre_street_out_v2
UNION
SELECT *
FROM difference_outside_flood_v3;

```

```{r}
#| eval: false
#| code-summary: "R: Visualizing the multi-step methodology"
porto_alegre_net_pre <- sf::st_read(eisenberg_connection, "porto_alegre_net_largest")
centrality_pre <- sf::st_read(eisenberg_connection, "centrality_weighted_100_bidirect_cleaned")
flooding <- sf::st_read(eisenberg_connection, "flooding_cleaned_porto")
net_in <- sf::st_read(eisenberg_connection, "porto_alegre_street_in_v2")
## subset
subset_network_pre <-  sf::st_read(eisenberg_connection, "porto_alegre_net_largest_subset")
subset_network_in <- sf::st_read("/home/ricardo/heigit_bookdown/data/porto_alegre_street_in_v3_subset.geojson") |> select("geometry")
subset_network_out <- sf::st_read("/home/ricardo/heigit_bookdown/data/porto_alegre_street_out_subset.geojson")  
subset_network_outside_flood <-sf::st_read("/home/ricardo/heigit_bookdown/data/porto_alegre_street_outside_subset.geojson") 
subset_network_post <- sf::st_read(eisenberg_connection, "reet_united_v3")
flooding <- sf::st_read(eisenberg_connection, "flooding_symple") |> st_as_sf() 
## centrality
mapview(subset_network_pre,
          color="#d4e7e7",
          lwd= 1,
          layer.name="0.Pre-flooding network",
          popup=popupTable(subset_network_pre,
          zcol=c("id","source","target","bidirectid"))) +
  mapview(flooding,
          color="darkblue",
          alpha.regions= 0.5,
          layer.name="0.Flooding layer") +
  mapview(subset_network_in,
          color="red",
          lwd= 1,
          hide = TRUE,
          layer.name="1.Network inside the flooding") +
  mapview(subset_network_out,
          color="yellow",
          lwd= 1.2,
          hide = TRUE,
          layer.name="2.Flooding outside the flooding mask",
          popup=popupTable(subset_network_out)) +
  mapview(subset_network_outside_flood,
          color="lightgreen",
          lwd= 1.4,
          hide = TRUE,
          layer.name="3 & 4. Network on the boundaries",
          popup=popupTable(subset_network_outside_flood)) +
    mapview(subset_network_post,
          color="green",
          lwd= 1,
          hide =TRUE,
          layer.name="5. Uniting network external to the boundires and network from outside",
          popup=popupTable(subset_network_post))
```


```{=html}
<iframe width="760" height="500" src="/media/network_post_creation.html" title = "Methodology to create post-flooding network "></iframe>
```

### Post-scenario

```{sql}
#| eval: false
#| code-summary: "PostGIS: Creating the post-disaster network"

---- Create vertices
SELECT pgr_createverticesTable('porto_alegre_net_post_v5');
----- Obtain components to be used later as a filter
CREATE TABLE component_analysis_network_porto_post AS
WITH porto_alegre_net_component_post AS (
SELECT 
  * 
FROM 
  pgr_strongComponents('SELECT id,
                               source,
                               target,
                               cost 
                        FROM 
                              porto_alegre_net_post_v5')),
porto_alegre_net_component_geom_post AS (
SELECT 
    net.*,
    net_geom.the_geom
FROM 
    porto_alegre_net_component_post AS net
JOIN 
    porto_alegre_net_post_v5  AS net_geom
ON 
    net.node = net_geom.source)
SELECT 
    component,
    st_union(the_geom) AS the_geom,
    st_length(st_union(the_geom)::geography)::int AS length
FROM  
    porto_alegre_net_component_geom_post
GROUP BY component
ORDER BY length DESC;
--- Summarise by component
CREATE TABLE components_network_post AS
WITH porto_alegre_net_component_post AS (
SELECT 
  * 
FROM 
  pgr_strongComponents('SELECT
                               id,
                               source,
                               target,
                               cost 
                        FROM 
                              porto_alegre_net_post_v5')),
--- Calculate the largest component from the network
largests_component_net_post AS (
    SELECT 
        component,
        the_geom,
        length::int
    FROM 
        component_analysis_network_porto_post)
SELECT * FROM largests_component_net_post;

---- Visualize which are more important
CREATE TABLE main_components_post AS
SELECT 
	* 
FROM 
	components_network_post
WHERE
	component IN (21,14,5187);

-------------- Remember to justify why 5

CREATE TABLE prueba_largest_network_post AS
WITH porto_alegre_net_component_post AS (
SELECT 
  * 
FROM 
  pgr_strongComponents('SELECT
                               id,
                               source,
                               target,
                               cost 
                        FROM 
                              porto_alegre_net_post_v5')),
--- Calculate the largest component from the network
largest_component_net_post AS (
    SELECT 
        component
    FROM 
        component_analysis_network_porto_post
    LIMIT 5),
--- Using the largest component from the network to filter
largeset_component_network_porto_post AS (
SELECT
    *  
FROM
    porto_alegre_net_component_post,
    largest_component_net_post
WHERE 
    porto_alegre_net_component_post.component = largest_component_net_post.component)
SELECT 
    net_multi_component.*
FROM 
    porto_alegre_net_post_v5 AS net_multi_component,
    largeset_component_network_porto_post AS net_largest_component
WHERE  
    net_multi_component.source IN (net_largest_component.node);
--- Snap 2728 points to post-scenario network
----
CREATE TABLE od_2728_snapped_post AS
SELECT DISTINCT ON (net.id)
       pt.id AS pt_id,
       net.id AS net_id,
       net.the_geom
FROM 
(select * 
FROM 
    od_2728 as pt) as pt
CROSS JOIN
LATERAL (SELECT
        * 
        FROM prueba_largest_network_post_vertices_pgr AS net
         ORDER BY net.the_geom <-> pt.geometry 
        LIMIT 1) AS net;
--- Create 100 origin samples from the snapped points post-scenario

CREATE TABLE weight_sampling_100_origin_post  AS
WITH porto_100_origin AS (
        SELECT
            * 
        FROM 
            od_2728_snapped_post 
        ORDER BY random() LIMIT 100)
        SELECT * FROM  porto_100_origin;  
--- Create 100 destination from the snapped points post-scenario

CREATE TABLE weight_sampling_100_destination_post  AS
WITH porto_100_destination AS (
        SELECT
            * 
        FROM 
            od_2728_snapped_post 
        ORDER BY random() LIMIT 100)
        SELECT * FROM  porto_100_destination;  
--- Indeces on origin
CREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);
CREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);
CREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);
--- Indeces on destination
CREATE INDEX weight_sampling_100_destination_post_idx_pt_id ON weight_sampling_100_destination_post USING btree(pt_id);
CREATE INDEX weight_sampling_100_destination_post_idx_net_id ON weight_sampling_100_destination_post USING btree(net_id);
CREATE INDEX weight_sampling_100_destination_post_idx_the_geom ON weight_sampling_100_destination_post USING gist(the_geom);
---- Indexes on origin
CREATE INDEX weight_sampling_100_origin_post_idx_pt_id ON weight_sampling_100_origin_post USING btree(pt_id);
CREATE INDEX weight_sampling_100_origin_post_idx_net_id ON weight_sampling_100_origin_post USING btree(net_id);
CREATE INDEX weight_sampling_100_origin_post_idx_the_geom ON weight_sampling_100_origin_post USING gist(the_geom);
---- Index on the new network
CREATE INDEX prueba_largest_network_post_idx_GEOM ON prueba_largest_network_post USING gist(the_geom);
CREATE INDEX prueba_largest_network_post_idx_id ON prueba_largest_network_post USING btree(id);
CREATE INDEX prueba_largest_network_post_idx_target ON prueba_largest_network_post USING btree(target);
CREATE INDEX prueba_largest_network_post_idx_source ON prueba_largest_network_post USING btree(source);
CREATE INDEX prueba_largest_network_post_idx_cost ON prueba_largest_network_post USING btree(cost);
---- Cluster 
CLUSTER prueba_largest_network_post USING prueba_largest_network_post_idx_GEOM;
---- Vacuum clean
VACUUM(full, ANALYZE) weight_sampling_100_origin;
VACUUM(full, ANALYZE) weight_sampling_100_destination;
VACUUM(full, ANALYZE) porto_alegre_net_largest;    

```


## Measuring centrality on networks

### Generating Origin-Destination

#### Sampling using regular distribution

A series of regular points that represented the origin and destination on the Area of Interest is created with the function  ["I_Grid_Point_Series"](https://gis.stackexchange.com/questions/4663/creating-regular-point-grid-inside-polygon-in-postgis).

The following code created 1258 points representing the origin or destination regularly separated by 0.01º.

The application of two indexes improved further queries on this new table. This was recommended because these points were outside the network requiring snapping, which is a spatial operation with relatively high computational costs. 
Apart from indexing the geometry column and the id, the query is constrained to a buffer of 0.02º to reduce computational costs.

```{sql}
#| eval: false
#| code-summary: "PostGIS: Creating a function to sample regularly"

--- Creating the function that sample regularly:
CREATE OR REPLACE FUNCTION I_Grid_Point_Series(geom geometry, x_side decimal, y_side decimal, spheroid boolean default false)
RETURNS SETOF geometry AS $BODY$
DECLARE
x_max decimal;
y_max decimal;
x_min decimal;
y_min decimal;
srid integer := 4326;
input_srid integer;
x_series DECIMAL;
y_series DECIMAL;
BEGIN
CASE st_srid(geom) WHEN 0 THEN
  geom := ST_SetSRID(geom, srid);
  RAISE NOTICE 'SRID Not Found.';
    ELSE
        RAISE NOTICE 'SRID Found.';
    END CASE;

    CASE spheroid WHEN false THEN
        RAISE NOTICE 'Spheroid False';
    else
        srid := 900913;
        RAISE NOTICE 'Spheroid True';
    END CASE;
    input_srid:=st_srid(geom);
    geom := st_transform(geom, srid);
    x_max := ST_XMax(geom);
    y_max := ST_YMax(geom);
    x_min := ST_XMin(geom);
    y_min := ST_YMin(geom);
    x_series := CEIL ( @( x_max - x_min ) / x_side);
    y_series := CEIL ( @( y_max - y_min ) / y_side );
RETURN QUERY
SELECT st_collect(st_setsrid(ST_MakePoint(x * x_side + x_min, y*y_side + y_min), srid)) FROM
generate_series(0, x_series) as x,
generate_series(0, y_series) as y
WHERE st_intersects(st_setsrid(ST_MakePoint(x*x_side + x_min, y*y_side + y_min), srid), geom);
END;
$BODY$ LANGUAGE plpgsql IMMUTABLE STRICT;
--- Using this function to create the sampling table
CREATE TABLE regular_point_od AS (
WITH multipoint_regular AS(
select 
    I_Grid_Point_Series(geom, 0.01,0.01, false) AS geom
    from porto_alegre_bbox as geom),
point_regular AS(
SELECT 
    st_setsrid((st_dump(geom)).geom, 4326)::geometry(Point, 4326) AS geom
FROM  multipoint_regular)
SELECT 
    row_number() over() AS id,
    geom
FROM 
    point_regular);
--- Applying indexes
CREATE INDEX idx_regular_point_od_geom ON regular_point_od USING GIST (geom);
CREATE INDEX idx_regular_point_od_id ON regular_point_od USING btree(id);
```

#### Sampling using weighted distribution

In R...

```{r}
#| eval: false 
#| code-summary: "R: Weighted sampling based on the built up density"

# Import data
## raster
buildings <- terra::rast('/home/ricardo/HeiGIT-Github/do_not_push_too_large/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif')
pop_ghs <- 2728
## AoI
aoi_bbox <- sf::read_sf('/home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_bbox_derived.geojson')
## reproject for clipping
aoi_bbox_reproj <- aoi_bbox |> st_transform(crs(buildings)) |> as_Spatial()
build_cropped <- crop(buildings, aoi_bbox_reproj)
## reproject for sampling
build_4326 <- terra::project(build_cropped, crs(aoi_bbox))
## weighted sampling
od <- spatSample(build_cropped, pop_ghs, "weights", as.points=TRUE, ) |> st_as_sf() |> st_transform(4326)
names(od)[1] <- 'building'
DBI::dbWriteTable(connection, 
                  DBI::Id(schema = "public", table = "od_2728"), 
                  od)
```

Snap the points

```{sql}
#| eval: false
#| code-summary: "PostGIS: Snapping samples to the closest node"
#| connection: eisenberg_connection

--- Create ID column
ALTER TABLE od_2728
ADD COLUMN id serial PRIMARY KEY;

--- Snap all the Apoints based on building density to the closest vertice within the network
CREATE TABLE od_2728_snapped AS
SELECT DISTINCT ON (net.id)
       pt.id AS pt_id,
       net.id AS net_id,
       net.the_geom
FROM 
(select * 
FROM 
    od_2728 as pt) as pt
CROSS JOIN
LATERAL (SELECT
        * 
        FROM porto_alegre_net_largest_vertices_pgr AS net
         ORDER BY net.the_geom <-> pt.geometry 
        LIMIT 1) AS net;
--- Create 100 origin samples from the snapped points

CREATE TABLE weight_sampling_100_origin  AS
WITH porto_100_origin AS (
        SELECT
            * 
        FROM 
            od_2728_snapped 
        ORDER BY random() LIMIT 100)
        SELECT * FROM  porto_100_origin;  
        
--- Create 100 destination from the snapped points
CREATE TABLE weight_sampling_100_destination  AS
WITH porto_100_destination AS (
        SELECT
            * 
        FROM 
            od_2728_snapped 
        ORDER BY random() LIMIT 100)
        SELECT * FROM  porto_100_destination;  
--- Indeces on origin
CREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);
CREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);
CREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);
--- Indeces on destination
CREATE INDEX weight_sampling_1000_destination_idx_pt_id ON weight_sampling_1000_destination USING btree(pt_id);
CREATE INDEX weight_sampling_1000_destination_idx_net_id ON weight_sampling_1000_destination USING btree(net_id);
CREATE INDEX weight_sampling_1000_destination_idx_the_geom ON weight_sampling_1000_destination USING gist(the_geom);
---- Cluster
CLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;
---- Vacuum clean
VACUUM(full, ANALYZE) weight_sampling_100_origin;
VACUUM(full, ANALYZE) weight_sampling_100_destination;
VACUUM(full, ANALYZE) porto_alegre_net_largest;
----

---- add ID
ALTER TABLE od_77763
ADD COLUMN id serial;
---- Rename the column name
ALTER TABLE od_77763
rename "GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0" 
to "build";
---- create spatial index
CREATE INDEX idx_od_77763_geom ON od_77763
       USING gist(geometry);
CREATE INDEX idx_od_77763_id on weighted_sampling
       USING btree(id);
CREATE INDEX idx_od_77763_geom_build on weighted_sampling 
       USING btree("build");
---- The current total is 77763

----
CREATE TABLE od_40420_snapped_origin AS
SELECT DISTINCT ON (net.id)
	   pt.id AS pt_id,
	   net.id AS net_id,
	   net.the_geom
FROM 
(select * 
FROM 
	od_77763 as pt) as pt
CROSS JOIN
LATERAL (SELECT
		* 
		FROM porto_alegre_net_largest_vertices_pgr AS net
		 ORDER BY net.the_geom <-> pt.geometry 
		LIMIT 1) AS net;
----

CREATE TABLE od_40420_snapped_destination AS
SELECT DISTINCT ON (net.id)
	   pt.id AS pt_id,
	   net.id AS net_id,
	   net.the_geom
FROM 
(select * 
FROM 
	od_77763 as pt) as pt
CROSS JOIN
LATERAL (SELECT
		* 
		FROM porto_alegre_net_largest_vertices_pgr AS net
		 ORDER BY net.the_geom <-> pt.geometry 
		LIMIT 1) AS net;

```

#### Selecting point of interest (Hospitals)

```{sql}
#| eval: false
#| code-summary: "PostGIS: Selecting hospistals in the AoI and snapping"

--- Create a table with the bounding box that contains Porto Alegre + GHS
CREATE TABLE porto_alegre_bbox AS
WITH porto_alegre_ghs AS(
    SELECT 
       ghs.*
    FROM
       urban_center_4326 AS ghs
    JOIN
       nuts 
    ON 
       st_intersects(nuts.geom, ghs.geom)
    WHERE 
        nuts.shapename = 'Porto Alegre'),
--- Bounding Box that contained the GHS in Porto Alegre
porto_alegre_ghs_bbox AS(
    SELECT 
        st_setsrid(st_extent(geom),4326) as geom_bbox
    FROM 
        porto_alegre_ghs)
 SELECT * FROM porto_alegre_ghs_bbox 
---Use the bounding box to select hospitals
CREATE TABLE hospital_rs_node_v2 AS
WITH hospital_rs_porto AS (
SELECT 
    h.*
FROM 
    hospitals_bed_rs AS h,
    porto_alegre_bbox bbox
WHERE st_intersects(h.geom, bbox.geom_bbox))
SELECT DISTINCT ON (h.cd_cnes)
    cd_cnes,
    ds_cnes,
    f.id,
    f.the_geom <-> h.geom AS distance,
    h.geom AS geom_hospital,
    f.the_geom AS geom_node
FROM hospital_rs_porto h
LEFT JOIN LATERAL
(SELECT 
    id, 
    the_geom
FROM porto_alegre_net_largest_vertices_pgr AS net
ORDER BY
    net.the_geom <-> h.geom
LIMIT 1) AS f ON true

---- Create index to optimize fuerther queries

CREATE INDEX idx_hospital_rs_node_v2 ON hospital_rs_node_v2 USING btree(id);
```

```{=html}

<iframe width="760" height="500" src="/media/map_sampling_example_v0.html" title = "Inspect of the components in the graph network "></iframe>

```




```{r}
#| message: false
#| warning: false
#| code-summary: "R: Visualizing regular and weighted sampling"
#| eval: false

## Crop and Reproject
## gdalwarp -te -4850853.201784615 -3737074.296348413 -4616291.881935796 -3495378.804761388 GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V
## gdalwarp -t_srs "EPSG:4326" GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0_RioGrandeDoSul.tif GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif
pal <- mapview::mapviewPalette("mapviewTopoColors")
ghs_build <- stack("/home/ricardo/heigit_bookdown/data/GHS_BUILT_V_E2020_GLOBE_R2023A_4326_100_V1_0_RioGrandeDoSul.tif")
ghs_smod <- stack("/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif")
ghs_smod_terra <- terra::rast("/home/ricardo/heigit_bookdown/data/GHS_SMOD_E2020_GLOBE_R2023A_4326_1000_V2_0_RioGrandeDoSul.tif")
regular_sampling <- st_read("/home/ricardo/heigit_bookdown/data/random_points_snapped.geojson")
regular_sampling_100 <- sample_n(regular_sampling, size = 100)
weighted_sampling_origin <- st_read(eisenberg_connection,"weight_sampling_100_origin")
weighted_sampling_destination <- st_read(eisenberg_connection,"weight_sampling_100_destination")

weighted_sampling_origin$sample <- "origin"
weighted_sampling_destination$sample <- "destination"
weighted_sampling_both <- rbind(weighted_sampling_origin,
                           weighted_sampling_destination )

weighted_sampling <- weighted_sampling_both |> mutate(sample = as.factor(weighted_sampling$sample))

poi_hospital <- st_read(eisenberg_connection,"hospital_rs_node_v2")
m <- matrix(c(
     0, 10, NA,   # Values >= 0 and <= 10 become 0
     10, 13, 1,  # Values > 10 and <= 21 become 1
     13, 29, 2,  # Values > 21 and <= 29 become 2
     30, 30, 3   # Values == 30 become 3
 ), ncol = 3, byrow = TRUE)
## Classify using the correct matrix
rc2 <- classify(ghs_smod_terra, m, include.lowest=TRUE)
rc2_factor <- as.factor(rc2)
levels(rc2_factor) <- data.frame(
  ID = 1:3,    # These should match the values in the classification
  category = c("Rural: 10-13", "Suburban: 13-29", "Urban Center: 30")
)
category_colors <- c("#008f44","#dedb96", "#cc9152")
mapview(ghs_build[[1]],
        layer.name ="Built-up volume",
        col.regions = pal(100),
        alpha.regions= 0.45,
        hide=TRUE) +
mapview(ghs_smod[[1]],
        layer.name = "Settlement classification",
        col.regions = pal(100),
        alpha.regions= 0.35,
        hide=TRUE) +
mapview(weighted_sampling,
        layer.name="Weighted samples",
        zcol="sample",
        col.regions=c("#2D5CA4","#00A3A0"),
        hide=TRUE,
        cex= 3) +
mapview(regular_sampling_100,
        color = "darkgray",
        col.regions="darkgray",
        cex= 3,
        legend= FALSE,
        hide=TRUE) +
mapview(subset(poi_hospital,
               select=c("cd_cnes",
                        "ds_cnes",
                        "id",
                        "geom_hospital")),
                layer.name = "POI - Hospitals",
                color= "darkred",
        col.regions="#CA2334",
               popup=popupTable(poi_hospital, zcol=c("cd_cnes","ds_cnes","id")))

```

### Calculating Sum Edge Betweenness

```{sql}
#| eval: false
#| code-summary: "PostGIS: Calculating edge betweenness centrality on the pre-disaster network"

--- Pre-scenario
 CREATE TABLE centrality_100_100_dijkstra AS
 SELECT   b.id,
 b.the_geom,
 count(the_geom) as centrality 
 FROM  pgr_dijkstra('SELECT  id,
                             source,
                            target,
                            cost
                      FROM porto_alegre_net_largest',
                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin  ),
                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination ),
                      directed := TRUE) j
                      left JOIN porto_alegre_street_united AS b
                      ON j.edge = b.id
                      GROUP BY  b.id, b.the_geom
                      ORDER BY centrality DESC;  
                      
                     
                     select * from porto_alegre_net_largest ;
---- Adding the bidirectid by joining the dijkstra table with the original
CREATE TABLE centrality_weighted_100_bidirect AS
SELECT t1.*,
    t2."bidirectid"
FROM
    centrality_100_100_dijkstra t1
JOIN
    porto_alegre_net_largest t2
ON
    t1.id = t2.id;
--- The final product requries  79941
select max("bidirectid") from centrality_weighted_100_bidirect;
create sequence bididirect_id_weight start 79941;
update centrality_weighted_100_bidirect
set "bidirectid" = nextval('bididirect_id_weight')
where "bidirectid" is null ;
---- verify
select count(*) 
from centrality_weighted_100_bidirect 
where "bidirectid" is null; --- 0
----
create table centrality_weighted_100_bidirect_group as 
select 
       "bidirectid",
       sum(centrality) as  centrality
from centrality_weighted_100_bidirect
group by "bidirectid"; --- this sum the centrality for duplicated bidirect id
---
---- now recover the id
create table centrality_weighted_100_bidirect_group_id as 
select t1.*, t2.id
from centrality_weighted_100_bidirect_group t1
join centrality_weighted_100_bidirect t2 
on t1."bidirectid" = t2."bidirectid"; 
---- add geometries
create table centrality_weighted_100_bidirect_cleaned as
select t1.*,
    t2.the_geom,
    t2.target,
    t2.source,
    t2.cost,
    t2."unidirectid"
from 
    centrality_weighted_100_bidirect_group_id t1
join
    porto_alegre_net_largest t2
on
    t1.id = t2.id;
--- The final product is: centrality_weighted_100_bidirect_cleaned

```


```{sql}
#| eval: false
#| code-summary: "PostGIS: Calculating edge betweenness centrality on the post-disaster network"

--- Post scenario

----routing
 CREATE TABLE centrality_100_100_dijkstra_post AS
 SELECT   b.id,
 b.the_geom,
 count(the_geom) as centrality 
 FROM  pgr_dijkstra('SELECT  id,
                             source,
                            target,
                            cost
                      FROM prueba_largest_network_post',
                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin_post  ),
                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination_post ),
                      directed := TRUE) j
                      left JOIN prueba_largest_network_post AS b
                      ON j.edge = b.id
                      GROUP BY  b.id, b.the_geom
                      ORDER BY centrality DESC;  
---- Adding the bidirectid by joining the dijkstra table with the original
CREATE TABLE centrality_weighted_100_bidirect_post AS
SELECT t1.*,
    t2."bidirectid"
FROM
    centrality_100_100_dijkstra_post t1
JOIN
    prueba_largest_network_post t2
ON
    t1.id = t2.id;
--- The final product requries  79918
select max("bidirectid") from centrality_weighted_100_bidirect_post; ---79918
create sequence bididirect_id_weight_post start 79918;
update centrality_weighted_100_bidirect_post
set "bidirectid" = nextval('bididirect_id_weight_post')
where "bidirectid" is null ;
   ---- verify
select count(*) 
from centrality_weighted_100_bidirect_post 
where "bidirectid" is null; --- 0              
----
create table centrality_weighted_100_bidirect_group_post as 
select 
       "bidirectid",
       sum(centrality) as  centrality
from centrality_weighted_100_bidirect_post
group by "bidirectid"; --- this sum the centrality for duplicated bidirect id
---        
---- now recover the id
create table centrality_weighted_100_bidirect_group_post_id as 
select t1.*, t2.id
from centrality_weighted_100_bidirect_group_post t1
join centrality_weighted_100_bidirect_post t2 
on t1."bidirectid" = t2."bidirectid"; 
---- add geometries
create table centrality_weighted_100_bidirect_cleaned_post as
select t1.*,
    t2.the_geom,
    t2.target,
    t2.source,
    t2.cost,
    t2."unidirectid"
from 
    centrality_weighted_100_bidirect_group_post_id t1
join
    prueba_largest_network_post t2
on
    t1.id = t2.id;       

```

#### Para hospitales


```{sql}
#| eval: false
#| code-summary: "PostGIS: Calculating edge betweenness centrality for each hospital" 

--- POST-EVENT
CREATE TABLE weight_sampling_454_origin_post  AS
WITH porto_454_origin_post AS (
        SELECT
            * 
        FROM 
            od_2728_snapped_post
        ORDER BY random() 
        LIMIT 454)
SELECT * FROM  porto_454_origin_post;	

---- Create index for origin
CREATE INDEX weight_sampling_454_origin_post_net_id ON weight_sampling_454_origin_post USING hash(net_id);
CREATE INDEX weight_sampling_454_origin_post_geom ON weight_sampling_454_origin_post USING gist(the_geom);
---- Running the query
 ---- Group by end_vid that represent destination
EXPLAIN ANALYZE
 CREATE TABLE centrality_424_hospitals_porto_end_id_centrality_post AS
 SELECT   
 b.id,
 j.end_vid,
 b.the_geom,
 count(the_geom) as centrality 
 FROM  pgr_dijkstra('SELECT  id,
                             source,
                            target,
                            cost
                      FROM prueba_largest_network_post',
                      ARRAY(SELECT net_id AS origin_id FROM weight_sampling_454_origin_post),
                      ARRAY(SELECT id AS destination_id FROM hospital_rs_destination),
                      directed := TRUE) j
            LEFT JOIN prueba_largest_network_post AS b
            ON j.edge = b.id
            GROUP BY  b.id, j.end_vid, b.the_geom
            ORDER BY centrality DESC;   



```


#### Edge betweenness

```{sql}
#| eval: false
#| connection: eisenberg_connection

CREATE TABLE od_40420_snapped_origin AS
SELECT DISTINCT ON (net.id)
       pt.id AS pt_id,
       net.id AS net_id,
       net.the_geom
FROM 
(select * 
FROM 
    od_77763 as pt) as pt
CROSS JOIN
LATERAL (SELECT
        * 
        FROM porto_alegre_net_largest_vertices_pgr AS net
         ORDER BY net.the_geom <-> pt.geometry 
        LIMIT 1) AS net;

```

```{sql}
#| eval: false
#| connection: eisenberg_connection

CREATE TABLE random_272_destination  AS
with random_272_destination AS (
        SELECT
            * 
        FROM 
            od_40420_snapped_origin 
        ORDER BY random() LIMIT 200)
        SELECT * FROM  random_272_destination;  

```


```{sql}
#| eval: false
#| connection: eisenberg_connection

CREATE TEMP TABLE vertices_lookup_v5 
AS             
WITH all_pairs AS (
  SELECT f.net_id AS fid, f.the_geom as fgeom,
         t.net_id AS tid, t.the_geom as tgeom
    FROM random_272_origin AS f,
         random_272_destination AS t
),
vertices AS (
  SELECT fid, tid,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> fgeom
       LIMIT 1) as fv,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> tgeom
       LIMIT 1) as tv
  FROM all_pairs
)
SELECT * FROM vertices;

```

```{sql}
#| eval: false
#| connection: eisenberg_connection

CREATE TABLE porto_272_272_dijkstra AS
 WITH pgr_result AS (
   SELECT pgr_dijkstra('SELECT id,
           source,
    target,
     cost FROM porto_alegre_net_largest',
     array_agg(fv), array_agg(tv), 
     directed := true
   ) FROM vertices_lookup_v5
 )
SELECT (pgr_dijkstra).*, a.fid, a.tid FROM pgr_result
JOIN vertices_lookup_v5 a
ON (pgr_dijkstra).start_vid = a.fv
AND (pgr_dijkstra).end_vid = a.tv;

```

```{sql}
#| eval: false
#| connection: eisenberg_connection

CREATE TEMP TABLE vertices_lookup_v5                     
AS             
WITH all_pairs AS (
  SELECT f.net_id AS fid, f.the_geom as fgeom,
         t.net_id AS tid, t.the_geom as tgeom
    FROM random_272_origin AS f,                                                                      
         random_272_destination AS t               
),                                                                                     
vertices AS (
  SELECT fid, tid,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> fgeom
       LIMIT 1) as fv,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> tgeom
       LIMIT 1) as tv
  FROM all_pairs
)
SELECT * FROM vertices;

```


```{sql}
#| eval: false
#| connection: eisenberg_connection


CREATE TEMP TABLE vertices_lookup_v5                     
AS             
WITH all_pairs AS (
  SELECT f.net_id AS fid, f.the_geom as fgeom,
         t.net_id AS tid, t.the_geom as tgeom
    FROM random_272_origin AS f,                                                                      
         random_272_destination AS t               
),                                                                                     
vertices AS (
  SELECT fid, tid,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> fgeom
       LIMIT 1) as fv,
     (SELECT id
        FROM porto_alegre_net_largest_vertices_pgr AS way
       ORDER BY way.the_geom <-> tgeom
       LIMIT 1) as tv
  FROM all_pairs
)
SELECT * FROM vertices;

```


### Calculating closeness

```{sql}
#| eval: false
#| code-summary: "PostGIS: Calculating closenness centrality"

---- Pre-Event
CREATE TABLE clossness_hospital_porto AS 
WITH dijkstra_cost AS (
SELECT * FROM pgr_dijkstraCostMatrix(
  'SELECT id, source, target, cost FROM porto_alegre_net_largest',
  (SELECT array_agg(id)
    FROM porto_alegre_net_largest_vertices_pgr
    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),
  true)),
closeness AS (
 SELECT dc.start_vid,
        sum(agg_cost)::int AS closeness
FROM dijkstra_cost dc
GROUP BY dc.start_vid
ORDER BY closeness DESC)
SELECT 
    h.cd_cnes,
    h.ds_cnes,
    c.closeness,
    h.distance,
    h.geom_node,
    h.geom_hospital
FROM 
    closeness AS c
LEFT JOIN
    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;
--- Post event
CREATE TABLE clossness_hospital_porto_post AS 
WITH dijkstra_cost AS (
SELECT * FROM pgr_dijkstraCostMatrix(
  'SELECT id, source, target, cost FROM prueba_largest_network_post',
  (SELECT array_agg(id)
    FROM prueba_largest_network_post_vertices_pgr
    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),
  true)),
closeness AS (
 SELECT dc.start_vid,
        sum(agg_cost)::int AS closeness
FROM dijkstra_cost dc
GROUP BY dc.start_vid
ORDER BY closeness DESC)
SELECT 
    h.cd_cnes,
    h.ds_cnes,
    c.closeness,
    h.distance,
    h.geom_node,
    h.geom_hospital
FROM 
    closeness AS c
LEFT JOIN
    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;   
    
---- Add centrality from post-event to pre-event
CREATE TABLE hospitals_closeness_both AS 
SELECT
	pre.cd_cnes,
	pre.ds_cnes,
	pre.closeness  AS closeness_pre,
	coalesce(post.closeness,0) AS closeness_post,
	pre.geom_node,
	pre.geom_hospital
FROM 
	clossness_hospital_porto AS pre
LEFT JOIN 
	clossness_hospital_porto_post AS post ON pre.cd_cnes  = post.cd_cnes

```

### Assesing vulnerability based on socioeconomic indicators

## Verifying results


