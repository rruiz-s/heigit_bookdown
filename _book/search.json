[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Connectivity for Disaster Resiliance",
    "section": "",
    "text": "Preface\nFrom natural disasters to human alterations in urban infrastructure, network connectivity is key for the population’s survival and for the metapopulation dynamics (Alberti 2008). Through networks, people, resources, goods, information or services flows (Heywood, Cornelius, and Carver 2011), and its spread is highly dependent on those segments with higher betweenness edge centrality (Kolaczyk and Csárdi 2014). Therefore, measuring the betweenness edge centrality on the different segments of the network help us to identify critical infrastructures that play a key role in the connectivity ensuring the movement along the network.\nAn interruption of the flow on these critical infrastructures can induce cascading effects causing physical, social or economic disruption (Petricola et al. 2022). Discrete events such as disasters can trigger these cascading events disrupting the landscape. These disruptions that test the system’s capacity to recover from the change, namely, its resilience (Scheffer 2009), can derive from climate change. Extreme events related to climate change such as storms or floods are being increased as well as the severity and frequency of most hazards (Kuffer et al. 2021). To minimize the humanitarian consequences of these severe events, the ability of disaster preparedness by finding criticality of infrastructure is key (Petricola et al. 2022).\nAs ecosystem services become more stressed in the future (Costanza et al. 1997), and the frequency and intensity of extreme natural hazards increases (“Summary for Policymakers,” 2023), increasing resilience is of critical importance for numerous frameworks. On a global scale, the Sendai Framework for Disaster Risk Reduction 1 aims to reduce disaster damage to critical infrastructure and disruption of basic services as part of the fourth global target. Likewise, (a) damage to critical infrastructure and (b) number of disruptions to basic services, attributed to disasters are part of the indicators 11.5.3 from the Sustainable Development Goal 11, “making cities and human settlements inclusive, safe, resilient and sustainable2“. Therefore, both frameworks aim to increase urban resilience (Etinay, Egbu, and Murray 2018).\nResilience is a term with different interpretations depending on the discipline (Therias and Rafiee 2023). For example, in ecology, resilience is defined as “the capacity of a system to recover to essentially the same state after a disturbance” (Scheffer 2009). In the context of this research, urban resilience is defined as “a property of the urban system that enables it to survive and thrive in the face of uncertainty, adversity, and change” (Abenayake et al. 2022a). Finding critical infrastructures can increase urban resilience by localizing where to build alternative routes to healthcare facilities (i.e., increasing redundancy) or finding structures, such as bridges, that have an important function in the network due to its connectivity (i.e., increasing robustness). Redundancy and robustness are two properties of the popular 4R model of resilience, which also includes resourcefulness and rapidity (Xu et al. 2024).\nOn a regional level, territorial resilience is one of the main themes in the EU R&I policy agenda for Nature-based Solutions. Resilience to natural disasters and adaptation to climate change are part of the future priorities defined in the Granada Declaration. On a national level, low-income countries with fewer resources to prepare, mitigate, and recover from climate change-related disasters, especially the population living in deprived areas, could benefit from this route service adapted to disaster management.\nCentrality analyses are constrained by its high computational costs, challenging their validity to represent real-world scenarios. To reduce these costs, studies did not use entire city- regions as area of interest (Abenayake et al. 2022b) Similarly, other studies on accessibility used different techniques to maximize computation efficiency (Spangler et al. 2023). Likewise, only a selection of the nodes was used to save computation time (Florath, Chanussot, and Keller 2024). Large spatial scales refined this strategy using network percolation, which allowed the study of flood disturbances on vast regions such as the entire road networks of China and the USA, among others (Van Ginkel et al. 2022). Currently, there are methods and technologies that aim to optimize queries reducing its computational costs.\nSpatial indexing systems are used to query large geospatial data sets, increasing their efficient queries. For example, Uber’s Hexagonal Hierarchical Spatial Index (H3) was applied to optimize emergency response routes (Diallo et al. 2023). These spatial indexes can be implemented into PostgreSQL using pgRouting, which has been previously used to find the shortest path to healthcare facilities (Alasadi et al., n.d.). Instead of PostgreSQL, other new database management system, such as DuckDB, was used to implement memory optimization (Kuiper, Boncz, and Muhleisen, n.d.). Therefore, the current contribution of this research would be applying these optimization processes running pgRouting to obtain different centrality and accessibility metrics.\n\n\n\n\nAbenayake, Chethika, Amila Jayasinghe, Hasintha Nawod Kalpana, Eshi Eranga Wijegunarathna, and P. K. S. Mahanama. 2022a. “An Innovative Approach to Assess the Impact of Urban Flooding: Modeling Transportation System Failure Due to Urban Flooding.” Applied Geography 147 (October): 102772. https://doi.org/10.1016/j.apgeog.2022.102772.\n\n\n———. 2022b. “An Innovative Approach to Assess the Impact of Urban Flooding: Modeling Transportation System Failure Due to Urban Flooding.” Applied Geography 147 (October): 102772. https://doi.org/10.1016/j.apgeog.2022.102772.\n\n\nAlasadi, Hamid Ali Abed, Mohammed Talib Aziz, Mohammed Dhiya, and Ahmed Abdulmajed. n.d. “A Network Analysis for Finding the Shortest Path in Hospital Information System with GIS and GPS.”\n\n\nAlberti, Marina. 2008. Advances in Urban Ecology. Boston, MA: Springer US. https://doi.org/10.1007/978-0-387-75510-6.\n\n\nCostanza, Robert, Ralph d’Arge, Rudolf De Groot, Stephen Farber, Monica Grasso, Bruce Hannon, Karin Limburg, et al. 1997. “The Value of the World’s Ecosystem Services and Natural Capital.” Nature 387 (6630): 253–60. https://doi.org/10.1038/387253a0.\n\n\nDiallo, Francis Patrick, Mihai Alexandru Preda, Alexandru-Ionuţ Mustaţă, and Bogdan-Costel Mocanu. 2023. “Using Spatial Indexing Systems to Optimize Emergency Response Routes.” In 2023 24th International Conference on Control Systems and Computer Science (CSCS), 544–50. Bucharest, Romania: IEEE. https://doi.org/10.1109/CSCS59211.2023.00092.\n\n\nEtinay, Nuha, Charles Egbu, and Virginia Murray. 2018. “Building Urban Resilience for Disaster Risk Management and Disaster Risk Reduction.” Procedia Engineering 212: 575–82. https://doi.org/10.1016/j.proeng.2018.01.074.\n\n\nFlorath, Janine, Jocelyn Chanussot, and Sina Keller. 2024. “Road Accessibility During Natural Hazards Based on Volunteered Geographic Information Data and Network Analysis.” ISPRS International Journal of Geo-Information 13 (4): 107. https://doi.org/10.3390/ijgi13040107.\n\n\nHeywood, D. Ian, Sarah Cornelius, and Steve Carver. 2011. An Introduction to Geographical Information Systems. 4th ed. Harlow, England ; Toronto: Prentice Hall.\n\n\nKolaczyk, Eric D., and Gábor Csárdi. 2014. Statistical Analysis of Network Data with r. Vol. 65. Use r! New York, NY: Springer New York. https://doi.org/10.1007/978-1-4939-0983-4.\n\n\nKuffer, Monika, Dana R. Thomson, Andrew Maki, Sabine Vanhuysse, Stefanos Georganos, Richard Sliuzas, and Claudio Persello. 2021. “EO-Based Low-Cost Frameworks to Address Global Urban Data GAPS on Deprivation and Multiple Hazards.” In 2021 IEEE International Geoscience and Remote Sensing Symposium IGARSS, 2106–9. Brussels, Belgium: IEEE. https://doi.org/10.1109/IGARSS47720.2021.9554094.\n\n\nKuiper, Laurens, Peter Boncz, and Hannes Muhleisen. n.d. “Robust External Hash Aggregation in the Solid State Age.”\n\n\nPetricola, Sami, Marcel Reinmuth, Sven Lautenbach, Charles Hatfield, and Alexander Zipf. 2022. “Assessing Road Criticality and Loss of Healthcare Accessibility During Floods: The Case of Cyclone Idai, Mozambique 2019.” International Journal of Health Geographics 21 (1): 14. https://doi.org/10.1186/s12942-022-00315-2.\n\n\nScheffer, Marten. 2009. “III.17 Alternative Stable States and Regime Shifts in Ecosystems.” In The Princeton Guide to Ecology, edited by Simon A. Levin, Stephen R. Carpenter, H. Charles J. Godfray, Ann P. Kinzig, Michel Loreau, Jonathan B. Losos, Brian Walker, and David S. Wilcove, 395–406. Princeton University Press. https://doi.org/10.1515/9781400833023.395.\n\n\nSpangler, Keith R., Paige Brochu, Amruta Nori-Sarma, Dennis Milechin, Michael Rickles, Brandeus Davis, Kimberly A. Dukes, and Kevin J. Lane. 2023. “Calculating Access to Parks and Other Polygonal Resources: A Description of Open-Source Methodologies.” Spatial and Spatio-Temporal Epidemiology 47 (November): 100606. https://doi.org/10.1016/j.sste.2023.100606.\n\n\nTherias, Adele, and Azarakhsh Rafiee. 2023. “City Digital Twins for Urban Resilience.” International Journal of Digital Earth 16 (2): 4164–90. https://doi.org/10.1080/17538947.2023.2264827.\n\n\nVan Ginkel, Kees C. H., Elco E. Koks, Frederique De Groen, Viet Dung Nguyen, and Lorenzo Alfieri. 2022. “Will River Floods ‘Tip’ European Road Networks? A Robustness Assessment.” Transportation Research Part D: Transport and Environment 108 (July): 103332. https://doi.org/10.1016/j.trd.2022.103332.\n\n\nXu, Shiying, Hao Chen, Adrian Wing-Keung Law, Feng Zhu, Daniel Martini, and Martin Lim. 2024. “Development of a Standardised Framework with Universal Core Indicators for Flood Resilience Assessment.” Natural Hazards, April. https://doi.org/10.1007/s11069-024-06631-z."
  },
  {
    "objectID": "intro.html",
    "href": "intro.html",
    "title": "1  Introduction",
    "section": "",
    "text": "2 MAP\nAÑadir An innovative approach to assess the impact of urban flooding: Modeling transportation system failure due to urban flooding\nTABLA\nThis is a book created from markdown and executable code.\nSee (knuth84?) for additional discussion of literate programming.\nShow the code\n1 + 1\n\n\n[1] 2\nThe research objective is to improve urban resilience by identifying critical infrastructures necessary for accessing healthcare facilities, comparing road connectivity and accessibility. The following research questions are addressed in this study:"
  },
  {
    "objectID": "intro.html#research-questions",
    "href": "intro.html#research-questions",
    "title": "1  Introduction",
    "section": "2.1 Research questions",
    "text": "2.1 Research questions\n\nHow did the road connectivity of the city network change after being impacted by the floodings in Rio Grande do Sul based on Edge Betweenness Centrality?\nWhich healthcare facilities were most affected by the floodings based on accessibility metrics?\nIn which area of a city’s network should decisions be made to reinforce or redesign routes to optimize healthcare accessibility and ensure minimal disruption during flood events?"
  },
  {
    "objectID": "methodology.html#data",
    "href": "methodology.html#data",
    "title": "2  Methodology",
    "section": "2.1 Data",
    "text": "2.1 Data"
  },
  {
    "objectID": "methodology.html#framework",
    "href": "methodology.html#framework",
    "title": "2  Methodology",
    "section": "2.2 Framework",
    "text": "2.2 Framework"
  },
  {
    "objectID": "methodology.html#area-of-interest",
    "href": "methodology.html#area-of-interest",
    "title": "2  Methodology",
    "section": "2.3 Area of Interest",
    "text": "2.3 Area of Interest\n\n2.3.1 Import\nThe tool ogr2ogr imported the data and adjust the geometry according to the recommendations. The recommendation was “pgRouting processes single features more effficiently than multiegeometries, so whenver possible, choose single over multi” mentioned in the book pgrouting.\n\n\nShow the code\n## OSM geometry: porto_alegre_net_pre.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_net_pre.geojson -nln porto_alegre_net_pre -lco GEOMETRY_NAME=the_geom -nlt LINESTRING -explodecollections \n\n## Administrative units: nuts.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/nuts.geojson -nln nuts -lco GEOMETRY_NAME=geom\n\n## Flooding extent: flooding_rio_grande_do_sul.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/flooding_rio_grande_do_sul.geojson -nln flooding_rio_grande_do_sul -lco GEOMETRY_NAME=the_geom \n\n## Building density: urban_center_4326.geojson\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/urban_center_4326.geojson -nln urban_center_4326 -lco GEOMETRY_NAME=geom \n\n## Hospitals\n### From Rio Grande do Sul Geoportal\nogr2ogr -f PostgreSQL PG:\"host=localhost port= 25432 user=docker password=docker dbname=gis schemas=heigit\" /home/ricardo/HeiGIT-Github/data_required_porto_alegre/Hospitais_com_Leitos_de_UTIs_no_RS.geojson -nln hospitals_bed_rs -lco GEOMETRY_NAME=geom\n\n\n\n2.3.1.1 Network: OpenStreetMap (OSM)\nThe following SQL code created the command to import OpenStreetMap (OSM) network using osmium. Firstly using the GHS-SMOD dataset the Global Human Settlement in Porto Alegre is chosen. Secondly, this GHS selected is used to create the bounding box that it is lastly used to generate the osmium code. The OSM network obtained with this osmium command was the input required for OpenRouteService (ORS).\n\n\nShow the code\n--- GHS urban area that intersected with Porto Alegre city\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n--- The command used the Porto Alegre GHS and its Bounding Box to import the OpenStreet Network. \nporto_alegre_ghs_bbox_osmium_command AS (\n    SELECT \n        ST_XMin(ST_SnapToGrid(geom_bbox, 0.0001)) AS min_lon,\n        St_xmax(ST_SnapToGrid(geom_bbox,0.0001)) AS max_lon,\n        St_ymin(ST_SnapToGrid(geom_bbox,0.0001)) AS min_lat,\n        St_ymax(ST_SnapToGrid(geom_bbox, 0.0001)) AS max_lat\n    FROM porto_alegre_ghs_bbox)\nSELECT\n    'osmium extract -b ' \n    || min_lon || ',' || min_lat || ',' || max_lon || ',' || max_lat || \n    ' sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf' AS osmium_command\nFROM porto_alegre_ghs_bbox_osmium_command;\n\n\n\n\n2.3.1.2 Graph: OpenRouteService (OSM)\nA docker for OpenRouteService transformed the OSM network that covered the GHS in Porto Alegre into a graph. In the “ors-config.yml” from OpenRotueService (ORS), the “source_file” parameter is set with the following directory.\n\n\nShow the code\nsource_file: /home/ors/files/puerto_alegre_urban_center.osm.pbf\n\n\nOpenRouteService (ORS) created a a routable network, assigining costs and adding information for each node, namely, “fromId” and “toId”. The network named as “porto_alegre_net” extracted from ORS was created using the R script “get_graph” from Marcel Reinmuth.\n\n\n\n2.3.2 Cleaning\nThe parameters start_vid, end_vid named “fromid” and “toid” in the network dataset from openrouteservice are transform into bigint data type to run the algorithm pgr_dijkstra as the official documentation indicates.\n\n\nShow the code\nselect * from porto_alegre_net_pre; \nALTER TABLE porto_alegre_net_pre \n    ALTER COLUMN \"toid\" type bigint,\n    ALTER COLUMN \"fromid\" type bigint,\n    ALTER COLUMN \"ogc_fid\" type bigint;\n\n\n\n2.3.2.1 Graph\nBefore using the graph, a quick inspection of the graph using the pgrouting function pgr_strongComponents() revealed how many components or isolated self-connecting network the graph had.\n\n\nShow the code\n--- Create a vertice table for pgr_dijkstra()\nSELECT \n  pgr_createVerticesTable('porto_alegre_net_pre', source:='fromid', target:='toid');\n  \n---- Add data to the vertices created table \nCREATE TABLE component_analysis_network_porto AS\nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\nporto_alegre_net_component_geom AS (\nSELECT \n    net.*,\n    net_geom.the_geom\nFROM \n    porto_alegre_net_component AS net\nJOIN \n    porto_alegre_net_pre  AS net_geom\nON \n    net.node = net_geom.fromid)\nSELECT \n    component,\n    st_union(the_geom) AS the_geom,\n    st_length(st_union(the_geom)::geography)::int AS length\nFROM  \n    porto_alegre_net_component_geom\nGROUP BY component\nORDER BY length DESC;\n\n\nThe largest length selected the network component of the study. The following code created this table ruling out the rest of the relatively small networks.\n\n\nShow the code\nCREATE TABLE porto_alegre_net_largest AS\n---- Obtain again table classifying nodes in different components           \nWITH porto_alegre_net_component AS (\nSELECT \n  * \nFROM \n  pgr_strongComponents('SELECT\n                               ogc_fid AS id,\n                               fromid AS source,\n                               toid AS target,\n                               weight AS cost \n                        FROM \n                              porto_alegre_net_pre')),\n--- Calculate the largest component from the network\nlargest_component_net AS (\n    SELECT \n        component\n    FROM \n        component_analysis_network_porto \n    LIMIT 1),\n--- Using the largest component from the network to filter\nlargeset_component_network_porto AS (\nSELECT\n    *  \nFROM\n    porto_alegre_net_component,\n    largest_component_net\nWHERE \n    porto_alegre_net_component.component = largest_component_net.component)\nSELECT \n    net_multi_component.*\nFROM \n    porto_alegre_net_pre AS net_multi_component,\n    largeset_component_network_porto AS net_largest_component\nWHERE  \n    net_multi_component.fromid IN (net_largest_component.node);\n\n\n\n\n2.3.2.2 Flooding mask\nThe downloaded flooding extent covered a larger area in Rio Grande do Sul. However, our area of interest for urban dense city was only Porto Alegre. Therefore, a serie of operations reduced the area focusing on Porto Alegre and at the same time improved the performance reducing the size of the mask.\nSubdividing the flooding mask to make the spatial indexes more efficient was the first step. The reason was the higher number of vertices of large objects and larges bounding boxes that hinder the spatial index performance (Link). After the spatial indexes were enable, the intersection with the Porto Alegre region subset the data.\n\n\nShow the code\n--- Select the GHS area that intersects with Porto Alegre\nCREATE TABLE flooding_subdivided_porto AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\n---Subdivide the flood extent to increase spatial indexes performance \nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul) \n---- Select the flooding subunits that intersects with the previous bounding box\nSELECT \n    flooding_sul_subdivided.* \nFROM \n    flooding_sul_subdivided, \n    porto_alegre_ghs_bbox\nWHERE \n    st_intersects(flooding_sul_subdivided.the_geom, porto_alegre_ghs_bbox.geom_bbox);\n\n\nAdditionally, the following code dissolve the borders to break the multipolygon into simple polygons to improve the performance of functions such as st_difference() (link).\n\n\nShow the code\nCREATE TABLE flooding_cleaned_porto AS\n    SELECT (ST_Dump(the_geom)).geom::geometry(polygon, 4326) geom FROM flooding_subdivided_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union AS\n    SELECT ST_Union(geom)::geometry(multipolygon, 4326) geom FROM flooding_cleaned_porto;\n    \nCREATE TABLE flooding_cleaned_porto_union_simple AS\n    SELECT (ST_Dump(geom)).geom::geometry(polygon, 4326) geom FROM flooding_cleaned_porto_union;\n    \nSELECT COUNT(*)\n     FROM flooding_cleaned_porto_union_simple ; --- count= 54.\n\nDELETE FROM flooding_cleaned_porto_union_simple WHERE ST_Area(geom) &lt; 0.0001; ---count= 2.\n\n\nThis allowed to calculate the area of each polygon finding slivers that were removed using the following code.\n\n\n\n2.3.3 Transform\nThe creation of the network after the disaster using the modified flooding mask and the creation of the origin-destination are considered in this section. The following code set the paremeters before carrying out these transformations following post 1 and post2.\nFirstly, add spatial index everywhere:\n\n\nShow the code\nCREATE INDEX idx_porto_alegre_net_largest_geom ON porto_alegre_net_largest USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_source ON porto_alegre_net_largest USING btree(fromid);\nCREATE INDEX idx_porto_alegre_net_largest_target ON porto_alegre_net_largest USING btree(toid);\nCREATE INDEX idx_porto_alegre_net_largest_id ON porto_alegre_net_largest USING btree(ogc_fid);\nCREATE INDEX idx_porto_alegre_net_largest_cost ON porto_alegre_net_largest USING btree(weight);\n\n\n\n\nShow the code\nSET max_parallel_workers_per_gather =4;\n\n\n\n\nShow the code\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n\n\n\n2.3.3.1 Network after the disaster\nA naive approach overlaying the flooding mask with the road network by the function st_difference() caused the crashing of the session. The follwing multi-step methodology reduced the processing cost make the query feasible using less resources.\n\nNetwork inside the flooding mask\nNetwork outside the flooding mask\nNetwork on the boundaries\nApplying st_difference\nUniting network external to the boundaries and network from outside\n\n1.The network contained by the flooding extent and its intersection is selected.\n\n\nShow the code\n------------- network inside\nCREATE TABLE porto_alegre_street_in_v2 AS\nSELECT net.id,\n    CASE \n        WHEN ST_Contains(flood.the_geom, net.the_geom)\n        THEN net.the_geom\n        ELSE st_intersection(net.the_geom, flood.the_geom)\n    END AS  geom\nFROM porto_alegre_net_largest net\nJOIN flooding_subdivided_porto flood\nON st_intersects(net.the_geom, flood.the_geom);\n\n\n\nInstead of using columns containing geometrical variables, the numerical ID is used. For this the previous step, which created a table with the network inside the flooding mask was required.\n\n\n\nShow the code\n----------------- network outside\nCREATE TABLE porto_alegre_street_out_v2 AS\nSELECT net.*\nFROM porto_alegre_net_largest net\nWHERE net.id NOT IN (\n    SELECT net.id\n    FROM porto_alegre_street_in_v2 net);\n\n\n\nThe function St_ExteriorRing() casted the geometry into linestring reducing the processing costs. This exterior ring selected the road segments located on the boundaries.\n\n\n\nShow the code\nCREATE TABLE porto_alegre_net_outside_v2 AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul),\nexterior_ring_porto_alegre_v2 AS (\nSELECT \n    ST_ExteriorRing((ST_Dump(union_geom)).geom) as geom\nFROM (\n    SELECT \n        ST_Union(flood.the_geom) as union_geom\n    FROM \n        porto_alegre_ghs_bbox as bbox\n    JOIN \n        flooding_sul_subdivided as flood\n    ON \n        ST_Intersects(flood.the_geom, bbox.geom_bbox)\n) AS subquery)\nSELECT net.id,\n    CASE\n        WHEN NOT ST_Contains(flood.geom, net.the_geom)\n        THEN net.the_geom\n            ELSE st_intersection(net.the_geom, flood.geom)\n    END AS  geom,\n    net.target,\n       net.source,\n       cost,\n       \"unidirectid\",\n       \"bidirectid\"\nFROM\nporto_alegre_net_largest AS net\nJOIN exterior_ring_porto_alegre_v2 flood ON\nst_intersects(net.the_geom, flood.geom);\n\n\n\n\n\n\n\nShow the code\n--- For the network\nCREATE INDEX idx_porto_alegre_net_outside_v2 ON porto_alegre_net_outside_v2 USING gist (geom);\n\nCLUSTER porto_alegre_net_outside_v2 USING idx_porto_alegre_net_outside_v2;\n\n--- For the flooding mask\nCREATE INDEX flooding_sul_subdivided_idx ON flooding_sul_subdivided USING gist (the_geom);\n\nCLUSTER flooding_sul_subdivided USING flooding_sul_subdivided_idx;\n\n---- Before doing difference\nVACUUM(FULL, ANALYZE) porto_alegre_net_outside_v2;\nVACUUM(FULL, ANALYZE) flooding_sul_subdivided;\n\n\nLastly, the difference is created using:\n\n\nShow the code\nCREATE TABLE flooding_symple as \nSELECT st_union(geom) as the_geom FROM flooding_cleaned_porto_union_simple;\n\nCREATE INDEX flooding_symple_idx ON flooding_symple USING gist (the_geom);\nCLUSTER flooding_symple USING flooding_symple_idx;\n\nCREATE TABLE difference_outside_flood_v3 AS\nSELECT net.id,\n        target,\n        source,\n        cost,\n        unidirectid,\n        bidirectid,\nst_difference(net.geom, flood.the_geom) AS the_geom\nFROM porto_alegre_net_outside_v2 AS net,\nflooding_symple  AS flood;\n\n\nAs a final step, they are merged.\n\n\nShow the code\n-----------\nCREATE TABLE porto_alegre_street_united AS\nSELECT *\nFROM porto_alegre_street_out\nUNION\nSELECT *\nFROM difference_outside_flood_v2;"
  },
  {
    "objectID": "methodology.html#routable-network",
    "href": "methodology.html#routable-network",
    "title": "2  Methodology",
    "section": "2.4 Routable network",
    "text": "2.4 Routable network\n\n\nShow the code\nSELECT pgr_createVerticesTable(\n            'porto_alegre_net_largest',\n            the_geom:= 'the_geom',\n            source:= 'fromid',\n            target:= 'toid')\n\n\n\n\nShow the code\n\nALTER TABLE od_2728\nADD COLUMN id SERIAL PRIMARY KEY;"
  },
  {
    "objectID": "methodology.html#centrality-analysis",
    "href": "methodology.html#centrality-analysis",
    "title": "2  Methodology",
    "section": "2.5 Centrality Analysis",
    "text": "2.5 Centrality Analysis\n\n2.5.1 Origin-Destination\n\n2.5.1.1 Random distribution (naive)\nA series of regular points that represented the origin and destination on the Area of Interest is created with the function “I_Grid_Point_Series”.\n\n\nShow the code\n--- Creating sampel data:\nCREATE OR REPLACE FUNCTION I_Grid_Point_Series(geom geometry, x_side decimal, y_side decimal, spheroid boolean default false)\nRETURNS SETOF geometry AS $BODY$\nDECLARE\nx_max decimal;\ny_max decimal;\nx_min decimal;\ny_min decimal;\nsrid integer := 4326;\ninput_srid integer;\nx_series DECIMAL;\ny_series DECIMAL;\nBEGIN\nCASE st_srid(geom) WHEN 0 THEN\n  geom := ST_SetSRID(geom, srid);\n  RAISE NOTICE 'SRID Not Found.';\n    ELSE\n        RAISE NOTICE 'SRID Found.';\n    END CASE;\n\n    CASE spheroid WHEN false THEN\n        RAISE NOTICE 'Spheroid False';\n    else\n        srid := 900913;\n        RAISE NOTICE 'Spheroid True';\n    END CASE;\n    input_srid:=st_srid(geom);\n    geom := st_transform(geom, srid);\n    x_max := ST_XMax(geom);\n    y_max := ST_YMax(geom);\n    x_min := ST_XMin(geom);\n    y_min := ST_YMin(geom);\n    x_series := CEIL ( @( x_max - x_min ) / x_side);\n    y_series := CEIL ( @( y_max - y_min ) / y_side );\nRETURN QUERY\nSELECT st_collect(st_setsrid(ST_MakePoint(x * x_side + x_min, y*y_side + y_min), srid)) FROM\ngenerate_series(0, x_series) as x,\ngenerate_series(0, y_series) as y\nWHERE st_intersects(st_setsrid(ST_MakePoint(x*x_side + x_min, y*y_side + y_min), srid), geom);\nEND;\n$BODY$ LANGUAGE plpgsql IMMUTABLE STRICT;\n\n\nThe following code created 1258 points representing the origin or destination regularly separated by 0.01º.\n\n\nShow the code\nCREATE TABLE regular_point_od AS (\nWITH multipoint_regular AS(\nselect \n    I_Grid_Point_Series(geom, 0.01,0.01, false) AS geom\n    from porto_alegre_bbox as geom),\npoint_regular AS(\nSELECT \n    st_setsrid((st_dump(geom)).geom, 4326)::geometry(Point, 4326) AS geom\nFROM  multipoint_regular)\nSELECT \n    row_number() over() AS id,\n    geom\nFROM \n    point_regular);\n\n\nThe application of two indexes improved further queries on this new table. This was recommended because these points were outside the network requiring snapping, which is a spatial operation with relatively high computational costs.\n\n\nShow the code\nCREATE INDEX idx_regular_point_od_geom ON regular_point_od USING GIST (geom);\nCREATE INDEX idx_regular_point_od_id ON regular_point_od USING btree(id);\n\n\nApart from indexing the geometry column and the id, the query is constrained to a buffer of 0.02º to reduce computational costs.\n\n\n2.5.1.2 Weighted on building volume\n\n\nShow the code\n# Import data\n## raster\nbuildings &lt;- terra::rast('/home/ricardo/HeiGIT-Github/do_not_push_too_large/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0/GHS_BUILT_V_E2020_GLOBE_R2023A_54009_100_V1_0.tif')\npop_ghs &lt;- 2728\n## AoI\naoi_bbox &lt;- sf::read_sf('/home/ricardo/HeiGIT-Github/data_required_porto_alegre/porto_alegre_bbox_derived.geojson')\n## reproject for clipping\naoi_bbox_reproj &lt;- aoi_bbox |&gt; st_transform(crs(buildings)) |&gt; as_Spatial()\nbuild_cropped &lt;- crop(buildings, aoi_bbox_reproj)\n## reproject for sampling\nbuild_4326 &lt;- terra::project(build_cropped, crs(aoi_bbox))\n## weighted sampling\nod &lt;- spatSample(build_cropped, pop_ghs, \"weights\", as.points=TRUE, ) |&gt; st_as_sf() |&gt; st_transform(4326)\nnames(od)[1] &lt;- 'building'\nDBI::dbWriteTable(connection, \n                  DBI::Id(schema = \"public\", table = \"od_2728\"), \n                  od)\n\n\nSnap the points\n\n\nShow the code\n--- Create ID column\n\nALTER TABLE od_2728\nADD COLUMN id serial primary key;\n\n--- Snap all the Apoints based on building density to the closest vertice within the network\nCREATE TABLE od_2728_snapped AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_2728 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n--- Create 100 origin samples from the snapped points\n\nCREATE TABLE weight_sampling_100_origin  AS\nWITH porto_100_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_origin;  \n--- Create 100 destination from the snapped points\n\nCREATE TABLE weight_sampling_100_destination  AS\nWITH porto_100_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped \n        ORDER BY random() LIMIT 100)\n        SELECT * FROM  porto_100_destination;  \n--- Indeces on origin\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(pt_id);\nCREATE INDEX weight_sampling_100_origin_idx_pt_id ON weight_sampling_100_origin USING btree(net_id);\nCREATE INDEX weight_sampling_100_origin_idx_the_geom ON weight_sampling_100_origin USING gist(the_geom);\n--- Indeces on destination\nCREATE INDEX weight_sampling_1000_destination_idx_pt_id ON weight_sampling_1000_destination USING btree(pt_id);\nCREATE INDEX weight_sampling_1000_destination_idx_net_id ON weight_sampling_1000_destination USING btree(net_id);\nCREATE INDEX weight_sampling_1000_destination_idx_the_geom ON weight_sampling_1000_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) weight_sampling_100_origin;\nVACUUM(full, ANALYZE) weight_sampling_100_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n\n\n\n\n2.5.1.3 Hospitals\n\n\nShow the code\n--- Create a table with the bounding box that contains Porto Alegre + GHS\nCREATE TABLE porto_alegre_bbox AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs)\n SELECT * FROM porto_alegre_ghs_bbox \n---Use the bounding box to select hospitals\nCREATE TABLE hospital_rs_node_v2 AS\nWITH hospital_rs_porto AS (\nSELECT \n    h.*\nFROM \n    hospitals_bed_rs AS h,\n    porto_alegre_bbox bbox\nWHERE st_intersects(h.geom, bbox.geom_bbox))\nSELECT DISTINCT ON (h.cd_cnes)\n    cd_cnes,\n    ds_cnes,\n    f.id,\n    f.the_geom &lt;-&gt; h.geom AS distance,\n    h.geom AS geom_hospital,\n    f.the_geom AS geom_node\nFROM hospital_rs_porto h\nLEFT JOIN LATERAL\n(SELECT \n    id, \n    the_geom\nFROM porto_alegre_net_largest_vertices_pgr AS net\nORDER BY\n    net.the_geom &lt;-&gt; h.geom\nLIMIT 1) AS f ON true\n\n---- Create index to optimize fuerther queries\n\nCREATE INDEX idx_hospital_rs_node_v2 ON hospital_rs_node_v2 USING btree(id);\n\n\n\n\n\n2.5.2 Centrality Analysis:\n\n\nShow the code\n--- Pre-scenario\n CREATE TABLE centrality_100_100_dijkstra AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_100_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_100_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_street_united AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n                      \n                     \n                     select * from porto_alegre_net_largest ;\n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_100_bidirect AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_100_100_dijkstra t1\nJOIN\n    porto_alegre_net_largest t2\nON\n    t1.id = t2.id;\n--- The final product requries  79941\nselect max(\"bidirectid\") from centrality_weighted_100_bidirect;\ncreate sequence bididirect_id_weight start 79941;\nupdate centrality_weighted_100_bidirect\nset \"bidirectid\" = nextval('bididirect_id_weight')\nwhere \"bidirectid\" is null ;\n---- verify\nselect count(*) \nfrom centrality_weighted_100_bidirect \nwhere \"bidirectid\" is null; --- 0\n----\ncreate table centrality_weighted_100_bidirect_group as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_100_bidirect\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---\n---- now recover the id\ncreate table centrality_weighted_100_bidirect_group_id as \nselect t1.*, t2.id\nfrom centrality_weighted_100_bidirect_group t1\njoin centrality_weighted_100_bidirect t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_100_bidirect_cleaned as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_100_bidirect_group_id t1\njoin\n    porto_alegre_net_largest t2\non\n    t1.id = t2.id;\n--- The final product is: centrality_weighted_100_bidirect_cleaned\n\n\n\n\nShow the code\n--- Post scenario\n\n----routing\n CREATE TABLE centrality_1000_1000_dijkstra_post AS\n SELECT   b.id,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  id,\n                             source,\n                            target,\n                            cost\n                      FROM porto_alegre_street_united',\n                      ARRAY(SELECT net_id AS start_id FROM weight_sampling_1000_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM weight_sampling_1000_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_street_united AS b\n                      ON j.edge = b.id\n                      GROUP BY  b.id, b.the_geom\n                      ORDER BY centrality DESC;  \n---- Adding the bidirectid by joining the dijkstra table with the original\nCREATE TABLE centrality_weighted_1000_bidirect_post AS\nSELECT t1.*,\n    t2.\"bidirectid\"\nFROM\n    centrality_1000_1000_dijkstra_post t1\nJOIN\n    porto_alegre_street_united t2\nON\n    t1.id = t2.id;\n--- The final product requries  79941\nselect max(\"bidirectid\") from centrality_weighted_1000_bidirect_post; ---62347\ncreate sequence bididirect_id_weight_post start 62347;\nupdate centrality_weighted_1000_bidirect_post\nset \"bidirectid\" = nextval('bididirect_id_weight_post')\nwhere \"bidirectid\" is null ;\n   ---- verify\nselect count(*) \nfrom centrality_weighted_1000_bidirect_post \nwhere \"bidirectid\" is null; --- 0              \n----\ncreate table centrality_weighted_1000_bidirect_group_post as \nselect \n       \"bidirectid\",\n       sum(centrality) as  centrality\nfrom centrality_weighted_1000_bidirect_post\ngroup by \"bidirectid\"; --- this sum the centrality for duplicated bidirect id\n---        \n---- now recover the id\ncreate table centrality_weighted_1000_bidirect_group_post_id as \nselect t1.*, t2.id\nfrom centrality_weighted_1000_bidirect_group_post t1\njoin centrality_weighted_1000_bidirect_post t2 \non t1.\"bidirectid\" = t2.\"bidirectid\"; \n---- add geometries\ncreate table centrality_weighted_1000_bidirect_cleaned_post as\nselect t1.*,\n    t2.the_geom,\n    t2.target,\n    t2.source,\n    t2.cost,\n    t2.\"unidirectid\"\nfrom \n    centrality_weighted_1000_bidirect_group_post_id t1\njoin\n    porto_alegre_street_united t2\non\n    t1.id = t2.id;\n\n\n\n2.5.2.1 Edge betweenness\n\n\nShow the code\n\nCREATE TABLE od_40420_snapped_origin AS\nSELECT DISTINCT ON (net.id)\n       pt.id AS pt_id,\n       net.id AS net_id,\n       net.the_geom\nFROM \n(select * \nFROM \n    od_77763 as pt) as pt\nCROSS JOIN\nLATERAL (SELECT\n        * \n        FROM porto_alegre_net_largest_vertices_pgr AS net\n         ORDER BY net.the_geom &lt;-&gt; pt.geometry \n        LIMIT 1) AS net;\n\n\n\n\nShow the code\nCREATE TABLE random_272_destination  AS\nwith random_272_destination AS (\n        SELECT\n            * \n        FROM \n            od_40420_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  random_272_destination;  \n\n\n\n\nShow the code\n\nCREATE TEMP TABLE vertices_lookup_v5 \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,\n         random_272_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\nShow the code\nCREATE TABLE porto_272_272_dijkstra AS\n WITH pgr_result AS (\n   SELECT pgr_dijkstra('SELECT id,\n           source,\n    target,\n     cost FROM porto_alegre_net_largest',\n     array_agg(fv), array_agg(tv), \n     directed := true\n   ) FROM vertices_lookup_v5\n )\nSELECT (pgr_dijkstra).*, a.fid, a.tid FROM pgr_result\nJOIN vertices_lookup_v5 a\nON (pgr_dijkstra).start_vid = a.fv\nAND (pgr_dijkstra).end_vid = a.tv;\n\n\n\n\nShow the code\n\nCREATE TEMP TABLE vertices_lookup_v5                     \nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom as fgeom,\n         t.net_id AS tid, t.the_geom as tgeom\n    FROM random_272_origin AS f,                                                                      \n         random_272_destination AS t               \n),                                                                                     \nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) as fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) as tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n\n\n\n\n2.5.2.2 Closeness\n\n\nShow the code\nCREATE TABLE clossness_hospital_porto AS \nWITH dijkstra_cost AS (\nSELECT * FROM pgr_dijkstraCostMatrix(\n  'SELECT id, source, target, cost FROM porto_alegre_net_largest',\n  (SELECT array_agg(id)\n    FROM porto_alegre_net_largest_vertices_pgr\n    WHERE id IN (SELECT id FROM hospital_rs_node_v3)),\n  true)),\ncloseness AS (\n SELECT dc.start_vid,\n        sum(agg_cost)::int AS closeness\nFROM dijkstra_cost dc\nGROUP BY dc.start_vid\nORDER BY closeness DESC)\nSELECT \n    h.cd_cnes,\n    h.ds_cnes,\n    c.closeness,\n    h.distance,\n    h.geom_node,\n    h.geom_hospital\nFROM \n    closeness AS c\nLEFT JOIN\n    hospital_rs_node_v3 AS h ON  c.start_vid = h.id;\n\n\n\n\n2.5.2.3 Vulnerability: Socioeconomic indicators\n\n\n2.5.2.4 Resiliance:"
  },
  {
    "objectID": "results.html#preparation",
    "href": "results.html#preparation",
    "title": "3  Results",
    "section": "3.1 Preparation",
    "text": "3.1 Preparation\nThe code ?lst-ghs-osmium covered the following area:\n\n\nShow the code\n# Osmium command\nosmium extract -b -51.2791,-30.1722,-50.9407,-29.8048 sul-240501.osm.pbf -o puerto_alegre_urban_center.osm.pbf"
  },
  {
    "objectID": "results.html#framework",
    "href": "results.html#framework",
    "title": "3  Results",
    "section": "3.2 Framework",
    "text": "3.2 Framework"
  },
  {
    "objectID": "results.html#osm-ors",
    "href": "results.html#osm-ors",
    "title": "3  Results",
    "section": "3.3 OSM & ORS",
    "text": "3.3 OSM & ORS\n\n\nShow the code\n# Load data\nors_network &lt;- st_read(eisenberg_connection, layer=\"porto_alegre_net_pre\")\nosm_network &lt;- st_read(eisenberg_connection, layer=\"puerto_alegre_ghs_osm\")\nors_network_subset &lt;- ors_network |&gt; head()\n## Create subset using a bounding box\nors_subset &lt;- ors_network |&gt; filter(id ==140210) |&gt; st_bbox()\nxrange &lt;- ors_subset$xmax - ors_subset$xmin\nyrange &lt;- ors_subset$ymax - ors_subset$ymin\n## Expand the bounding box\nors_subset[1] &lt;- ors_subset[1] - (4 * xrange) # xmin - left\nors_subset[3] &lt;- ors_subset[3] + (4 * xrange) # xmax - right\nors_subset[2] &lt;- ors_subset[2] - (2 * yrange) # ymin - bottom\nors_subset[4] &lt;- ors_subset[4] + (2 * yrange) # ymax - top\n## Convert bounding box into polygon\nors_subset_bbox &lt;- ors_subset %&gt;%  # \n  st_as_sfc() \n## Use the polygon to subset the network\nintersection_ors &lt;- sf::st_intersection(ors_network, ors_subset_bbox)\nintersection_osm &lt;- sf::st_intersection(osm_network, ors_subset_bbox)\n## Mapview maps\nm1 &lt;- mapview(intersection_osm, \n              color=\"#6e93ff\",\n              layer.name =\"OpenStreetMap - Geometry\",\n              popup=popupTable(intersection_osm, \n                               zcol=c(\"id\",\"osm_id\",\"name\",\"geom\")))\nm2 &lt;- mapview(intersection_ors,\n              color =\"#d50038\",\n              layer.name=\"OpenRouteService -Graph\",\n              popup=popupTable(intersection_ors))\n\nsync(m1,m2)"
  },
  {
    "objectID": "results.html#sampling-points",
    "href": "results.html#sampling-points",
    "title": "3  Results",
    "section": "3.4 Sampling points",
    "text": "3.4 Sampling points"
  },
  {
    "objectID": "results.html#network",
    "href": "results.html#network",
    "title": "3  Results",
    "section": "3.5 Network",
    "text": "3.5 Network\n\n3.5.1 Post-event\n\n\nShow the code\n--- pre_net_subset\nCREATE TABLE porto_alegre_net_largest_subset AS                  \nSELECT\n  *\nFROM \n  porto_alegre_net_largest AS net\nWHERE \nnet.the_geom && \nst_setsrid(\n            st_makeenvelope(\n              -51.214287,-30.020226,-51.12934,-29.945862),4326)\n--- subset_network_in\n\n------------- network inside\n\nCREATE TEMPORARY TABLE flooding_subdivided_porto_join AS\nSELECT st_union(the_geom) AS the_geom FROM flooding_subdivided_porto ;\n\n----\nCREATE TABLE porto_alegre_street_in_v3 AS\nSELECT net.id,\n    CASE \n        WHEN ST_Contains(flood.the_geom, net.the_geom)\n        THEN net.the_geom\n        ELSE st_intersection(net.the_geom, flood.the_geom)\n    END AS  geom\nFROM porto_alegre_net_largest_subset net\nJOIN flooding_subdivided_porto_join flood\nON st_intersects(net.the_geom, flood.the_geom);         \n\n--- subset_network_out\nCREATE TABLE porto_alegre_street_out_v3 AS\nSELECT net.*\nFROM porto_alegre_net_largest_subset\nWHERE net.id NOT IN (\n    SELECT net.id\n    FROM porto_alegre_street_in_v3 net);\n    \n---- network outside\n\nCREATE TABLE porto_alegre_net_outside_v3 AS\nWITH porto_alegre_ghs AS(\n    SELECT \n       ghs.*\n    FROM\n       urban_center_4326 AS ghs\n    JOIN\n       nuts \n    ON \n       st_intersects(nuts.geom, ghs.geom)\n    WHERE \n        nuts.shapename = 'Porto Alegre'),\n--- Bounding Box that contained the GHS in Porto Alegre\nporto_alegre_ghs_bbox AS(\n    SELECT \n        st_setsrid(st_extent(geom),4326) as geom_bbox\n    FROM \n        porto_alegre_ghs),\nflooding_sul_subdivided AS (\n        SELECT \n            st_subdivide(geom) as the_geom\n        FROM\n            flooding_rio_grande_do_sul),\nexterior_ring_porto_alegre_v2 AS (\nSELECT \n    ST_ExteriorRing((ST_Dump(union_geom)).geom) as geom\nFROM (\n    SELECT \n        ST_Union(flood.the_geom) as union_geom\n    FROM \n        porto_alegre_ghs_bbox as bbox\n    JOIN \n        flooding_sul_subdivided as flood\n    ON \n        ST_Intersects(flood.the_geom, bbox.geom_bbox)\n) AS subquery)\nSELECT net.id,\n    CASE\n        WHEN NOT ST_Contains(flood.geom, net.the_geom)\n        THEN net.the_geom\n            ELSE st_intersection(net.the_geom, flood.geom)\n    END AS  geom,\n    net.target,\n       net.source,\n       cost,\n       \"unidirectid\",\n       \"bidirectid\"\nFROM\nporto_alegre_net_largest_subset AS net\nJOIN exterior_ring_porto_alegre_v2 flood ON\nst_intersects(net.the_geom, flood.geom)\nWHERE \n  net.the_geom && \n    st_setsrid(\n    st_makeenvelope(-51.214287,-30.020226,-51.12934,-29.945862),4326);\n----\n--- For the network\nCREATE INDEX idx_porto_alegre_net_outside_v2 ON porto_alegre_net_outside_v2 USING gist (geom);\n\nCLUSTER porto_alegre_net_outside_v2 USING idx_porto_alegre_net_outside_v2;\n\n--- For the flooding mask\nCREATE INDEX flooding_sul_subdivided_idx ON flooding_sul_subdivided USING gist (the_geom);\n\nCLUSTER flooding_sul_subdivided USING flooding_sul_subdivided_idx;\n\n---- Before doing difference\nVACUUM(FULL, ANALYZE) porto_alegre_net_outside_v2;\nVACUUM(FULL, ANALYZE) flooding_sul_subdivided;\n----\nCREATE INDEX idx_porto_alegre_net_outside_v3 ON porto_alegre_net_outside_v3 USING gist (geom);\n\nCLUSTER porto_alegre_net_outside_v3 USING idx_porto_alegre_net_outside_v3;\n\n--- For the flooding mask\nCREATE INDEX flooding_subdivided_porto_join_idx ON flooding_subdivided_porto_join USING gist (the_geom);\n\nCLUSTER flooding_subdivided_porto_join USING flooding_subdivided_porto_join_idx ;\n\n---- Before doing difference\nVACUUM(FULL, ANALYZE) porto_alegre_net_outside_v3;\nVACUUM(FULL, ANALYZE) flooding_subdivided_porto_join;\n\n----\n\nCREATE TABLE flooding_symple as \nSELECT st_union(geom) as the_geom FROM flooding_cleaned_porto_union_simple;\n\nCREATE INDEX flooding_symple_idx ON flooding_symple USING gist (the_geom);\nCLUSTER flooding_symple USING flooding_symple_idx;\n\nCREATE TABLE difference_outside_flood_v4 AS\nSELECT net.id,\n        target,\n        source,\n        cost,\n        unidirectid,\n        bidirectid,\nst_difference(net.geom, flood.the_geom) AS the_geom\nFROM porto_alegre_net_outside_v3 AS net,\nflooding_symple  AS flood;\n\n-----------\nCREATE TABLE porto_alegre_street_united_v3 AS\nSELECT *\nFROM porto_alegre_street_out_v3\nUNION\nSELECT *\nFROM difference_outside_flood_v4;\n\n--- Final product: porto_alegre_street_united_v2\n\n\n\n\nShow the code\nporto_alegre_net_pre &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_net_largest\")\ncentrality_pre &lt;- sf::st_read(eisenberg_connection, \"centrality_weighted_100_bidirect_cleaned\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_cleaned_porto\")\nnet_in &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_in_v2\")\n## subset\nsubset_network_pre &lt;-  sf::st_read(eisenberg_connection, \"porto_alegre_net_largest_subset\")\nsubset_network_in &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_in_v3\")\nsubset_network_out &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_out_v3\")\nsubset_network_outside_flood &lt;-sf::st_read(eisenberg_connection, \"difference_outside_flood_v4\")\nsubset_network_post &lt;- sf::st_read(eisenberg_connection, \"porto_alegre_street_united_v3\")\nflooding &lt;- sf::st_read(eisenberg_connection, \"flooding_symple\")\n  \n\n## centrality\ncentrality_pre_map &lt;- mapview(subset_network_pre,\n                              color=\"#d4e7e7\",\n                              lwd= 0.8,\n                             layer.name=\"Pre-flooding network\",\n                             popup=popupTable(subset_network_pre,\n                                              zcol=c(\"id\",\"source\",\"target\",\"bidirectid\")))\n\n\n\n\nShow the code\n\nCREATE TABLE subset_post_scenario_bbox AS\nSELECT st_setsrid(\n            st_makeenvelope(\n              -51.214287,-30.020226,-51.12934,-29.945862),4326) AS geom;\n\n\n\n\nShow the code\n\n---- Generate the subset to be visualize\n\nCREATE TABLE porto_alegre_net_largest_subset AS                  \nSELECT\n  st_intersection(net.the_geom, bbox.geom) \nFROM \n  porto_alegre_net_largest AS net\nWHERE \nporto_alegre_net_largest.the_geom && subset_post_scenario_bbox; \n\n\n\n\nShow the code\nSELECT\n  st_intersection(net.the_geom, bbox.geom)\nFROM \n  porto_alegre_net_largest AS net,\nsubset_post_scenario_bbox AS bbox;\n\n\n\n\n3.5.2 Performance\n\n\nShow the code\nlibrary(readODS)\nlibrary(ggplot2)\nperformance &lt;- readODS::read_ods(\"metrics.ods\")\ndf &lt;- performance |&gt; select(c(\"number_od\",\"method\",\"algorithm\",\"time\",\"max_centrality\",\"count_rows\")) |&gt; filter(!is.na(time) & method=='naive')\n\nggplot(df,aes(x=number_od, y=time, group = algorithm)) +\n  geom_line(aes(color=algorithm)) +\n   geom_point(aes(color=algorithm))\n\n\n\n\n\n\n\n3.5.3 Centrality\n\n\n3.5.4 Closeness\n\n\nShow the code\nclosseness &lt;-sf::st_read(eisenberg_connection, \n                            layer = \"clossness_hospital_porto\")\nclosseness_no_geom &lt;- closseness |&gt;  \n                          arrange(ds_cnes, closeness) |&gt;\n                          mutate(lng= \n                              unlist(map(geom_hospital,1)),\n                           lat=\n                              unlist(map(geom_hospital,2)),\n                           closeness_norm = \n                          (closseness$closeness - min(closseness$closeness)) / (max(closseness$closeness) - min(closseness$closeness)) * 100,\n                          position = rank(-closeness),\n                          ds_cnes =stringr::str_to_title(ds_cnes)) |&gt;\n                  sf::st_drop_geometry()\n\nDT::datatable(subset(closseness_no_geom, select=c(\"position\",\"cd_cnes\",\"ds_cnes\",\"closeness\")),\n              extensions=\"Buttons\",\n                         options=list(\n                           dom=\"Bfrtip\",\n                           buttons=c(\"copy\",\"csv\",\"pdf\"),\n                            initComplete = JS(\n    \"function(settings, json) {\",\n    \"$(this.api().table().header()).css({'background-color': '#d50038', 'color': '#fff'});\",\n    \"}\")\n                                  )\n              ) |&gt; \n    DT::formatStyle(\"closeness\",\n              background=DT::styleColorBar(range(closseness_no_geom$closeness),'#ee8b8b'),\n                backgroundSize = '98% 88%',\n  backgroundRepeat = 'no-repeat',\n  backgroundPosition = 'center') \n\n\n\n\n\n\n\n\n\nShow the code\n## hospital with closenesss values\nclosseness_df &lt;- closseness |&gt;  arrange(ds_cnes, closeness) |&gt;\n                    mutate(lng= \n                              unlist(map(geom_hospital,1)),\n                           lat=\n                              unlist(map(geom_hospital,2)),\n                           closeness_norm = \n                          (closseness$closeness - min(closseness$closeness)) / (max(closseness$closeness) - min(closseness$closeness)) * 100,\n                          position = rank(-closeness))\n## Create Color palette for visualization\npal &lt;- colorQuantile(palette = \"OrRd\",closseness_df$closeness, n=4 )\n\n## Create leaflet product\n\nicons &lt;- makeAwesomeIcon(\n  icon = 'fa-heartbeat',\n  iconColor = \"#FFFFFF\",\n  markerColor = \"#57142c\",\n  library = \"fa\"\n)\nleaflet(closseness_df) |&gt;\n    addProviderTiles(providers$OpenStreetMap.HOT) |&gt;\n    addCircles(data =closseness_df , radius = ~sqrt(closeness)*10, fillOpacity = .50, color =~pal(closeness)) |&gt;\n  addAwesomeMarkers(data=closseness_df,\n                          icon =icons,\n                          popup= ~paste0(\"&lt;b&gt;Código CNES: &lt;/b&gt;\", cd_cnes, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Nome: &lt;/b&gt;\", ds_cnes, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt; Closeness&lt;/b&gt;:\", closeness, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Longitude: &lt;/b&gt;\", lng, \"&lt;br/&gt;\",\n                                   \"&lt;b&gt;Posição &lt;/b&gt;\", position, \"&lt;br/&gt;\"))"
  },
  {
    "objectID": "conclussion.html#framework",
    "href": "conclussion.html#framework",
    "title": "4  Conclussion",
    "section": "4.1 Framework",
    "text": "4.1 Framework"
  },
  {
    "objectID": "conclussion.html#data",
    "href": "conclussion.html#data",
    "title": "4  Conclussion",
    "section": "4.2 Data",
    "text": "4.2 Data"
  },
  {
    "objectID": "conclussion.html#sampling-points",
    "href": "conclussion.html#sampling-points",
    "title": "4  Conclussion",
    "section": "4.3 Sampling points",
    "text": "4.3 Sampling points"
  },
  {
    "objectID": "conclussion.html#network",
    "href": "conclussion.html#network",
    "title": "4  Conclussion",
    "section": "4.4 Network",
    "text": "4.4 Network"
  },
  {
    "objectID": "future_work.html#point-of-interest",
    "href": "future_work.html#point-of-interest",
    "title": "5  Future work & suggestions",
    "section": "5.1 Point of interest",
    "text": "5.1 Point of interest"
  },
  {
    "objectID": "future_work.html#perforamance",
    "href": "future_work.html#perforamance",
    "title": "5  Future work & suggestions",
    "section": "5.2 Perforamance",
    "text": "5.2 Perforamance\n\n5.2.1 Flood mask\nUsing the dilate and erode method described in the slide 95/187 (link) could remove small islands that increase the computational costs. A use case of this technique is observed when simplifying coastlines (link)"
  },
  {
    "objectID": "future_work.html#data",
    "href": "future_work.html#data",
    "title": "5  Future work & suggestions",
    "section": "5.3 Data",
    "text": "5.3 Data"
  },
  {
    "objectID": "future_work.html#sampling-points",
    "href": "future_work.html#sampling-points",
    "title": "5  Future work & suggestions",
    "section": "5.4 Sampling points",
    "text": "5.4 Sampling points"
  },
  {
    "objectID": "future_work.html#network",
    "href": "future_work.html#network",
    "title": "5  Future work & suggestions",
    "section": "5.5 Network",
    "text": "5.5 Network"
  },
  {
    "objectID": "appendix.html#section",
    "href": "appendix.html#section",
    "title": "6  Performance tests",
    "section": "6.1 10",
    "text": "6.1 10\n#| eval: false"
  },
  {
    "objectID": "appendix.html#section-1",
    "href": "appendix.html#section-1",
    "title": "6  Performance tests",
    "section": "6.2 50",
    "text": "6.2 50\n#| eval: false"
  },
  {
    "objectID": "appendix.html#section-2",
    "href": "appendix.html#section-2",
    "title": "6  Performance tests",
    "section": "6.3 100",
    "text": "6.3 100\n#| eval: false"
  },
  {
    "objectID": "appendix.html#section-3",
    "href": "appendix.html#section-3",
    "title": "6  Performance tests",
    "section": "6.4 200",
    "text": "6.4 200\n#| eval: false\n---- Create 200 origin\nCREATE TABLE sampling_weight_200_origin  AS\nwith porto_200_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_origin;  \n---- Create 200 destination\nCREATE TABLE sampling_weight_200_destination  AS\nwith porto_200_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 200)\n        SELECT * FROM  porto_200_destination;  \n---- Create index for origin\nCREATE INDEX idx_sampling_weight_200_origin_net_id ON sampling_weight_200_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_origin_geom ON sampling_weight_200_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_200_destination_net_id ON sampling_weight_200_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_200_destination_geom ON sampling_weight_200_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_200_origin;\nVACUUM(full, ANALYZE) sampling_weight_200_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n--- Run query\n\nEXPLAIN ANALYZE\n CREATE TABLE centrality_200_200_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM porto_200_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM porto_200_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;"
  },
  {
    "objectID": "appendix.html#section-4",
    "href": "appendix.html#section-4",
    "title": "6  Performance tests",
    "section": "6.5 300",
    "text": "6.5 300\n#| eval:  false\n\n\n---- 300\n\n---- Create 300 origin\nCREATE TABLE sampling_weight_300_origin  AS\nwith porto_300_origin AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 300)\n        SELECT * FROM  porto_300_origin;  \n---- Create 300 destination\nCREATE TABLE sampling_weight_300_destination  AS\nwith porto_300_destination AS (\n        SELECT\n            * \n        FROM \n            od_2728_snapped_origin \n        ORDER BY random() LIMIT 300)\n        SELECT * FROM  porto_300_destination;  \n---- Create index for origin\nCREATE INDEX idx_sampling_weight_300_origin_net_id ON sampling_weight_300_origin USING hash(net_id);\nCREATE INDEX idx_sampling_weight_300_origin_geom ON sampling_weight_300_origin USING gist(the_geom);\n---- Create index for destination\nCREATE INDEX idx_sampling_weight_300_destination_net_id ON sampling_weight_300_destination USING hash(net_id);\nCREATE INDEX idx_sampling_weight_300_destination_geom ON sampling_weight_300_destination USING gist(the_geom);\n---- Cluster\nCLUSTER porto_alegre_net_largest USING idx_porto_alegre_net_largest_geom;\n---- Vacuum clean\nVACUUM(full, ANALYZE) sampling_weight_300_origin;\nVACUUM(full, ANALYZE) sampling_weight_300_destination;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n--- Run query\n\n EXPLAIN ANALYZE\n CREATE TABLE centrality_300_300_porto AS\n SELECT   b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \n FROM  pgr_dijkstra('SELECT  ogc_fid AS id,\n                              fromid AS source,\n                            toid AS target,\n                            weight AS cost\n                      FROM porto_alegre_net_largest',\n                      ARRAY(SELECT net_id AS start_id FROM sampling_weight_300_origin  ),\n                      ARRAY(SELECT net_id AS end_id FROM sampling_weight_300_destination ),\n                      directed := TRUE) j\n                      left JOIN porto_alegre_net_largest AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n                      \n--- check max centrality                     \nSELECT max(centrality) FROM centrality_300_300_porto ;\n--- check max rows\nSELECT  count(*) FROM centrality_10_10_porto;"
  },
  {
    "objectID": "appendix.html#method-array_agg",
    "href": "appendix.html#method-array_agg",
    "title": "6  Performance tests",
    "section": "6.6 Method array_agg()",
    "text": "6.6 Method array_agg()\n#| eval: false\n\n\n---- create origin_destination\nCREATE TEMP TABLE vertices_lookup_10\nAS             \nWITH all_pairs AS (\n  SELECT f.net_id AS fid, f.the_geom AS fgeom,\n         t.net_id AS tid, t.the_geom AS tgeom\n    FROM random_10_origin AS f,\n         random_10_destination AS t\n),\nvertices AS (\n  SELECT fid, tid,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; fgeom\n       LIMIT 1) AS fv,\n     (SELECT id\n        FROM porto_alegre_net_largest_vertices_pgr AS way\n       ORDER BY way.the_geom &lt;-&gt; tgeom\n       LIMIT 1) AS tv\n  FROM all_pairs\n)\nSELECT * FROM vertices;\n---- Number of OD\nSELECT count(*) FROM vertices_lookup_10;\n---- Create index\nCREATE INDEX idx_vertices_lookup_10_fid ON vertices_lookup_10 USING hash(fid);\nCREATE INDEX idx_vertices_lookup_10_tid ON vertices_lookup_10 USING hash(tid);\nCREATE INDEX idx_vertices_lookup_10_fv ON vertices_lookup_10 USING hash(fv);\nCREATE INDEX idx_vertices_lookup_10_tv ON vertices_lookup_10 USING hash(tv);\n---- Vacuum and clean\nVACUUM(full, ANALYZE) vertices_lookup_10;\nVACUUM(full, ANALYZE) porto_alegre_net_largest;\n---- Run query using array_agg()\nEXPLAIN ANALYZE\nCREATE TABLE porto_100_dijkstra_agg AS\nWITH pgr_result AS (\n  SELECT pgr_dijkstra('SELECT ogc_fid AS id,\n             fromid AS source,\n            toid AS target,\n             weight AS cost FROM porto_alegre_net_largest',\n    array_agg(fv), array_agg(tv), \n    directed := true\n  ) FROM vertices_lookup_10 \n) \nSELECT \n b.ogc_fid,\n b.the_geom,\n count(the_geom) as centrality \nFROM pgr_result\nLEFT JOIN porto_alegre_net_largest AS b\nON (pgr_dijkstra).edge = b.ogc_fid\nGROUP BY \n    the_geom, b.ogc_fid\nORDER BY \n    centrality DESC;\n---- Max centrality value\nselect max(centrality) FROM porto_100_dijkstra_agg ;\n---- Number of rows\nselect count(*) FROM porto_100_dijkstra_agg ;"
  },
  {
    "objectID": "appendix.html#pgr_astrar",
    "href": "appendix.html#pgr_astrar",
    "title": "6  Performance tests",
    "section": "6.7 pgr_astrar()",
    "text": "6.7 pgr_astrar()\n#| eval: false\n\nSELECT * FROM random_10_origin ro ;\n \nCREATE TABLE porto_alegre_net_largest_astar AS          \nWITH porto_alegre_net_astart AS (\nSELECT \n*,\nst_startpoint(the_geom) AS start_pt,\nst_endpoint(the_geom) AS end_pt\nFROM porto_alegre_net_largest AS net)\nSELECT *,\n    st_x(start_pt) AS x1,\n    st_y(start_pt) AS y1,\n    st_x(end_pt) AS x2,\n    st_y(end_pt) AS y2\nFROM porto_alegre_net_astart;\n---- adding spatial index\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_the_geom ON porto_alegre_net_largest_astar  USING gist(the_geom);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_start ON porto_alegre_net_largest_astar  USING gist(start_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_end ON porto_alegre_net_largest_astar USING gist(end_pt);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x1 ON porto_alegre_net_largest_astar  USING btree(x1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y1 ON porto_alegre_net_largest_astar USING btree(y1);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_x2 ON porto_alegre_net_largest_astar USING btree(x2);\nCREATE INDEX idx_porto_alegre_net_largest_astar_net_y2 ON porto_alegre_net_largest_astar USING btree(y2);                      \n----- RUnning the query\nEXPLAIN ANALYZE\nCREATE TABLE centrality_10_10_porto_astrar AS\nSELECT  \n    b.ogc_fid,\n     b.the_geom,\n    count(the_geom) as centrality \nFROM pgr_astar(\n    'SELECT ogc_fid AS id,\n            fromid AS source,\n            toid AS target,\n            weight AS cost,\n            x1,\n            y1,\n            x2,\n            y2\n    FROM porto_alegre_net_largest_astar',\n    ARRAY(SELECT net_id FROM  random_10_origin),\n    ARRAY(SELECT net_id FROM  random_10_destination),\n             directed:=TRUE,\n             heuristic:=2) j\n                      left JOIN porto_alegre_net_largest_astar AS b\n                      ON j.edge = b.ogc_fid\n                      GROUP BY  b.ogc_fid, b.the_geom\n                      ORDER BY centrality DESC;  \n\n--- check max centrality                     \nSELECT max(centrality) FROM centrality_10_10_porto_astrar ;\n--- check max rows\nSELECT  count(*) FROM centrality_10_10_porto_astrar;"
  }
]